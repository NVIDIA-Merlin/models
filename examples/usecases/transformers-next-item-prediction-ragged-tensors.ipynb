{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a556f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions anda\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80126e",
   "metadata": {},
   "source": [
    "Run the below cell once in a new TF 22.09 container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ab59df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/NVIDIA-Merlin/Models\n",
      "   042ee198f..ada3874d6 main                    -> origin/main\n",
      " * [new branch]        add_category_encoding_test -> origin/add_category_encoding_test\n",
      " * [new branch]        add_lightfm_and_explicit_training_example -> origin/add_lightfm_and_explicit_training_example\n",
      " * [new branch]        add_logo_tracking_to_07 -> origin/add_logo_tracking_to_07\n",
      " * [new branch]        add_notebooks_test      -> origin/add_notebooks_test\n",
      " * [new branch]        break_ties              -> origin/break_ties\n",
      " * [new branch]        change_two_tower_api_test -> origin/change_two_tower_api_test\n",
      " * [new branch]        dataset/booking         -> origin/dataset/booking\n",
      " * [new branch]        fix-contrastive-predictions -> origin/fix-contrastive-predictions\n",
      " * [new branch]        fix_lightfm_evaluate    -> origin/fix_lightfm_evaluate\n",
      " * [new branch]        fix_mtl_metrics         -> origin/fix_mtl_metrics\n",
      " * [new branch]        fix_notebooks           -> origin/fix_notebooks\n",
      " * [new branch]        fix_regression          -> origin/fix_regression\n",
      " * [new branch]        fix_test_07             -> origin/fix_test_07\n",
      "   ed4470c3d..5db2d18a8 gh-pages                -> origin/gh-pages\n",
      " + bbf11f70a...a94410bf6 hashed_cross_test       -> origin/hashed_cross_test  (forced update)\n",
      " * [new branch]        laiacano/tox            -> origin/laiacano/tox\n",
      " * [new branch]        layer_freezing_test     -> origin/layer_freezing_test\n",
      " * [new branch]        masking_transforms      -> origin/masking_transforms\n",
      " * [new branch]        mlm                     -> origin/mlm\n",
      " * [new branch]        mlm_alt                 -> origin/mlm_alt\n",
      " + 7b6a5bec3...d53430381 mtl_loss                -> origin/mtl_loss  (forced update)\n",
      " * [new branch]        mtl_models              -> origin/mtl_models\n",
      " * [new branch]        mtl_regularization      -> origin/mtl_regularization\n",
      " * [new branch]        multi_optimizer_example -> origin/multi_optimizer_example\n",
      " * [new branch]        neg_sampling            -> origin/neg_sampling\n",
      " * [new branch]        ranking_models_inputs   -> origin/ranking_models_inputs\n",
      " * [new branch]        refactor-docs-reqs      -> origin/refactor-docs-reqs\n",
      " * [new branch]        refactor/docs-reqs      -> origin/refactor/docs-reqs\n",
      " * [new branch]        release-22.10           -> origin/release-22.10\n",
      " * [new branch]        reset-metrics           -> origin/reset-metrics\n",
      " * [new branch]        revert-813-laiacano/tox-and-tmpdir -> origin/revert-813-laiacano/tox-and-tmpdir\n",
      " * [new branch]        session_based           -> origin/session_based\n",
      " * [new branch]        t4rec_use_case          -> origin/t4rec_use_case\n",
      " * [new branch]        tf/add-bokeh-to-dev     -> origin/tf/add-bokeh-to-dev\n",
      " * [new branch]        tf/categorical-prediction-2 -> origin/tf/categorical-prediction-2\n",
      " * [new branch]        tf/combinators-base     -> origin/tf/combinators-base\n",
      " + 475008f4b...e74b63abd tf/contrastive-prediction -> origin/tf/contrastive-prediction  (forced update)\n",
      " * [new branch]        tf/dep-prediction-tasks -> origin/tf/dep-prediction-tasks\n",
      " * [new branch]        tf/embeddings_regularization -> origin/tf/embeddings_regularization\n",
      " * [new branch]        tf/fix_mlm_test         -> origin/tf/fix_mlm_test\n",
      " * [new branch]        tf/input-block-filter   -> origin/tf/input-block-filter\n",
      " * [new branch]        tf/inputs-concat        -> origin/tf/inputs-concat\n",
      " * [new branch]        tf/logq_correction      -> origin/tf/logq_correction\n",
      " * [new branch]        tf/mlm-schema           -> origin/tf/mlm-schema\n",
      " * [new branch]        tf/output-block         -> origin/tf/output-block\n",
      "   7d64ab0eb..bda5a0064 tf/prediction           -> origin/tf/prediction\n",
      " * [new branch]        tf/retrieval-model-v2   -> origin/tf/retrieval-model-v2\n",
      " + ad1f4c3d9...e361d0314 tf/retrieval-models     -> origin/tf/retrieval-models  (forced update)\n",
      " * [new branch]        tf/target-propagation   -> origin/tf/target-propagation\n",
      " * [new branch]        tf/tf-cont-list         -> origin/tf/tf-cont-list\n",
      " * [new branch]        tf/topk_recommender     -> origin/tf/topk_recommender\n",
      " * [new branch]        tf/transformer-block    -> origin/tf/transformer-block\n",
      " * [new branch]        tf/transformer_block    -> origin/tf/transformer_block\n",
      " * [new branch]        tf/xlnet-bug            -> origin/tf/xlnet-bug\n",
      " * [new branch]        update_example_01       -> origin/update_example_01\n",
      " * [new branch]        update_examples_with_tracking_logo -> origin/update_examples_with_tracking_logo\n",
      " * [new branch]        wide_deep_example_test  -> origin/wide_deep_example_test\n",
      " * [new branch]        wideanddeep_example     -> origin/wideanddeep_example\n",
      " * [new branch]        xgboost/predict-without-target -> origin/xgboost/predict-without-target\n",
      " * [new tag]           v0.9.0                  -> v0.9.0\n",
      " * [new tag]             untagged-d7fc9188a485f21abbc4 -> untagged-d7fc9188a485f21abbc4\n",
      " * [new tag]             v0.7.0                  -> v0.7.0\n",
      " * [new tag]             v0.8.0                  -> v0.8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 042ee198f..ada3874d6\n",
      "Fast-forward\n",
      " .github/workflows/datasets.yml                     |    2 +-\n",
      " .github/workflows/docs-sched-rebuild.yaml          |    2 +-\n",
      " .github/workflows/implicit.yml                     |    2 +-\n",
      " .github/workflows/lightfm.yml                      |    2 +-\n",
      " .github/workflows/pytorch.yml                      |    2 +-\n",
      " .github/workflows/tensorflow.yml                   |   23 +-\n",
      " .github/workflows/xgboost.yml                      |    2 +-\n",
      " .pre-commit-config.yaml                            |    1 +\n",
      " CONTRIBUTING.md                                    |   44 +\n",
      " Makefile                                           |    2 +-\n",
      " ci/test_unit.sh                                    |    2 +-\n",
      " examples/01-Getting-started.ipynb                  |   11 +-\n",
      " ...2-Merlin-Models-and-NVTabular-integration.ipynb |  242 +-\n",
      " examples/03-Exploring-different-models.ipynb       | 1256 +++--\n",
      " examples/04-Exporting-ranking-models.ipynb         |  170 +-\n",
      " examples/05-Retrieval-Model.ipynb                  | 1605 ++++--\n",
      " ...-your-own-architecture-with-Merlin-Models.ipynb |  484 +-\n",
      " ...al-ML-models-using-the-Merlin-Models-API.ipynb} |  279 +-\n",
      " examples/images/bi-lstm_ecommerce.png              |  Bin 0 -> 82761 bytes\n",
      " examples/images/mlp_ecommerce.png                  |  Bin 0 -> 83306 bytes\n",
      " examples/images/wide_and_deep.png                  |  Bin 0 -> 89058 bytes\n",
      " ...ing-of-large-embedding-tables-by-LazyAdam.ipynb |  476 ++\n",
      " examples/usecases/e-commerce.ipynb                 |  531 --\n",
      " ...on-based-next-item-prediction-for-fashion.ipynb | 1635 ++++++\n",
      " .../entertainment-with-pretrained-embeddings.ipynb |  456 +-\n",
      " .../incremental-training-with-layer-freezing.ipynb |  915 ++++\n",
      " examples/usecases/music-streaming.ipynb            |  454 --\n",
      " ...etrieval-with-hyperparameter-optimization.ipynb | 5763 ++++++++++++++++++++\n",
      " examples/usecases/social.ipynb                     |  522 --\n",
      " merlin/datasets/ecommerce/__init__.py              |    9 +-\n",
      " merlin/datasets/ecommerce/dressipi/__init__.py     |   15 +\n",
      " merlin/datasets/ecommerce/dressipi/dataset.py      |  126 +\n",
      " .../ecommerce/dressipi/preprocessed/__init__.py    |   15 +\n",
      " .../ecommerce/dressipi/preprocessed/schema.pbtxt   |  386 ++\n",
      " merlin/datasets/ecommerce/transactions/__init__.py |   15 +\n",
      " .../datasets/ecommerce/transactions/schema.pbtxt   |   65 +\n",
      " merlin/datasets/entertainment/movielens/dataset.py |    2 +-\n",
      " merlin/datasets/synthetic.py                       |    7 +-\n",
      " merlin/models/__init__.py                          |    4 +\n",
      " merlin/models/_version.py                          |   74 +-\n",
      " merlin/models/api.py                               |   76 +\n",
      " merlin/models/io.py                                |   47 +\n",
      " merlin/models/lightfm/__init__.py                  |    4 +-\n",
      " merlin/models/tf/__init__.py                       |  127 +-\n",
      " merlin/models/tf/blocks/dlrm.py                    |   70 +-\n",
      " merlin/models/tf/blocks/interaction.py             |   98 +-\n",
      " merlin/models/tf/blocks/multi_optimizers.py        |  249 -\n",
      " merlin/models/tf/blocks/optimizer.py               |  435 ++\n",
      " merlin/models/tf/blocks/retrieval/base.py          |   33 +-\n",
      " .../tf/blocks/retrieval/matrix_factorization.py    |    6 +-\n",
      " merlin/models/tf/blocks/sampling/cross_batch.py    |   11 +\n",
      " merlin/models/tf/blocks/sampling/in_batch.py       |    6 +\n",
      " merlin/models/tf/core/aggregation.py               |    9 +-\n",
      " merlin/models/tf/core/base.py                      |   10 +\n",
      " merlin/models/tf/core/combinators.py               |  305 +-\n",
      " merlin/models/tf/core/encoder.py                   |  487 ++\n",
      " merlin/models/tf/core/index.py                     |    3 +-\n",
      " merlin/models/tf/core/prediction.py                |   72 +-\n",
      " merlin/models/tf/core/tabular.py                   |   36 +-\n",
      " merlin/models/tf/core/transformations.py           |  811 ---\n",
      " merlin/models/tf/data_augmentation/misc.py         |   50 -\n",
      " merlin/models/tf/inputs/base.py                    |  132 +-\n",
      " merlin/models/tf/inputs/continuous.py              |   13 +\n",
      " merlin/models/tf/inputs/embedding.py               |  495 +-\n",
      " merlin/models/tf/{dataset.py => loader.py}         |   91 +-\n",
      " merlin/models/tf/losses/pairwise.py                |   10 +-\n",
      " merlin/models/tf/metrics/topk.py                   |  199 +-\n",
      " merlin/models/tf/models/base.py                    |  889 ++-\n",
      " merlin/models/tf/models/benchmark.py               |    2 +-\n",
      " merlin/models/tf/models/ranking.py                 |  443 +-\n",
      " merlin/models/tf/models/retrieval.py               |  280 +-\n",
      " .../tf/{data_augmentation => outputs}/__init__.py  |    0\n",
      " merlin/models/tf/outputs/base.py                   |  293 +\n",
      " merlin/models/tf/outputs/classification.py         |  358 ++\n",
      " merlin/models/tf/outputs/contrastive.py            |  377 ++\n",
      " merlin/models/tf/outputs/regression.py             |   56 +\n",
      " .../models/tf/outputs/sampling}/__init__.py        |    0\n",
      " merlin/models/tf/outputs/sampling/base.py          |  176 +\n",
      " merlin/models/tf/outputs/sampling/in_batch.py      |  113 +\n",
      " merlin/models/tf/outputs/sampling/popularity.py    |  114 +\n",
      " merlin/models/tf/outputs/topk.py                   |  305 ++\n",
      " merlin/models/tf/prediction_tasks/base.py          |   84 +-\n",
      " .../models/tf/prediction_tasks/classification.py   |    2 +-\n",
      " merlin/models/tf/prediction_tasks/multi.py         |   20 +\n",
      " merlin/models/tf/prediction_tasks/next_item.py     |   65 +-\n",
      " merlin/models/tf/prediction_tasks/regression.py    |   35 +-\n",
      " merlin/models/tf/prediction_tasks/retrieval.py     |   35 +-\n",
      " merlin/models/tf/transformers/__init__.py          |   15 +\n",
      " merlin/models/tf/transformers/block.py             |  560 ++\n",
      " merlin/models/tf/transformers/transforms.py        |  155 +\n",
      " merlin/models/tf/transforms/__init__.py            |   15 +\n",
      " merlin/models/tf/transforms/bias.py                |  302 +\n",
      " merlin/models/tf/transforms/features.py            |  972 ++++\n",
      " .../negative_sampling.py                           |   35 +-\n",
      " .../tf/{data_augmentation => transforms}/noise.py  |    1 -\n",
      " merlin/models/tf/transforms/regularization.py      |   42 +\n",
      " merlin/models/tf/transforms/sequence.py            |  755 +++\n",
      " merlin/models/tf/transforms/tensor.py              |  284 +\n",
      " merlin/models/tf/utils/batch_utils.py              |   10 +-\n",
      " merlin/models/tf/utils/testing_utils.py            |   72 +-\n",
      " merlin/models/tf/utils/tf_utils.py                 |  119 +-\n",
      " merlin/models/utils/dataset.py                     |   63 +-\n",
      " merlin/models/utils/dependencies.py                |    8 +\n",
      " merlin/models/utils/example_utils.py               |    8 +-\n",
      " merlin/models/utils/schema_utils.py                |   16 +-\n",
      " merlin/models/xgb/__init__.py                      |   98 +-\n",
      " pyproject.toml                                     |    1 +\n",
      " requirements/dev.txt                               |   17 +-\n",
      " requirements/docs.txt                              |   11 +\n",
      " requirements/tensorflow.txt                        |    2 +-\n",
      " requirements/transformers.txt                      |    1 +\n",
      " setup.py                                           |    8 +-\n",
      " tests/common/tf/retrieval/retrieval_config.py      |   83 +-\n",
      " .../common/tf/retrieval/retrieval_tests_common.py  |    8 +\n",
      " tests/common/tf/retrieval/retrieval_utils.py       |  261 +-\n",
      " tests/conftest.py                                  |    4 +-\n",
      " .../tf/retrieval/test_integration_retrieval.py     |    8 +-\n",
      " .../tf/test_ci_03_exploring_different_models.py    |   10 +-\n",
      " .../tf/test_ci_07_xgboost_integration.py           |   20 -\n",
      " tests/unit/datasets/test_ecommerce.py              |    2 +-\n",
      " tests/unit/datasets/test_synthetic.py              |   36 +-\n",
      " tests/unit/lightfm/test_lightfm.py                 |   19 +-\n",
      " tests/unit/tf/blocks/retrieval/test_base.py        |   56 +\n",
      " tests/unit/tf/blocks/retrieval/test_two_tower.py   |   27 +-\n",
      " tests/unit/tf/blocks/sampling/__init__.py          |    0\n",
      " tests/unit/tf/blocks/sampling/test_cross_batch.py  |   40 +\n",
      " tests/unit/tf/blocks/sampling/test_in_batch.py     |   26 +\n",
      " tests/unit/tf/blocks/test_dlrm.py                  |   81 +-\n",
      " tests/unit/tf/blocks/test_interactions.py          |   64 +-\n",
      " tests/unit/tf/blocks/test_multi_optimizers.py      |  273 -\n",
      " tests/unit/tf/blocks/test_optimizer.py             |  634 +++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tests/unit/tf/core/test_combinators.py             |  106 +-\n",
      " tests/unit/tf/core/test_encoder.py                 |  125 +\n",
      " tests/unit/tf/core/test_index.py                   |    7 +-\n",
      " tests/unit/tf/core/test_tabular.py                 |   38 +-\n",
      " tests/unit/tf/core/test_transformations.py         |  544 --\n",
      " tests/unit/tf/data_augmentation/test_misc.py       |   41 -\n",
      " tests/unit/tf/examples/test_02_dataschema.py       |   12 +-\n",
      " .../examples/test_03_exploring_different_models.py |   20 +-\n",
      " .../tf/examples/test_04_export_ranking_models.py   |   10 +-\n",
      " .../tf/examples/test_05_export_retrieval_model.py  |   14 +-\n",
      " .../examples/test_06_advanced_own_architecture.py  |    8 +-\n",
      " .../examples/test_07_train_traditional_models.py   |   23 +\n",
      " ...test_usecase_accelerate_training_by_lazyadam.py |   46 +\n",
      " .../test_usecase_ecommerce_session_based.py        |   59 +\n",
      " ..._usecase_incremental_training_layer_freezing.py |   34 +\n",
      " .../examples/test_usecase_pretrained_embeddings.py |    4 +-\n",
      " .../tf/examples/test_usecase_retrieval_with_hpo.py |   34 +\n",
      " tests/unit/tf/inputs/test_continuous.py            |   29 +\n",
      " tests/unit/tf/inputs/test_embedding.py             |  325 +-\n",
      " tests/unit/tf/inputs/test_tabular.py               |   84 +-\n",
      " tests/unit/tf/metrics/test_metrics_topk.py         |   74 +-\n",
      " tests/unit/tf/models/test_base.py                  |  734 ++-\n",
      " tests/unit/tf/models/test_ranking.py               |  392 +-\n",
      " tests/unit/tf/models/test_retrieval.py             |  608 ++-\n",
      " tests/unit/tf/outputs/__init__.py                  |    0\n",
      " tests/unit/tf/outputs/test_base.py                 |  115 +\n",
      " tests/unit/tf/outputs/test_classification.py       |  106 +\n",
      " tests/unit/tf/outputs/test_contrastive.py          |  264 +\n",
      " tests/unit/tf/outputs/test_regression.py           |   37 +\n",
      " tests/unit/tf/outputs/test_sampling.py             |   78 +\n",
      " tests/unit/tf/outputs/test_topk.py                 |   70 +\n",
      " tests/unit/tf/prediction_tasks/test_multi_task.py  |  115 +-\n",
      " tests/unit/tf/prediction_tasks/test_regression.py  |   25 +-\n",
      " tests/unit/tf/prediction_tasks/test_retrieval.py   |   33 +\n",
      " tests/unit/tf/{test_dataset.py => test_loader.py}  |   47 +-\n",
      " tests/unit/tf/transformers/__init__.py             |   18 +\n",
      " tests/unit/tf/transformers/test_block.py           |  326 ++\n",
      " tests/unit/tf/transformers/test_transforms.py      |   61 +\n",
      " tests/unit/tf/transforms/__init__.py               |   15 +\n",
      " tests/unit/tf/transforms/test_bias.py              |  101 +\n",
      " tests/unit/tf/transforms/test_features.py          | 1012 ++++\n",
      " .../test_negative_sampling.py                      |   65 +-\n",
      " .../test_noise.py                                  |    0\n",
      " tests/unit/tf/transforms/test_sequence.py          |  322 ++\n",
      " tests/unit/tf/transforms/test_tensor.py            |   86 +\n",
      " tests/unit/tf/utils/test_dataset.py                |   71 +\n",
      " tests/unit/tf/utils/test_tf_utils.py               |   60 +\n",
      " tests/unit/torch/features/test_embedding.py        |    1 +\n",
      " tests/unit/xgb/test_xgboost.py                     |   63 +\n",
      " tox.ini                                            |   30 +\n",
      " versioneer.py                                      |  280 +-\n",
      " 182 files changed, 29384 insertions(+), 5743 deletions(-)\n",
      " rename examples/{07-Train-an-xgboost-model-using-the-Merlin-Models-API.ipynb => 07-Train-traditional-ML-models-using-the-Merlin-Models-API.ipynb} (53%)\n",
      " create mode 100644 examples/images/bi-lstm_ecommerce.png\n",
      " create mode 100644 examples/images/mlp_ecommerce.png\n",
      " create mode 100644 examples/images/wide_and_deep.png\n",
      " create mode 100644 examples/usecases/accelerate-training-of-large-embedding-tables-by-LazyAdam.ipynb\n",
      " delete mode 100644 examples/usecases/e-commerce.ipynb\n",
      " create mode 100644 examples/usecases/ecommerce-session-based-next-item-prediction-for-fashion.ipynb\n",
      " create mode 100644 examples/usecases/incremental-training-with-layer-freezing.ipynb\n",
      " delete mode 100644 examples/usecases/music-streaming.ipynb\n",
      " create mode 100644 examples/usecases/retrieval-with-hyperparameter-optimization.ipynb\n",
      " delete mode 100644 examples/usecases/social.ipynb\n",
      " create mode 100644 merlin/datasets/ecommerce/dressipi/__init__.py\n",
      " create mode 100644 merlin/datasets/ecommerce/dressipi/dataset.py\n",
      " create mode 100644 merlin/datasets/ecommerce/dressipi/preprocessed/__init__.py\n",
      " create mode 100644 merlin/datasets/ecommerce/dressipi/preprocessed/schema.pbtxt\n",
      " create mode 100644 merlin/datasets/ecommerce/transactions/__init__.py\n",
      " create mode 100644 merlin/datasets/ecommerce/transactions/schema.pbtxt\n",
      " create mode 100644 merlin/models/api.py\n",
      " create mode 100644 merlin/models/io.py\n",
      " delete mode 100644 merlin/models/tf/blocks/multi_optimizers.py\n",
      " create mode 100644 merlin/models/tf/blocks/optimizer.py\n",
      " create mode 100644 merlin/models/tf/core/encoder.py\n",
      " delete mode 100644 merlin/models/tf/core/transformations.py\n",
      " delete mode 100644 merlin/models/tf/data_augmentation/misc.py\n",
      " rename merlin/models/tf/{dataset.py => loader.py} (88%)\n",
      " rename merlin/models/tf/{data_augmentation => outputs}/__init__.py (100%)\n",
      " create mode 100644 merlin/models/tf/outputs/base.py\n",
      " create mode 100644 merlin/models/tf/outputs/classification.py\n",
      " create mode 100644 merlin/models/tf/outputs/contrastive.py\n",
      " create mode 100644 merlin/models/tf/outputs/regression.py\n",
      " rename {tests/unit/tf/data_augmentation => merlin/models/tf/outputs/sampling}/__init__.py (100%)\n",
      " create mode 100644 merlin/models/tf/outputs/sampling/base.py\n",
      " create mode 100644 merlin/models/tf/outputs/sampling/in_batch.py\n",
      " create mode 100644 merlin/models/tf/outputs/sampling/popularity.py\n",
      " create mode 100644 merlin/models/tf/outputs/topk.py\n",
      " create mode 100644 merlin/models/tf/transformers/__init__.py\n",
      " create mode 100644 merlin/models/tf/transformers/block.py\n",
      " create mode 100644 merlin/models/tf/transformers/transforms.py\n",
      " create mode 100644 merlin/models/tf/transforms/__init__.py\n",
      " create mode 100644 merlin/models/tf/transforms/bias.py\n",
      " create mode 100644 merlin/models/tf/transforms/features.py\n",
      " rename merlin/models/tf/{data_augmentation => transforms}/negative_sampling.py (85%)\n",
      " rename merlin/models/tf/{data_augmentation => transforms}/noise.py (99%)\n",
      " create mode 100644 merlin/models/tf/transforms/regularization.py\n",
      " create mode 100644 merlin/models/tf/transforms/sequence.py\n",
      " create mode 100644 merlin/models/tf/transforms/tensor.py\n",
      " create mode 100644 requirements/docs.txt\n",
      " create mode 100644 requirements/transformers.txt\n",
      " delete mode 100644 tests/integration/tf/test_ci_07_xgboost_integration.py\n",
      " create mode 100644 tests/unit/tf/blocks/retrieval/test_base.py\n",
      " create mode 100644 tests/unit/tf/blocks/sampling/__init__.py\n",
      " create mode 100644 tests/unit/tf/blocks/sampling/test_cross_batch.py\n",
      " create mode 100644 tests/unit/tf/blocks/sampling/test_in_batch.py\n",
      " delete mode 100644 tests/unit/tf/blocks/test_multi_optimizers.py\n",
      " create mode 100644 tests/unit/tf/blocks/test_optimizer.py\n",
      " create mode 100644 tests/unit/tf/core/test_encoder.py\n",
      " delete mode 100644 tests/unit/tf/core/test_transformations.py\n",
      " delete mode 100644 tests/unit/tf/data_augmentation/test_misc.py\n",
      " create mode 100644 tests/unit/tf/examples/test_07_train_traditional_models.py\n",
      " create mode 100644 tests/unit/tf/examples/test_usecase_accelerate_training_by_lazyadam.py\n",
      " create mode 100644 tests/unit/tf/examples/test_usecase_ecommerce_session_based.py\n",
      " create mode 100644 tests/unit/tf/examples/test_usecase_incremental_training_layer_freezing.py\n",
      " create mode 100644 tests/unit/tf/examples/test_usecase_retrieval_with_hpo.py\n",
      " create mode 100644 tests/unit/tf/outputs/__init__.py\n",
      " create mode 100644 tests/unit/tf/outputs/test_base.py\n",
      " create mode 100644 tests/unit/tf/outputs/test_classification.py\n",
      " create mode 100644 tests/unit/tf/outputs/test_contrastive.py\n",
      " create mode 100644 tests/unit/tf/outputs/test_regression.py\n",
      " create mode 100644 tests/unit/tf/outputs/test_sampling.py\n",
      " create mode 100644 tests/unit/tf/outputs/test_topk.py\n",
      " create mode 100644 tests/unit/tf/prediction_tasks/test_retrieval.py\n",
      " rename tests/unit/tf/{test_dataset.py => test_loader.py} (89%)\n",
      " create mode 100644 tests/unit/tf/transformers/__init__.py\n",
      " create mode 100644 tests/unit/tf/transformers/test_block.py\n",
      " create mode 100644 tests/unit/tf/transformers/test_transforms.py\n",
      " create mode 100644 tests/unit/tf/transforms/__init__.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create mode 100644 tests/unit/tf/transforms/test_bias.py\n",
      " create mode 100644 tests/unit/tf/transforms/test_features.py\n",
      " rename tests/unit/tf/{data_augmentation => transforms}/test_negative_sampling.py (79%)\n",
      " rename tests/unit/tf/{data_augmentation => transforms}/test_noise.py (100%)\n",
      " create mode 100644 tests/unit/tf/transforms/test_sequence.py\n",
      " create mode 100644 tests/unit/tf/transforms/test_tensor.py\n",
      " create mode 100644 tests/unit/tf/utils/test_dataset.py\n",
      " create mode 100644 tox.ini\n",
      "Processing /models\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: merlin-core>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-models==0.9.0+29.gada3874d6) (0.5.0+1.g1354dcf)\n",
      "Requirement already satisfied: distributed>=2021.11.2 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (2022.3.0)\n",
      "Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (4.64.0)\n",
      "Requirement already satisfied: betterproto<2.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (1.2.5)\n",
      "Requirement already satisfied: pandas<1.4.0dev0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (1.3.5)\n",
      "Requirement already satisfied: dask>=2021.11.2 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (2022.3.0)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (6.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (21.3)\n",
      "Requirement already satisfied: tensorflow-metadata>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (1.9.0)\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (0.56.0)\n",
      "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (3.19.4)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (5.9.1)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (2.1.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (0.12.0)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (2.2.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (1.0.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (6.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (1.7.0)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (8.1.3)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (2.4.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (6.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (3.1.2)\n",
      "Requirement already satisfied: stringcase in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (1.2.0)\n",
      "Requirement already satisfied: grpclib in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (2022.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from dask>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (2022.7.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (3.0.9)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (1.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (1.56.4)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (45.2.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (0.39.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (4.12.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->distributed>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (2.1.1)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (4.1.0)\n",
      "Requirement already satisfied: multidict in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (6.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (1.14.0)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask>=2021.11.2->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (1.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata; python_version < \"3.9\"->numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (3.8.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (4.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+29.gada3874d6) (6.0.1)\n",
      "Building wheels for collected packages: merlin-models\n",
      "  Building wheel for merlin-models (PEP 517): started\n",
      "  Building wheel for merlin-models (PEP 517): finished with status 'done'\n",
      "  Created wheel for merlin-models: filename=merlin_models-0.9.0+29.gada3874d6-py3-none-any.whl size=356772 sha256=95bb0d6a5c66ec1ec391306ac66218c546a41cc2649430de3a3b86d9c2a5134d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_flhr8cx/wheels/4d/e8/98/0493db55fff90dc9af123f55a9455b96f7f8166c912a02c8a6\n",
      "Successfully built merlin-models\n",
      "Installing collected packages: merlin-models\n",
      "  Attempting uninstall: merlin-models\n",
      "    Found existing installation: merlin-models 0.6.0+8.g042ee198\n",
      "    Uninstalling merlin-models-0.6.0+8.g042ee198:\n",
      "      Successfully uninstalled merlin-models-0.6.0+8.g042ee198\n",
      "Successfully installed merlin-models-0.9.0+29.gada3874d6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/NVIDIA-Merlin/NVTabular\n",
      "   32ed5fa12..8e7edbafd  main               -> origin/main\n",
      " * [new branch]          add_sum_to_supported_aggregations -> origin/add_sum_to_supported_aggregations\n",
      " * [new branch]          bschifferer-remove_examples_1 -> origin/bschifferer-remove_examples_1\n",
      " * [new branch]          fix_data_path      -> origin/fix_data_path\n",
      "   a21889439..6248e0809  gh-pages           -> origin/gh-pages\n",
      " * [new branch]          laiacano/workflow-subgraph -> origin/laiacano/workflow-subgraph\n",
      " + 7538edd60...c665caf12 nvtabular_examples -> origin/nvtabular_examples  (forced update)\n",
      " * [new branch]          release-22.10      -> origin/release-22.10\n",
      " * [new branch]          remove_poetry      -> origin/remove_poetry\n",
      " * [new tag]             v1.6.0             -> v1.6.0\n",
      " * [new tag]             v1.4.0             -> v1.4.0\n",
      " * [new tag]             v1.5.0             -> v1.5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 32ed5fa12..8e7edbafd\n",
      "Fast-forward\n",
      " .github/workflows/cpu-ci.yml                       |   38 +-\n",
      " .github/workflows/docs-sched-rebuild.yaml          |   20 +-\n",
      " .pre-commit-config.yaml                            |   87 +-\n",
      " .pylintrc                                          |    7 +-\n",
      " MANIFEST.in                                        |    2 +-\n",
      " bench/datasets/test_dataset.py                     |    2 +-\n",
      " bench/datasets/tools/train_pytorch.py              |   94 -\n",
      " bench/examples/MultiGPUBench.md                    |    4 +-\n",
      " bench/examples/dataloader_bench.py                 |  107 -\n",
      " bench/examples/gpu_benchmark.ipynb                 |  588 ------\n",
      " bench/torch/criteo/criteo-example-basedl.ipynb     |  321 ---\n",
      " bench/torch/criteo/criteo-example-iterdl.ipynb     |  329 ----\n",
      " bench/torch/criteo/criteo-example-nvtdl.ipynb      |  608 ------\n",
      " bench/torch/criteo/criteo-example-petastorm.ipynb  |  269 ---\n",
      " ci/ignore_codespell_words.txt                      |    2 +\n",
      " ci/test_integration.sh                             |    1 -\n",
      " docs/Makefile                                      |    5 +\n",
      " docs/source/resources/architecture.md              |    2 +-\n",
      " docs/source/toc.yaml                               |   18 -\n",
      " docs/source/training/tensorflow.rst                |    3 +-\n",
      " examples/01-Getting-started.ipynb                  |  765 ++++++++\n",
      " examples/02-Advanced-NVTabular-workflow.ipynb      | 1092 +++++++++++\n",
      " .../03-Running-on-multiple-GPUs-or-on-CPU.ipynb    |  790 ++++++++\n",
      " examples/README.md                                 |   16 +-\n",
      " .../01-Download-Convert.ipynb                      |  426 ----\n",
      " .../02-ETL-with-NVTabular.ipynb                    |  509 -----\n",
      " .../03-Training-with-TF.ipynb                      |  501 -----\n",
      " examples/advanced-ops-outbrain/README.md           |   14 -\n",
      " examples/imgs/dask-dataframe.svg                   |  225 +++\n",
      " examples/multi-gpu-movielens/torch_trainer_dist.py |  149 --\n",
      " .../multi-gpu-toy-example/multi-gpu_dask.ipynb     |    4 +-\n",
      " .../scaling-criteo/02-ETL-with-NVTabular.ipynb     |    2 +-\n",
      " .../scaling-criteo/03-Training-with-FastAI.ipynb   |  378 ----\n",
      " examples/scaling-criteo/README.md                  |    2 +-\n",
      " .../01-Download-Convert.ipynb                      |  796 --------\n",
      " .../02-ETL-with-NVTabular.ipynb                    |  552 ------\n",
      " .../03-Training-with-FastAI.ipynb                  |  658 -------\n",
      " .../03-Training-with-PyTorch.ipynb                 |  507 -----\n",
      " .../03-Training-with-TF.ipynb                      |  569 ------\n",
      " examples/tabular-data-rossmann/README.md           |   14 -\n",
      " examples/tensorflow/TFRecords-To-Parquet.ipynb     |    2 +-\n",
      " examples/tensorflow/using-feature-columns.ipynb    |  969 ----------\n",
      " ...-ETL-with-NVTabular-Training-with-XGBoost.ipynb | 2044 --------------------\n",
      " nvtabular/__init__.py                              |    8 +-\n",
      " nvtabular/_version.py                              |  241 ++-\n",
      " nvtabular/framework_utils/tensorflow/__init__.py   |    2 +-\n",
      " .../tensorflow/feature_column_utils.py             |    2 +-\n",
      " .../framework_utils/tensorflow/layers/__init__.py  |    6 +-\n",
      " .../framework_utils/tensorflow/layers/embedding.py |    2 +-\n",
      " nvtabular/framework_utils/torch/utils.py           |   20 -\n",
      " nvtabular/inference/triton/__init__.py             |    2 +-\n",
      " nvtabular/inference/triton/benchmarking_tools.py   |    2 +-\n",
      " nvtabular/inference/triton/ensemble.py             |   18 +-\n",
      " nvtabular/inference/workflow/hugectr.py            |    2 +-\n",
      " nvtabular/loader/__init__.py                       |    5 +-\n",
      " nvtabular/loader/backend.py                        |  649 +------\n",
      " nvtabular/loader/tensorflow.py                     |  256 +--\n",
      " nvtabular/loader/tf_utils.py                       |    2 +-\n",
      " nvtabular/loader/torch.py                          |  126 +-\n",
      " nvtabular/ops/__init__.py                          |   52 +-\n",
      " nvtabular/ops/add_metadata.py                      |    3 +-\n",
      " nvtabular/ops/bucketize.py                         |    3 +-\n",
      " nvtabular/ops/categorify.py                        |  213 +-\n",
      " nvtabular/ops/clip.py                              |    3 +-\n",
      " nvtabular/ops/column_similarity.py                 |    5 +-\n",
      " nvtabular/ops/data_stats.py                        |    7 +-\n",
      " nvtabular/ops/difference_lag.py                    |    3 +-\n",
      " nvtabular/ops/drop_low_cardinality.py              |    5 +-\n",
      " nvtabular/ops/dropna.py                            |    3 +-\n",
      " nvtabular/ops/fill.py                              |    5 +-\n",
      " nvtabular/ops/filter.py                            |    3 +-\n",
      " nvtabular/ops/groupby.py                           |   14 +-\n",
      " nvtabular/ops/hash_bucket.py                       |    5 +-\n",
      " nvtabular/ops/hashed_cross.py                      |    3 +-\n",
      " nvtabular/ops/join_external.py                     |    3 +-\n",
      " nvtabular/ops/join_groupby.py                      |    7 +-\n",
      " nvtabular/ops/lambdaop.py                          |    3 +-\n",
      " nvtabular/ops/list_slice.py                        |    3 +-\n",
      " nvtabular/ops/logop.py                             |    3 +-\n",
      " nvtabular/ops/moments.py                           |    2 +-\n",
      " nvtabular/ops/normalize.py                         |    7 +-\n",
      " nvtabular/ops/reduce_dtype_size.py                 |  171 +-\n",
      " nvtabular/ops/rename.py                            |    3 +-\n",
      " nvtabular/ops/stat_operator.py                     |    2 +-\n",
      " nvtabular/ops/target_encoding.py                   |   11 +-\n",
      " nvtabular/ops/value_counts.py                      |    5 +-\n",
      " nvtabular/tools/data_gen.py                        |   23 +-\n",
      " nvtabular/workflow/__init__.py                     |    4 +-\n",
      " nvtabular/workflow/workflow.py                     |  246 +--\n",
      " poetry.lock                                        | 1109 -----------\n",
      " pyproject.toml                                     |   10 -\n",
      " requirements-dev.txt                               |   40 -\n",
      " requirements-test.txt                              |    2 +\n",
      " requirements.txt                                   |    1 -\n",
      " requirements/base.txt                              |    3 +\n",
      " requirements/dev.txt                               |   13 +\n",
      " requirements/docs.txt                              |   10 +\n",
      " requirements-gpu.txt => requirements/gpu.txt       |    2 +-\n",
      " requirements/test.txt                              |   36 +\n",
      " setup.py                                           |   39 +-\n",
      " tests/conftest.py                                  |   16 +-\n",
      " .../common/parsers/benchmark_parsers.py            |   17 -\n",
      " tests/integration/common/parsers/criteo_parsers.py |   43 +-\n",
      " .../integration/common/parsers/rossmann_parsers.py |   14 +-\n",
      " tests/integration/test_criteo.py                   |   26 -\n",
      " tests/integration/test_movielens.py                |    5 +-\n",
      " tests/integration/test_nvt_hugectr.py              |    5 +-\n",
      " tests/integration/test_rossman.py                  |  325 ----\n",
      " tests/unit/examples/test_01-Getting-started.py     |   79 +\n",
      " .../test_02-Advanced-NVTabular-workflow.py         |  100 +\n",
      " .../test_03-Running-on-multiple-GPUs-or-on-CPU.py  |   59 +\n",
      " tests/unit/loader/test_dataloader_backend.py       |  157 --\n",
      " tests/unit/loader/test_tf_dataloader.py            |    5 +-\n",
      " tests/unit/loader/test_torch_dataloader.py         |  177 +-\n",
      " tests/unit/ops/test_categorify.py                  |   43 +\n",
      " tests/unit/ops/test_groupyby.py                    |   33 +-\n",
      " tests/unit/ops/test_ops_schema.py                  |    6 +-\n",
      " tests/unit/ops/test_reduce_dtype_size.py           |    6 +-\n",
      " tests/unit/test_notebooks.py                       |  182 --\n",
      " tests/unit/test_triton_inference.py                |    2 +-\n",
      " tox.ini                                            |   91 +\n",
      " versioneer.py                                      |  706 +++++--\n",
      " 122 files changed, 4651 insertions(+), 14365 deletions(-)\n",
      " delete mode 100755 bench/datasets/tools/train_pytorch.py\n",
      " delete mode 100644 bench/examples/dataloader_bench.py\n",
      " delete mode 100644 bench/examples/gpu_benchmark.ipynb\n",
      " delete mode 100644 bench/torch/criteo/criteo-example-basedl.ipynb\n",
      " delete mode 100644 bench/torch/criteo/criteo-example-iterdl.ipynb\n",
      " delete mode 100644 bench/torch/criteo/criteo-example-nvtdl.ipynb\n",
      " delete mode 100644 bench/torch/criteo/criteo-example-petastorm.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create mode 100644 examples/01-Getting-started.ipynb\n",
      " create mode 100644 examples/02-Advanced-NVTabular-workflow.ipynb\n",
      " create mode 100644 examples/03-Running-on-multiple-GPUs-or-on-CPU.ipynb\n",
      " delete mode 100644 examples/advanced-ops-outbrain/01-Download-Convert.ipynb\n",
      " delete mode 100644 examples/advanced-ops-outbrain/02-ETL-with-NVTabular.ipynb\n",
      " delete mode 100644 examples/advanced-ops-outbrain/03-Training-with-TF.ipynb\n",
      " delete mode 100644 examples/advanced-ops-outbrain/README.md\n",
      " create mode 100644 examples/imgs/dask-dataframe.svg\n",
      " delete mode 100644 examples/multi-gpu-movielens/torch_trainer_dist.py\n",
      " delete mode 100644 examples/scaling-criteo/03-Training-with-FastAI.ipynb\n",
      " delete mode 100755 examples/tabular-data-rossmann/01-Download-Convert.ipynb\n",
      " delete mode 100755 examples/tabular-data-rossmann/02-ETL-with-NVTabular.ipynb\n",
      " delete mode 100755 examples/tabular-data-rossmann/03-Training-with-FastAI.ipynb\n",
      " delete mode 100755 examples/tabular-data-rossmann/03-Training-with-PyTorch.ipynb\n",
      " delete mode 100755 examples/tabular-data-rossmann/03-Training-with-TF.ipynb\n",
      " delete mode 100644 examples/tabular-data-rossmann/README.md\n",
      " delete mode 100644 examples/tensorflow/using-feature-columns.ipynb\n",
      " delete mode 100644 examples/winning-solution-recsys2020-twitter/01-02-04-Download-Convert-ETL-with-NVTabular-Training-with-XGBoost.ipynb\n",
      " delete mode 100644 poetry.lock\n",
      " delete mode 100644 requirements-dev.txt\n",
      " create mode 100644 requirements-test.txt\n",
      " delete mode 100644 requirements.txt\n",
      " create mode 100644 requirements/base.txt\n",
      " create mode 100644 requirements/dev.txt\n",
      " create mode 100644 requirements/docs.txt\n",
      " rename requirements-gpu.txt => requirements/gpu.txt (72%)\n",
      " create mode 100644 requirements/test.txt\n",
      " delete mode 100644 tests/integration/test_rossman.py\n",
      " create mode 100644 tests/unit/examples/test_01-Getting-started.py\n",
      " create mode 100644 tests/unit/examples/test_02-Advanced-NVTabular-workflow.py\n",
      " create mode 100644 tests/unit/examples/test_03-Running-on-multiple-GPUs-or-on-CPU.py\n",
      " delete mode 100644 tests/unit/loader/test_dataloader_backend.py\n",
      " create mode 100644 tox.ini\n",
      "Processing /nvtabular\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting merlin-dataloader>=0.0.2\n",
      "  Downloading merlin-dataloader-0.0.2.tar.gz (44 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from nvtabular==1.6.0+3.g8e7edbafd) (1.8.1)\n",
      "Requirement already satisfied: merlin-core>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from nvtabular==1.6.0+3.g8e7edbafd) (0.5.0+1.g1354dcf)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scipy->nvtabular==1.6.0+3.g8e7edbafd) (1.21.5)\n",
      "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (3.19.4)\n",
      "Requirement already satisfied: betterproto<2.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (1.2.5)\n",
      "Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (4.64.0)\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (0.56.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (21.3)\n",
      "Requirement already satisfied: dask>=2021.11.2 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (2022.3.0)\n",
      "Requirement already satisfied: tensorflow-metadata>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (1.9.0)\n",
      "Requirement already satisfied: distributed>=2021.11.2 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (2022.3.0)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (6.0.0)\n",
      "Requirement already satisfied: pandas<1.4.0dev0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (1.3.5)\n",
      "Requirement already satisfied: stringcase in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (1.2.0)\n",
      "Requirement already satisfied: grpclib in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (0.4.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (4.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from numba>=0.54->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (45.2.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (0.39.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (3.0.9)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from dask>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (2.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from dask>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (6.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (0.12.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from dask>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (2022.7.0)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (1.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (1.56.4)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (1.0.4)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (8.1.3)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (6.2)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (2.2.0)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (5.9.1)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (2.4.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (3.1.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (1.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (2022.1)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (4.1.0)\n",
      "Requirement already satisfied: multidict in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (6.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata; python_version < \"3.9\"->numba>=0.54->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (3.8.1)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->distributed>=2021.11.2->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (1.14.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.6.0+3.g8e7edbafd) (4.0.0)\n",
      "Building wheels for collected packages: nvtabular, merlin-dataloader\n",
      "  Building wheel for nvtabular (PEP 517): started\n",
      "  Building wheel for nvtabular (PEP 517): finished with status 'done'\n",
      "  Created wheel for nvtabular: filename=nvtabular-1.6.0+3.g8e7edbafd-cp38-cp38-linux_x86_64.whl size=257744 sha256=c8f4b0f97d1b07c93b854c889f24c475d2957332c97abef19ca59ba4b971efd9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_d_560nq/wheels/df/bf/c2/9cc2a62fe6da42038c26a9c0c4e25f9767093528b102fa30a2\n",
      "  Building wheel for merlin-dataloader (PEP 517): started\n",
      "  Building wheel for merlin-dataloader (PEP 517): finished with status 'done'\n",
      "  Created wheel for merlin-dataloader: filename=merlin_dataloader-0.0.2-py3-none-any.whl size=29204 sha256=80e3c0fdc7b4c03c92737072e71ef93263f83584512fe01c8de1a15c1b31414c\n",
      "  Stored in directory: /root/.cache/pip/wheels/d5/ce/8c/31476c01e0b5a2278110fe2092bdd911efb0e5b83d0d3550ca\n",
      "Successfully built nvtabular merlin-dataloader\n",
      "Installing collected packages: merlin-dataloader, nvtabular\n",
      "  Attempting uninstall: nvtabular\n",
      "    Found existing installation: nvtabular 1.3.3+1.g32ed5fa12\n",
      "    Uninstalling nvtabular-1.3.3+1.g32ed5fa12:\n",
      "      Successfully uninstalled nvtabular-1.3.3+1.g32ed5fa12\n",
      "Successfully installed merlin-dataloader-0.0.2 nvtabular-1.6.0+3.g8e7edbafd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/NVIDIA-Merlin/core\n",
      "   1354dcf..563be4b  main                -> origin/main\n",
      " * [new branch]      fix-with-properties -> origin/fix-with-properties\n",
      " * [new branch]      fix_versioneer      -> origin/fix_versioneer\n",
      "   331e11f..971dbe5  gh-pages            -> origin/gh-pages\n",
      " * [new branch]      laiacano/subgraph   -> origin/laiacano/subgraph\n",
      " * [new branch]      release-22.10       -> origin/release-22.10\n",
      "   af238d2..b48439a  tags-intersection   -> origin/tags-intersection\n",
      " * [new tag]         v0.8.0              -> v0.8.0\n",
      " * [new tag]         v0.6.0              -> v0.6.0\n",
      " * [new tag]         v0.7.0              -> v0.7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 1354dcf..563be4b\n",
      "Fast-forward\n",
      " .github/workflows/cpu-ci.yml              |  46 ++--\n",
      " .github/workflows/cpu-models.yml          |  37 +++\n",
      " .github/workflows/cpu-nvtabular.yml       |  37 +++\n",
      " .github/workflows/cpu-systems.yml         |  37 +++\n",
      " .github/workflows/docs-sched-rebuild.yaml |  14 +-\n",
      " .pre-commit-config.yaml                   |  14 +-\n",
      " merlin/core/__init__.py                   |   5 +-\n",
      " merlin/core/_version.py                   |  29 ++-\n",
      " merlin/core/compat.py                     |  16 +-\n",
      " merlin/core/dispatch.py                   |  31 ++-\n",
      " merlin/core/protocols.py                  | 197 ++++++++++++++++\n",
      " merlin/dag/__init__.py                    |   1 +\n",
      " merlin/dag/base_operator.py               | 108 ++++++++-\n",
      " merlin/dag/dictarray.py                   | 121 ++++++++++\n",
      " merlin/dag/executors.py                   | 325 +++++++++++++++++++++++++++\n",
      " merlin/dag/graph.py                       |  18 +-\n",
      " merlin/dag/node.py                        |  60 ++++-\n",
      " merlin/dag/ops/concat_columns.py          |  19 +-\n",
      " merlin/dag/ops/selection.py               |  12 +-\n",
      " merlin/dag/ops/subset_columns.py          |  10 +-\n",
      " merlin/dag/ops/subtraction.py             |  19 +-\n",
      " merlin/dag/selector.py                    |  42 +++-\n",
      " merlin/schema/io/tensorflow_metadata.py   |   7 +-\n",
      " merlin/schema/schema.py                   |  49 +++-\n",
      " merlin/schema/tags.py                     |  43 +++-\n",
      " pyproject.toml                            |   2 +-\n",
      " requirements-dev.txt                      |  21 --\n",
      " requirements-docs.txt                     |  14 ++\n",
      " requirements-gpu.txt                      |   6 +\n",
      " requirements-test-cpu.txt                 |   2 +\n",
      " requirements-test-gpu.txt                 |   2 +\n",
      " requirements-test.txt                     |  19 ++\n",
      " requirements.txt                          |   5 +-\n",
      " setup.cfg                                 |   2 +-\n",
      " setup.py                                  |   1 +\n",
      " tests/unit/core/test_dispatch.py          |   2 +\n",
      " tests/unit/core/test_protocols.py         |  41 ++++\n",
      " tests/unit/core/test_version.py           |  22 ++\n",
      " tests/unit/dag/ops/test_selection.py      |  11 +\n",
      " tests/unit/dag/test_column_selector.py    |  40 ++++\n",
      " tests/unit/dag/test_dictarray.py          |  34 +++\n",
      " tests/unit/dag/test_executors.py          |  57 +++++\n",
      " tests/unit/dag/test_graph.py              |  73 ++++++\n",
      " tests/unit/schema/test_column_schemas.py  | 361 +++++++++++++-----------------\n",
      " tests/unit/schema/test_schema.py          |  77 +++++++\n",
      " tests/unit/schema/test_schema_io.py       | 135 ++++++++++-\n",
      " tests/unit/{dag => schema}/test_tags.py   |  10 +-\n",
      " tox.ini                                   | 141 ++++++++++++\n",
      " versioneer.py                             | 186 +++++++++------\n",
      " 49 files changed, 2125 insertions(+), 436 deletions(-)\n",
      " create mode 100644 .github/workflows/cpu-models.yml\n",
      " create mode 100644 .github/workflows/cpu-nvtabular.yml\n",
      " create mode 100644 .github/workflows/cpu-systems.yml\n",
      " create mode 100644 merlin/core/protocols.py\n",
      " create mode 100644 merlin/dag/dictarray.py\n",
      " create mode 100644 merlin/dag/executors.py\n",
      " create mode 100644 requirements-docs.txt\n",
      " create mode 100644 requirements-gpu.txt\n",
      " create mode 100644 requirements-test-cpu.txt\n",
      " create mode 100644 requirements-test-gpu.txt\n",
      " create mode 100644 requirements-test.txt\n",
      " create mode 100644 tests/unit/core/test_protocols.py\n",
      " create mode 100644 tests/unit/core/test_version.py\n",
      " create mode 100644 tests/unit/dag/test_dictarray.py\n",
      " create mode 100644 tests/unit/dag/test_executors.py\n",
      " rename tests/unit/{dag => schema}/test_tags.py (92%)\n",
      " create mode 100644 tox.ini\n",
      "Processing /core\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: tensorflow-metadata>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.8.0+5.g563be4b) (1.9.0)\n",
      "Requirement already satisfied: pandas<1.4.0dev0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.8.0+5.g563be4b) (1.3.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.8.0+5.g563be4b) (21.3)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.8.0+5.g563be4b) (6.0.0)\n",
      "Requirement already satisfied: distributed>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.8.0+5.g563be4b) (2022.3.0)\n",
      "Requirement already satisfied: betterproto<2.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.8.0+5.g563be4b) (1.2.5)\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.8.0+5.g563be4b) (0.56.0)\n",
      "Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.8.0+5.g563be4b) (4.64.0)\n",
      "Requirement already satisfied: dask>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.8.0+5.g563be4b) (2022.3.0)\n",
      "Collecting fsspec==2022.5.0\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.8.0+5.g563be4b) (3.19.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core==0.8.0+5.g563be4b) (1.56.4)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core==0.8.0+5.g563be4b) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core==0.8.0+5.g563be4b) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core==0.8.0+5.g563be4b) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core==0.8.0+5.g563be4b) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->merlin-core==0.8.0+5.g563be4b) (3.0.9)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (5.9.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (3.1.2)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (2.1.0)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (8.1.3)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (2.4.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (1.0.4)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (2.2.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (0.12.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (6.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (6.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (1.7.0)\n",
      "Requirement already satisfied: stringcase in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core==0.8.0+5.g563be4b) (1.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: grpclib in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core==0.8.0+5.g563be4b) (0.4.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core==0.8.0+5.g563be4b) (4.12.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core==0.8.0+5.g563be4b) (0.39.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from numba>=0.54->merlin-core==0.8.0+5.g563be4b) (45.2.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas<1.4.0dev0,>=1.2.0->merlin-core==0.8.0+5.g563be4b) (1.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->distributed>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (2.1.1)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (1.0.1)\n",
      "Requirement already satisfied: multidict in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core==0.8.0+5.g563be4b) (6.0.2)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core==0.8.0+5.g563be4b) (4.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata; python_version < \"3.9\"->numba>=0.54->merlin-core==0.8.0+5.g563be4b) (3.8.1)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask>=2022.3.0->merlin-core==0.8.0+5.g563be4b) (1.0.0)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core==0.8.0+5.g563be4b) (4.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core==0.8.0+5.g563be4b) (6.0.1)\n",
      "Building wheels for collected packages: merlin-core\n",
      "  Building wheel for merlin-core (PEP 517): started\n",
      "  Building wheel for merlin-core (PEP 517): finished with status 'done'\n",
      "  Created wheel for merlin-core: filename=merlin_core-0.8.0+5.g563be4b-py3-none-any.whl size=118346 sha256=72e0fef89baba9c555ed1b7c0507b35c580e17ab30ff47b8058762183027a37b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tlmeid_0/wheels/8f/da/8c/c779661788874afaa32fd10abeac6016635956e3bad9940584\n",
      "Successfully built merlin-core\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: dask-cudf 22.4.0 requires cupy-cuda117, which is not installed.\n",
      "ERROR: cudf 22.4.0 requires cupy-cuda117, which is not installed.\n",
      "ERROR: dask-cuda 22.4.0 has requirement click==8.0.4, but you'll have click 8.1.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: fsspec, merlin-core\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.0\n",
      "    Uninstalling fsspec-2022.7.0:\n",
      "      Successfully uninstalled fsspec-2022.7.0\n",
      "  Attempting uninstall: merlin-core\n",
      "    Found existing installation: merlin-core 0.5.0+1.g1354dcf\n",
      "    Uninstalling merlin-core-0.5.0+1.g1354dcf:\n",
      "      Successfully uninstalled merlin-core-0.5.0+1.g1354dcf\n",
      "Successfully installed fsspec-2022.5.0 merlin-core-0.8.0+5.g563be4b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/NVIDIA-Merlin/systems\n",
      "   f1e2bf5a..d2737da6 main                    -> origin/main\n",
      " * [new branch]      add_implicit_example    -> origin/add_implicit_example\n",
      " * [new branch]      add_xgboost_serving_example -> origin/add_xgboost_serving_example\n",
      " * [new branch]      docs-tox                -> origin/docs-tox\n",
      " + 3a5a6026...a99e7a54 feature/pytorch         -> origin/feature/pytorch  (forced update)\n",
      " * [new branch]      feature/t4r-serving     -> origin/feature/t4r-serving\n",
      " * [new branch]      feature/torchscript     -> origin/feature/torchscript\n",
      " * [new branch]      fix/multihot-schemas    -> origin/fix/multihot-schemas\n",
      " * [new branch]      fix/pytest-feast        -> origin/fix/pytest-feast\n",
      " * [new branch]      fix/tf-input-shapes     -> origin/fix/tf-input-shapes\n",
      " * [new branch]      fix/torch-importorskip  -> origin/fix/torch-importorskip\n",
      "   ab3ac95d..803699dc gh-pages                -> origin/gh-pages\n",
      " * [new branch]      laiacano/feast-tests    -> origin/laiacano/feast-tests\n",
      " * [new branch]      laiacano/softmax-multiple-output -> origin/laiacano/softmax-multiple-output\n",
      " * [new branch]      laiacano/update-tests   -> origin/laiacano/update-tests\n",
      " * [new branch]      laiacano/upgrade-feast  -> origin/laiacano/upgrade-feast\n",
      "   9910bfe0..8898b039 merlin_models_xgboost   -> origin/merlin_models_xgboost\n",
      " * [new branch]      refactor/dtypes         -> origin/refactor/dtypes\n",
      " * [new branch]      refactor/organize-tests -> origin/refactor/organize-tests\n",
      "   5c2987f9..73139e8b refactor/schema-validation-hook -> origin/refactor/schema-validation-hook\n",
      " * [new branch]      release-22.10           -> origin/release-22.10\n",
      " * [new tag]         v0.7.0                  -> v0.7.0\n",
      " * [new tag]           v0.5.0                  -> v0.5.0\n",
      " * [new tag]           v0.6.0                  -> v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating f1e2bf5a..d2737da6\n",
      "Fast-forward\n",
      " .github/workflows/cpu-ci.yml                       |  39 +-\n",
      " .github/workflows/docs-sched-rebuild.yaml          |  12 +-\n",
      " .github/workflows/fil.yml                          |  42 --\n",
      " .pre-commit-config.yaml                            |   2 +-\n",
      " .pylintrc                                          |   5 +-\n",
      " MANIFEST.in                                        |   3 +-\n",
      " ci/ignore_codespell_words.txt                      |   1 +\n",
      " ci/test_unit.sh                                    |   3 +-\n",
      " docs/Makefile                                      |   4 +\n",
      " docs/README.md                                     |   2 +-\n",
      " docs/source/api.rst                                |   1 -\n",
      " docs/source/toc.yaml                               |   1 +\n",
      " ...ing-An-Implicit-Model-With-Merlin-Systems.ipynb | 486 ++++++++++++++++++\n",
      " ...ving-An-XGboost-Model-With-Merlin-Systems.ipynb | 542 +++++++++++++++++++++\n",
      " ...erving-Ranking-Models-With-Merlin-Systems.ipynb |   8 +-\n",
      " merlin/systems/__init__.py                         |   5 +-\n",
      " merlin/systems/_version.py                         |  34 +-\n",
      " merlin/systems/dag/ensemble.py                     | 194 ++++----\n",
      " merlin/systems/dag/node.py                         |  52 +-\n",
      " merlin/systems/dag/op_runner.py                    |  31 +-\n",
      " merlin/systems/dag/ops/__init__.py                 |  29 ++\n",
      " merlin/systems/dag/ops/faiss.py                    |  81 ++-\n",
      " merlin/systems/dag/ops/feast.py                    |  49 +-\n",
      " merlin/systems/dag/ops/fil.py                      |  76 ++-\n",
      " merlin/systems/dag/ops/implicit.py                 | 181 +++++++\n",
      " merlin/systems/dag/ops/operator.py                 | 151 +++---\n",
      " merlin/systems/dag/ops/pytorch.py                  | 243 +++++++++\n",
      " merlin/systems/dag/ops/session_filter.py           |  63 +--\n",
      " merlin/systems/dag/ops/softmax_sampling.py         |  32 +-\n",
      " merlin/systems/dag/ops/tensorflow.py               | 190 ++++----\n",
      " merlin/systems/dag/ops/unroll_features.py          |  26 +-\n",
      " merlin/systems/dag/ops/workflow.py                 | 131 ++++-\n",
      " merlin/systems/dag/runtimes/__init__.py            |  18 +\n",
      " merlin/systems/dag/runtimes/base_runtime.py        |  59 +++\n",
      " merlin/systems/dag/runtimes/triton/__init__.py     |  20 +\n",
      " merlin/systems/dag/runtimes/triton/ops/__init__.py |  15 +\n",
      " merlin/systems/dag/runtimes/triton/ops/operator.py |  55 +++\n",
      " .../systems/dag/runtimes/triton/ops/tensorflow.py  | 165 +++++++\n",
      " merlin/systems/dag/runtimes/triton/runtime.py      | 347 +++++++++++++\n",
      " merlin/systems/triton/__init__.py                  | 105 +++-\n",
      " merlin/systems/triton/export.py                    |  33 +-\n",
      " merlin/systems/triton/models/__init__.py           |  15 +\n",
      " merlin/systems/triton/models/executor_model.py     | 141 ++++++\n",
      " .../systems/triton/{ => models}/oprunner_model.py  |  34 +-\n",
      " merlin/systems/triton/models/pytorch_model.py      | 223 +++++++++\n",
      " .../systems/triton/{ => models}/workflow_model.py  |   0\n",
      " merlin/systems/triton/utils.py                     |  99 +++-\n",
      " pytest.ini                                         |   4 +\n",
      " requirements-dev.txt                               |  38 --\n",
      " requirements-gpu.txt                               |   5 -\n",
      " requirements.txt => requirements/base.txt          |   1 +\n",
      " requirements/dev.txt                               |  14 +\n",
      " requirements/docs.txt                              |  17 +\n",
      " requirements/gpu.txt                               |   6 +\n",
      " requirements/test-cpu.txt                          |  10 +\n",
      " requirements/test-gpu.txt                          |   4 +\n",
      " requirements/test.txt                              |  22 +\n",
      " setup.cfg                                          |   2 +-\n",
      " setup.py                                           |  41 +-\n",
      " tests/conftest.py                                  |   4 -\n",
      " tests/integration/t4r/test_pytorch_backend.py      | 166 +++++++\n",
      " ...erving_an_implicit_model_with_merlin_systems.py |  59 +++\n",
      " ...serving_an_xgboost_model_with_merlin_systems.py |  50 ++\n",
      " ...t_serving_ranking_models_with_merlin_systems.py |  48 +-\n",
      " tests/unit/systems/dag/ops/__init__.py             |   0\n",
      " .../{test_ensemble_ops.py => dag/ops/test_ops.py}  |  21 +-\n",
      " tests/unit/systems/dag/runtimes/__init__.py        |  15 +\n",
      " tests/unit/systems/dag/runtimes/test_triton.py     |  79 +++\n",
      " tests/unit/systems/dag/test_ensemble.py            |  56 +++\n",
      " tests/unit/systems/dag/test_executors.py           | 128 +++++\n",
      " tests/unit/systems/{ => dag}/test_graph.py         |  23 +-\n",
      " tests/unit/systems/dag/test_model_registry.py      |  59 +++\n",
      " tests/unit/systems/{ => dag}/test_op_runner.py     |  54 +-\n",
      " tests/unit/systems/ops/__init__.py                 |  15 +\n",
      " tests/unit/systems/ops/faiss/test_executor.py      |  90 ++++\n",
      " tests/unit/systems/ops/feast/__init__.py           |  19 +\n",
      " tests/unit/systems/ops/feast/test_op.py            | 221 +++++++++\n",
      " tests/unit/systems/{ => ops}/fil/__init__.py       |   0\n",
      " tests/unit/systems/ops/fil/test_ensemble.py        |  80 +++\n",
      " tests/unit/systems/{ => ops}/fil/test_forest.py    |  46 +-\n",
      " .../{fil/test_fil.py => ops/fil/test_op.py}        |   5 +-\n",
      " tests/unit/systems/ops/implicit/__init__.py        |  19 +\n",
      " tests/unit/systems/ops/implicit/test_executor.py   |  66 +++\n",
      " tests/unit/systems/ops/implicit/test_op.py         | 128 +++++\n",
      " tests/unit/systems/ops/nvtabular/__init__.py       |  19 +\n",
      " tests/unit/systems/ops/nvtabular/test_ensemble.py  |  81 +++\n",
      " .../nvtabular/test_op.py}                          |  26 +-\n",
      " tests/unit/systems/ops/tf/__init__.py              |  19 +\n",
      " tests/unit/systems/{ => ops/tf}/test_ensemble.py   | 119 ++---\n",
      " .../tf/test_op.py}                                 |  51 +-\n",
      " tests/unit/systems/ops/torch/__init__.py           |  19 +\n",
      " tests/unit/systems/ops/torch/test_op.py            | 234 +++++++++\n",
      " tests/unit/systems/test_model_registry.py          |  16 -\n",
      " tests/unit/systems/utils/ops.py                    |  18 +-\n",
      " tests/unit/systems/utils/triton.py                 |  42 --\n",
      " tests/unit/{systems => }/test_export.py            |  12 +-\n",
      " tests/{unit => version}/test_version.py            |   5 +-\n",
      " tox.ini                                            |  93 ++++\n",
      " versioneer.py                                      | 191 +++++---\n",
      " 99 files changed, 5728 insertions(+), 880 deletions(-)\n",
      " delete mode 100644 .github/workflows/fil.yml\n",
      " create mode 100644 examples/Serving-An-Implicit-Model-With-Merlin-Systems.ipynb\n",
      " create mode 100644 examples/Serving-An-XGboost-Model-With-Merlin-Systems.ipynb\n",
      " create mode 100644 merlin/systems/dag/ops/implicit.py\n",
      " create mode 100644 merlin/systems/dag/ops/pytorch.py\n",
      " create mode 100644 merlin/systems/dag/runtimes/__init__.py\n",
      " create mode 100644 merlin/systems/dag/runtimes/base_runtime.py\n",
      " create mode 100644 merlin/systems/dag/runtimes/triton/__init__.py\n",
      " create mode 100644 merlin/systems/dag/runtimes/triton/ops/__init__.py\n",
      " create mode 100644 merlin/systems/dag/runtimes/triton/ops/operator.py\n",
      " create mode 100644 merlin/systems/dag/runtimes/triton/ops/tensorflow.py\n",
      " create mode 100644 merlin/systems/dag/runtimes/triton/runtime.py\n",
      " create mode 100644 merlin/systems/triton/models/__init__.py\n",
      " create mode 100644 merlin/systems/triton/models/executor_model.py\n",
      " rename merlin/systems/triton/{ => models}/oprunner_model.py (80%)\n",
      " create mode 100644 merlin/systems/triton/models/pytorch_model.py\n",
      " rename merlin/systems/triton/{ => models}/workflow_model.py (100%)\n",
      " create mode 100644 pytest.ini\n",
      " delete mode 100644 requirements-dev.txt\n",
      " delete mode 100644 requirements-gpu.txt\n",
      " rename requirements.txt => requirements/base.txt (66%)\n",
      " create mode 100644 requirements/dev.txt\n",
      " create mode 100644 requirements/docs.txt\n",
      " create mode 100644 requirements/gpu.txt\n",
      " create mode 100644 requirements/test-cpu.txt\n",
      " create mode 100644 requirements/test-gpu.txt\n",
      " create mode 100644 requirements/test.txt\n",
      " create mode 100644 tests/integration/t4r/test_pytorch_backend.py\n",
      " create mode 100644 tests/unit/examples/test_serving_an_implicit_model_with_merlin_systems.py\n",
      " create mode 100644 tests/unit/examples/test_serving_an_xgboost_model_with_merlin_systems.py\n",
      " create mode 100644 tests/unit/systems/dag/ops/__init__.py\n",
      " rename tests/unit/systems/{test_ensemble_ops.py => dag/ops/test_ops.py} (80%)\n",
      " create mode 100644 tests/unit/systems/dag/runtimes/__init__.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create mode 100644 tests/unit/systems/dag/runtimes/test_triton.py\n",
      " create mode 100644 tests/unit/systems/dag/test_ensemble.py\n",
      " create mode 100644 tests/unit/systems/dag/test_executors.py\n",
      " rename tests/unit/systems/{ => dag}/test_graph.py (59%)\n",
      " create mode 100644 tests/unit/systems/dag/test_model_registry.py\n",
      " rename tests/unit/systems/{ => dag}/test_op_runner.py (75%)\n",
      " create mode 100644 tests/unit/systems/ops/__init__.py\n",
      " create mode 100644 tests/unit/systems/ops/faiss/test_executor.py\n",
      " create mode 100644 tests/unit/systems/ops/feast/__init__.py\n",
      " create mode 100644 tests/unit/systems/ops/feast/test_op.py\n",
      " rename tests/unit/systems/{ => ops}/fil/__init__.py (100%)\n",
      " create mode 100644 tests/unit/systems/ops/fil/test_ensemble.py\n",
      " rename tests/unit/systems/{ => ops}/fil/test_forest.py (74%)\n",
      " rename tests/unit/systems/{fil/test_fil.py => ops/fil/test_op.py} (98%)\n",
      " create mode 100644 tests/unit/systems/ops/implicit/__init__.py\n",
      " create mode 100644 tests/unit/systems/ops/implicit/test_executor.py\n",
      " create mode 100644 tests/unit/systems/ops/implicit/test_op.py\n",
      " create mode 100644 tests/unit/systems/ops/nvtabular/__init__.py\n",
      " create mode 100644 tests/unit/systems/ops/nvtabular/test_ensemble.py\n",
      " rename tests/unit/systems/{test_inference_ops.py => ops/nvtabular/test_op.py} (73%)\n",
      " create mode 100644 tests/unit/systems/ops/tf/__init__.py\n",
      " rename tests/unit/systems/{ => ops/tf}/test_ensemble.py (67%)\n",
      " rename tests/unit/systems/{test_tensorflow_inf_op.py => ops/tf/test_op.py} (69%)\n",
      " create mode 100644 tests/unit/systems/ops/torch/__init__.py\n",
      " create mode 100644 tests/unit/systems/ops/torch/test_op.py\n",
      " delete mode 100644 tests/unit/systems/test_model_registry.py\n",
      " delete mode 100644 tests/unit/systems/utils/triton.py\n",
      " rename tests/unit/{systems => }/test_export.py (88%)\n",
      " rename tests/{unit => version}/test_version.py (82%)\n",
      " create mode 100644 tox.ini\n",
      "Processing /systems\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: nvtabular>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-systems==0.7.0+10.gd2737da6) (1.6.0+3.g8e7edbafd)\n",
      "Requirement already satisfied: requests<3,>=2.10 in /usr/lib/python3/dist-packages (from merlin-systems==0.7.0+10.gd2737da6) (2.22.0)\n",
      "Requirement already satisfied: merlin-core>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-systems==0.7.0+10.gd2737da6) (0.8.0+5.g563be4b)\n",
      "Requirement already satisfied: merlin-dataloader>=0.0.2 in /usr/local/lib/python3.8/dist-packages (from nvtabular>=1.0.0->merlin-systems==0.7.0+10.gd2737da6) (0.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from nvtabular>=1.0.0->merlin-systems==0.7.0+10.gd2737da6) (1.8.1)\n",
      "Requirement already satisfied: dask>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (2022.3.0)\n",
      "Requirement already satisfied: fsspec==2022.5.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (2022.5.0)\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (0.56.0)\n",
      "Requirement already satisfied: betterproto<2.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (1.2.5)\n",
      "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (3.19.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (21.3)\n",
      "Requirement already satisfied: tensorflow-metadata>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (1.9.0)\n",
      "Requirement already satisfied: distributed>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (2022.3.0)\n",
      "Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (4.64.0)\n",
      "Requirement already satisfied: pandas<1.4.0dev0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (1.3.5)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (6.0.0)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scipy->nvtabular>=1.0.0->merlin-systems==0.7.0+10.gd2737da6) (1.21.5)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (1.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (6.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (2.1.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (0.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (45.2.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (0.39.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (4.12.0)\n",
      "Requirement already satisfied: grpclib in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (0.4.2)\n",
      "Requirement already satisfied: stringcase in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (3.0.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (1.56.4)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (2.4.0)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (8.1.3)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (6.2)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (1.7.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (1.0.4)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (5.9.1)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (2.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (2022.1)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (1.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata; python_version < \"3.9\"->numba>=0.54->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (3.8.1)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (4.1.0)\n",
      "Requirement already satisfied: multidict in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (6.0.2)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->distributed>=2022.3.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (1.14.0)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (4.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-systems==0.7.0+10.gd2737da6) (6.0.1)\n",
      "Building wheels for collected packages: merlin-systems\n",
      "  Building wheel for merlin-systems (PEP 517): started\n",
      "  Building wheel for merlin-systems (PEP 517): finished with status 'done'\n",
      "  Created wheel for merlin-systems: filename=merlin_systems-0.7.0+10.gd2737da6-py3-none-any.whl size=91050 sha256=511c153f0873db67dba727a9ceb276870f1dda9545a960ef2f095076a4786ff1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_mk8a0g9/wheels/1f/e9/71/1b0c6295aa7f4b37cb70292d96d87d9f38204674e6531bdda6\n",
      "Successfully built merlin-systems\n",
      "Installing collected packages: merlin-systems\n",
      "  Attempting uninstall: merlin-systems\n",
      "    Found existing installation: merlin-systems 0.4.0+2.gf1e2bf5\n",
      "    Uninstalling merlin-systems-0.4.0+2.gf1e2bf5:\n",
      "      Successfully uninstalled merlin-systems-0.4.0+2.gf1e2bf5\n",
      "Successfully installed merlin-systems-0.7.0+10.gd2737da6\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.7.0)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.10.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.41.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.4)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.34.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.9.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (4.12.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.8.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Installing collected packages: keras, flatbuffers, tensorboard, tensorflow-estimator, tensorflow\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
      "Successfully installed flatbuffers-22.10.26 keras-2.10.0 tensorboard-2.10.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0\n",
      "Collecting transformers==4.21\n",
      "  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.21) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.21) (2022.7.25)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.21) (3.7.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.21) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.21) (0.8.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.21) (1.21.5)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers==4.21) (2.22.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.21) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.21) (3.0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: transformers4rec 0.1.11+1.g3367d725 has requirement transformers<4.19, but you'll have transformers 4.21.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.12.0\n",
      "    Uninstalling transformers-4.12.0:\n",
      "      Successfully uninstalled transformers-4.12.0\n",
      "Successfully installed tokenizers-0.12.1 transformers-4.21.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /models && git pull && pip install .\n",
    "cd /nvtabular && git pull && pip install .\n",
    "cd /core && git pull && pip install .\n",
    "cd /systems && git pull && pip install .\n",
    "\n",
    "pip install tensorflow\n",
    "pip install transformers==4.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1edd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I do not need this on my machine, but can be helpful if you encounter issues\n",
    "\n",
    "# import os\n",
    "# os.environ[\"FORCE_TF_AVAILABLE\"]=\"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d1452",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_models_entertainment-with-pretrained-embeddings/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Transformer-based architecture for next-item prediction task\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this use case we will train a Transformer-based architecture for next-item prediction task.\n",
    "\n",
    "We will use the [booking.com dataset](https://github.com/bookingcom/ml-dataset-mdt) to train a session-based model. The dataset contains 1,166,835 of anonymized hotel reservations in the train set and 378,667 in the test set. Each reservation is a part of a customer's trip (identified by `utrip_id`) which includes consecutive reservations.\n",
    "\n",
    "We will reshape the data to organize it into 'sessions'. Each session will be a full customer itinerary in chronological order. The goal will be to predict the city_id of the final reservation of each trip.\n",
    "\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Training a Transformer-based architecture for next-item prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cccd005",
   "metadata": {},
   "source": [
    "## Downloading and preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b619b",
   "metadata": {},
   "source": [
    "You can download the full dataset from GitHub [here](https://github.com/bookingcom/ml-dataset-mdt). Please place it alognside this notebook (or alternatively, change the `DATAPATH` to point to where it is located)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9dccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.core.dispatch import get_lib\n",
    "import numpy as np\n",
    "\n",
    "DATAPATH = 'ml-dataset-mdt'\n",
    "\n",
    "itineraries = get_lib().read_csv(f'{DATAPATH}/train_set.csv', parse_dates=['checkin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b7224",
   "metadata": {},
   "source": [
    "Each reservation has a unique `utrip_id`. During each trip a customer vists several destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192a5727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000027</td>\n",
       "      <td>2016-08-13</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>8183</td>\n",
       "      <td>desktop</td>\n",
       "      <td>7168</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>1000027_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000027</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>2016-08-16</td>\n",
       "      <td>15626</td>\n",
       "      <td>desktop</td>\n",
       "      <td>7168</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>1000027_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000027</td>\n",
       "      <td>2016-08-16</td>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>60902</td>\n",
       "      <td>desktop</td>\n",
       "      <td>7168</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>1000027_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000027</td>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>30628</td>\n",
       "      <td>desktop</td>\n",
       "      <td>253</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>1000027_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000033</td>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>2016-04-11</td>\n",
       "      <td>38677</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>1000033_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id    checkin    checkout  city_id device_class  affiliate_id  \\\n",
       "0  1000027 2016-08-13  2016-08-14     8183      desktop          7168   \n",
       "1  1000027 2016-08-14  2016-08-16    15626      desktop          7168   \n",
       "2  1000027 2016-08-16  2016-08-18    60902      desktop          7168   \n",
       "3  1000027 2016-08-18  2016-08-21    30628      desktop           253   \n",
       "4  1000033 2016-04-09  2016-04-11    38677       mobile           359   \n",
       "\n",
       "  booker_country hotel_country   utrip_id  \n",
       "0        Elbonia        Gondal  1000027_1  \n",
       "1        Elbonia        Gondal  1000027_1  \n",
       "2        Elbonia        Gondal  1000027_1  \n",
       "3        Elbonia        Gondal  1000027_1  \n",
       "4         Gondal  Cobra Island  1000033_1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itineraries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554eb309",
   "metadata": {},
   "source": [
    "We will limit the sequence length to between 2 and 10 trips. That will capture upwards of 95% datapoints!\n",
    "\n",
    "We don't want to train on trips that are shorter than two hops -- our model would not be able to learn much from such sequences. Additionally, such short sequences are uncharacteristically short for this dataset.\n",
    "\n",
    "Besides, training on unusually long or short sequences, that are far outside of the most common sequence length, might not be the best use of our compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "346b871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TRIP_LENGTH = 10\n",
    "MIN_TRIP_LENGTH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a0702c",
   "metadata": {},
   "source": [
    "Let us now split the data into a train and validation set based on trip ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05b2c1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217686"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utrip_ids = itineraries.utrip_id.unique().sample(frac=1)\n",
    "len(utrip_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7754847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_utrip_ids = utrip_ids[:160_000]\n",
    "validation_set_utrip_ids = utrip_ids[160_000:]\n",
    "\n",
    "train_set = itineraries[itineraries.utrip_id.isin(train_set_utrip_ids)]\n",
    "validation_set = itineraries[itineraries.utrip_id.isin(validation_set_utrip_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc3992",
   "metadata": {},
   "source": [
    "We can now begin with data preprocessing.\n",
    "\n",
    "We will combine trips into \"sessions\", discard trips that are either too short or too long and calculate total trip length in stops.\n",
    "\n",
    "We will use nvtabular for this work. It offers optimized tabular data preprocessing operators that run on the GPU. If you would like to learn more about this software library, please take a look [here](https://github.com/NVIDIA-Merlin/NVTabular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69ec166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 23:26:22.931194: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-03 23:26:23.028255: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-03 23:26:23.583523: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib\n",
      "2022-11-03 23:26:23.583628: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib\n",
      "2022-11-03 23:26:23.583634: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 23:26:24.124821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 23:26:24.125382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 23:26:24.125521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 23:26:24.148560: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-03 23:26:24.149842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 23:26:24.150026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 23:26:24.150159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 23:26:24.436136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 23:26:24.436321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 23:26:24.436458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 23:26:24.436585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24576 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:08:00.0, compute capability: 7.5\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nvtabular import *\n",
    "from nvtabular import ops\n",
    "from merlin.models.tf import Loader\n",
    "\n",
    "from merlin.schema.tags import Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3435af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_dataset = Dataset(train_set)\n",
    "validation_set_dataset = Dataset(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60bd5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_features = ['city_id', 'booker_country', 'utrip_id', 'checkin'] >> ops.Groupby(\n",
    "    groupby_cols=['utrip_id'],\n",
    "    sort_cols=['checkin'],\n",
    "    aggs={\n",
    "        'city_id': ['list', 'count'],\n",
    "        'booker_country': ['list']\n",
    "    }\n",
    ")\n",
    "\n",
    "groupby_features_truncated_city = groupby_features['city_id_list'] >> ops.Categorify() >> ops.AddTags([Tags.SEQUENCE, Tags.ITEM, Tags.ITEM_ID])\n",
    "groupby_features_truncated_country = groupby_features['booker_country_list'] >> ops.Categorify() >> ops.AddTags([Tags.SEQUENCE, Tags.ITEM])\n",
    "city_id_count = groupby_features['city_id_count'] >> ops.AddTags([Tags.CONTEXT, Tags.ITEM, Tags.CONTINUOUS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6105767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = Workflow(groupby_features_truncated_city + groupby_features_truncated_country)# + city_id_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edcbcdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_set_processed = wf.fit_transform(train_set_dataset)\n",
    "validation_set_processed = wf.fit_transform(validation_set_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a6675",
   "metadata": {},
   "source": [
    "Our data consists of a sequence of visited `city_ids`, a sequence of `booker_countries` (represented as integer categories) and a `city_id_count` column (which contains the count of visited cities in a trip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dee6b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id_list</th>\n",
       "      <th>booker_country_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4483, 174, 3640, 2112]</td>\n",
       "      <td>[3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[57, 1154, 89, 592, 57]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5, 7, 20, 1014, 64, 51, 3]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4510, 266, 696, 237, 338]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[8, 388, 79, 7, 159, 290, 81, 81, 712, 7394]</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   city_id_list  \\\n",
       "0                       [4483, 174, 3640, 2112]   \n",
       "1                       [57, 1154, 89, 592, 57]   \n",
       "2                   [5, 7, 20, 1014, 64, 51, 3]   \n",
       "3                    [4510, 266, 696, 237, 338]   \n",
       "4  [8, 388, 79, 7, 159, 290, 81, 81, 712, 7394]   \n",
       "\n",
       "              booker_country_list  \n",
       "0                    [3, 3, 3, 3]  \n",
       "1                 [1, 1, 1, 1, 1]  \n",
       "2           [2, 2, 2, 2, 2, 2, 2]  \n",
       "3                 [1, 1, 1, 1, 1]  \n",
       "4  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_processed.compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89cc3a0",
   "metadata": {},
   "source": [
    "We are now ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7ee9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import merlin.models.tf as mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95c794",
   "metadata": {},
   "source": [
    "Let's identify two schemas. The first one for sequential features, the other for context features (`city_id_count`) that we will broadcast to the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4813456",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_schema = train_set_processed.schema.select_by_tag(Tags.SEQUENCE)\n",
    "context_schema = train_set_processed.schema.select_by_tag(Tags.CONTEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d422833",
   "metadata": {},
   "source": [
    "Let's also identify the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e34a3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_set_processed.schema.select_by_tag(Tags.SEQUENCE).column_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7ce10e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id_list</th>\n",
       "      <th>booker_country_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4483, 174, 3640, 2112]</td>\n",
       "      <td>[3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[57, 1154, 89, 592, 57]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5, 7, 20, 1014, 64, 51, 3]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4510, 266, 696, 237, 338]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[8, 388, 79, 7, 159, 290, 81, 81, 712, 7394]</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159995</th>\n",
       "      <td>[3, 68, 396, 174, 3]</td>\n",
       "      <td>[2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159996</th>\n",
       "      <td>[104, 734, 104, 104]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159997</th>\n",
       "      <td>[95, 89, 95, 60]</td>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159998</th>\n",
       "      <td>[26, 1002, 938, 651]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159999</th>\n",
       "      <td>[72, 361, 908, 361, 1831, 474, 13725, 943, 233...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             city_id_list  \\\n",
       "0                                 [4483, 174, 3640, 2112]   \n",
       "1                                 [57, 1154, 89, 592, 57]   \n",
       "2                             [5, 7, 20, 1014, 64, 51, 3]   \n",
       "3                              [4510, 266, 696, 237, 338]   \n",
       "4            [8, 388, 79, 7, 159, 290, 81, 81, 712, 7394]   \n",
       "...                                                   ...   \n",
       "159995                               [3, 68, 396, 174, 3]   \n",
       "159996                               [104, 734, 104, 104]   \n",
       "159997                                   [95, 89, 95, 60]   \n",
       "159998                               [26, 1002, 938, 651]   \n",
       "159999  [72, 361, 908, 361, 1831, 474, 13725, 943, 233...   \n",
       "\n",
       "                                  booker_country_list  \n",
       "0                                        [3, 3, 3, 3]  \n",
       "1                                     [1, 1, 1, 1, 1]  \n",
       "2                               [2, 2, 2, 2, 2, 2, 2]  \n",
       "3                                     [1, 1, 1, 1, 1]  \n",
       "4                      [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]  \n",
       "...                                               ...  \n",
       "159995                                [2, 2, 2, 2, 2]  \n",
       "159996                                   [1, 1, 1, 1]  \n",
       "159997                                   [2, 2, 2, 2]  \n",
       "159998                                   [1, 1, 1, 1]  \n",
       "159999  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "\n",
       "[160000 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_processed.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0d0224d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>city_id_list</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.ITEM_ID, Tags.ID, Tags...</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.city_id_list.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>36065</td>\n",
       "      <td>city_id_list</td>\n",
       "      <td>36066</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>booker_country_list</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.ITEM, Tags.SEQUENCE)</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.booker_country_list.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>booker_country_list</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'city_id_list', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM_ID: 'item_id'>, <Tags.ID: 'id'>, <Tags.ITEM: 'item'>, <Tags.SEQUENCE: 'sequence'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.city_id_list.parquet', 'domain': {'min': 0, 'max': 36065, 'name': 'city_id_list'}, 'embedding_sizes': {'cardinality': 36066, 'dimension': 512}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': True}, {'name': 'booker_country_list', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>, <Tags.SEQUENCE: 'sequence'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.booker_country_list.parquet', 'domain': {'min': 0, 'max': 5, 'name': 'booker_country_list'}, 'embedding_sizes': {'cardinality': 6, 'dimension': 16}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': True}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_processed.schema.select_by_tag(Tags.SEQUENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a12009aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_processed.schema.select_by_tag(Tags.CONTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cddfd424",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(train_set_processed, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbcbcef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06e3c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.models.tf.transforms.features import BroadcastToSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2969bf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdb4e34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>city_id_list</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.ITEM_ID, Tags.ID, Tags...</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.city_id_list.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>36065</td>\n",
       "      <td>city_id_list</td>\n",
       "      <td>36066</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>booker_country_list</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.ITEM, Tags.SEQUENCE)</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.booker_country_list.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>booker_country_list</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'city_id_list', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM_ID: 'item_id'>, <Tags.ID: 'id'>, <Tags.ITEM: 'item'>, <Tags.SEQUENCE: 'sequence'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.city_id_list.parquet', 'domain': {'min': 0, 'max': 36065, 'name': 'city_id_list'}, 'embedding_sizes': {'cardinality': 36066, 'dimension': 512}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': True}, {'name': 'booker_country_list', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>, <Tags.SEQUENCE: 'sequence'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.booker_country_list.parquet', 'domain': {'min': 0, 'max': 5, 'name': 'booker_country_list'}, 'embedding_sizes': {'cardinality': 6, 'dimension': 16}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': True}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40e5c0",
   "metadata": {},
   "source": [
    "## Without broadcasting of context features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eff5ac50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id_list</th>\n",
       "      <th>booker_country_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4483, 174, 3640, 2112]</td>\n",
       "      <td>[3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[57, 1154, 89, 592, 57]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5, 7, 20, 1014, 64, 51, 3]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4510, 266, 696, 237, 338]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[8, 388, 79, 7, 159, 290, 81, 81, 712, 7394]</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159995</th>\n",
       "      <td>[3, 68, 396, 174, 3]</td>\n",
       "      <td>[2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159996</th>\n",
       "      <td>[104, 734, 104, 104]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159997</th>\n",
       "      <td>[95, 89, 95, 60]</td>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159998</th>\n",
       "      <td>[26, 1002, 938, 651]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159999</th>\n",
       "      <td>[72, 361, 908, 361, 1831, 474, 13725, 943, 233...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             city_id_list  \\\n",
       "0                                 [4483, 174, 3640, 2112]   \n",
       "1                                 [57, 1154, 89, 592, 57]   \n",
       "2                             [5, 7, 20, 1014, 64, 51, 3]   \n",
       "3                              [4510, 266, 696, 237, 338]   \n",
       "4            [8, 388, 79, 7, 159, 290, 81, 81, 712, 7394]   \n",
       "...                                                   ...   \n",
       "159995                               [3, 68, 396, 174, 3]   \n",
       "159996                               [104, 734, 104, 104]   \n",
       "159997                                   [95, 89, 95, 60]   \n",
       "159998                               [26, 1002, 938, 651]   \n",
       "159999  [72, 361, 908, 361, 1831, 474, 13725, 943, 233...   \n",
       "\n",
       "                                  booker_country_list  \n",
       "0                                        [3, 3, 3, 3]  \n",
       "1                                     [1, 1, 1, 1, 1]  \n",
       "2                               [2, 2, 2, 2, 2, 2, 2]  \n",
       "3                                     [1, 1, 1, 1, 1]  \n",
       "4                      [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]  \n",
       "...                                               ...  \n",
       "159995                                [2, 2, 2, 2, 2]  \n",
       "159996                                   [1, 1, 1, 1]  \n",
       "159997                                   [2, 2, 2, 2]  \n",
       "159998                                   [1, 1, 1, 1]  \n",
       "159999  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "\n",
       "[160000 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_processed.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce5796ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.Model(\n",
    "    mm.InputBlockV2(\n",
    "        seq_schema,\n",
    "        embeddings=mm.Embeddings(\n",
    "            train_set_processed.schema.select_by_tag(Tags.CATEGORICAL), sequence_combiner=None\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    mm.GPT2Block(d_model=40, n_head=4, n_layer=2, pre=mm.ReplaceMaskedEmbeddings()),\n",
    "    mm.CategoricalOutput(\n",
    "        train_set_processed.schema.select_by_name(target),\n",
    "        default_loss=\"categorical_crossentropy\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b1d0bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['model/embeddings:0', 'model/embeddings:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  1/157 [..............................] - ETA: 6:13 - loss: 0.9872 - recall_at_10: 0.0000e+00 - mrr_at_10: 0.0000e+00 - ndcg_at_10: 0.0000e+00 - map_at_10: 0.0000e+00 - precision_at_10: 0.0000e+00 - regularization_loss: 0.0000e+00"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: 5, 6, 6, ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(run_eagerly\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequenceMaskRandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasking_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/models/base.py:856\u001b[0m, in \u001b[0;36mBaseModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, train_metrics_steps, pre, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_compile_cache()\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pre \u001b[38;5;241m=\u001b[39m pre\n\u001b[0;32m--> 856\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre:\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pre\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/models/base.py:671\u001b[0m, in \u001b[0;36mBaseModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    668\u001b[0m x, y, sample_weight \u001b[38;5;241m=\u001b[39m unpack_x_y_sample_weight(data)\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_pre\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 671\u001b[0m     out \u001b[38;5;241m=\u001b[39m call_layer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pre, x, targets\u001b[38;5;241m=\u001b[39my, features\u001b[38;5;241m=\u001b[39mx, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, Prediction):\n\u001b[1;32m    673\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moutputs, out\u001b[38;5;241m.\u001b[39mtargets\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/utils/tf_utils.py:437\u001b[0m, in \u001b[0;36mcall_layer\u001b[0;34m(layer, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m         call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m    435\u001b[0m         filtered_kwargs \u001b[38;5;241m=\u001b[39m filter_kwargs(filtered_kwargs, call_fn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_k)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/core/tabular.py:478\u001b[0m, in \u001b[0;36m_tabular_call\u001b[0;34m(self, inputs, pre, post, merge_with, aggregation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_call(inputs, transformations\u001b[38;5;241m=\u001b[39mpre)\n\u001b[1;32m    477\u001b[0m \u001b[38;5;66;03m# This will call the `call` method implemented by the super class.\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    481\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_call(\n\u001b[1;32m    482\u001b[0m         outputs, transformations\u001b[38;5;241m=\u001b[39mpost, merge_with\u001b[38;5;241m=\u001b[39mmerge_with, aggregation\u001b[38;5;241m=\u001b[39maggregation\n\u001b[1;32m    483\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/config/schema.py:58\u001b[0m, in \u001b[0;36mSchemaMixin.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_schema()\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/transforms/sequence.py:452\u001b[0m, in \u001b[0;36mSequenceMaskRandom.compute_mask\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m\"\"\"Selects (masks) some positions of the targets to be predicted.\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03mThis method is called by Keras after call()\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03mand returns the targets mask that will\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03mbe assigned to the input tensors and targets, being accessible\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03mby tensor._keras_mask\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    451\u001b[0m item_id_seq \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_name]\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_target_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_id_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m inputs_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/transforms/sequence.py:482\u001b[0m, in \u001b[0;36mSequenceMaskRandom._generate_target_mask\u001b[0;34m(self, ids_seq)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m\"\"\"Generates a target mask according to the defined probability and\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03mto the constraints for Masked Language Modeling training (i.e., each\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03msequence might have between 1 and length-1 masked positions.)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;124;03m    selected to be predicted\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    480\u001b[0m row_lengths \u001b[38;5;241m=\u001b[39m ids_seq\u001b[38;5;241m.\u001b[39mrow_lengths(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 482\u001b[0m assertion_min_seq_length \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAssert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_lengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow_lengths\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies([assertion_min_seq_length]):\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;66;03m# Targets are masked according to a probability\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     target_mask_by_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_masked_by_prob(row_lengths, prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasking_prob)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: 5, 6, 6, ..."
     ]
    }
   ],
   "source": [
    "model.compile(run_eagerly=True, optimizer='adam', loss=\"categorical_crossentropy\")\n",
    "model.fit(loader, pre=mm.SequenceMaskRandom(schema=seq_schema, target=target, masking_prob=0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b1bc6",
   "metadata": {},
   "source": [
    "## Specifying correct input_dimensions (before broadcasting) in model constructor (d_model=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf216b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = mm.Model(\n",
    "#     mm.InputBlockV2(\n",
    "#         train_set_processed.schema,\n",
    "#         embeddings=mm.Embeddings(\n",
    "#             train_set_processed.schema.select_by_tag(Tags.CATEGORICAL), sequence_combiner=None\n",
    "#         ),\n",
    "#         post=BroadcastToSequence(context_schema, seq_schema)\n",
    "#     ),\n",
    "\n",
    "#     mm.GPT2Block(d_model=40, n_head=4, n_layer=2, pre=mm.ReplaceMaskedEmbeddings()),\n",
    "#     mm.CategoricalOutput(\n",
    "#         train_set_processed.schema.select_by_name(target),\n",
    "#         default_loss=\"categorical_crossentropy\",\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f16fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(run_eagerly=True, optimizer='adam', loss=\"categorical_crossentropy\")\n",
    "# model.fit(loader, pre=mm.SequenceMaskRandom(schema=train_set_processed.schema, target=target, masking_prob=0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a8e13f",
   "metadata": {},
   "source": [
    "## Specifying correct input_dimensions (after broadcasting) in model constructor (d_model=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = mm.Model(\n",
    "#     mm.InputBlockV2(\n",
    "\n",
    "#         train_set_processed.schema,\n",
    "#         embeddings=mm.Embeddings(\n",
    "#             train_set_processed.schema.select_by_tag(Tags.CATEGORICAL), sequence_combiner=None\n",
    "#         ),\n",
    "#         post=BroadcastToSequence(context_schema, seq_schema)\n",
    "#     ),\n",
    "\n",
    "#     mm.GPT2Block(d_model=41, n_head=4, n_layer=2, pre=mm.ReplaceMaskedEmbeddings()),\n",
    "#     mm.CategoricalOutput(\n",
    "#         train_set_processed.schema.select_by_name(target),\n",
    "#         default_loss=\"categorical_crossentropy\",\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00160990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(run_eagerly=True, optimizer='adam', loss=\"categorical_crossentropy\")\n",
    "# model.fit(loader, pre=mm.SequenceMaskRandom(schema=train_set_processed.schema, target=target, masking_prob=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c662af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first stab at evaluation\n",
    "\n",
    "# loader_eval = Loader(validation_set_processed, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(loader_eval, batch_size=1024, pre=mm.SequenceMaskLast(schema=train_set_processed.schema, target=target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
