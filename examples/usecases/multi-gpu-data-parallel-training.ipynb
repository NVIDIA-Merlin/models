{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e493825",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b423f1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Multi-GPU Data Parallel Training with Merlin Models and Horovod\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container. \n",
    "\n",
    "There are multiple ways to scale training pipeline to multiple GPUs:\n",
    "- Model Parallel: If the model is too large to fit on a single GPU, the parameters are distributed over multiple GPUs\n",
    "- Data Parallel: Every GPU has a copy of all model parameters and runs the forward/backward pass for it's batch.\n",
    "\n",
    "In this example, we demonstrate how to scale a training pipeline to multi-GPU, single node. The goal is to maximize throughput and reduce training time. In that way, models can be trained more frequently and researches can run more experiments in a shorter time duration.\n",
    "\n",
    "It is equivalent to train with a larger batch-size. As we are using more GPUs, we have more computational resources and can achieve higher throughput. All model parameters fits on a single GPU. Every worker (each GPU) has a copy of the model parameters and runs the forward pass on their local batch. The workers synchronize the gradients with each other, which can introduce an overhead. \n",
    "\n",
    "We are using [horovod](https://github.com/horovod/horovod) to schedule data parallel model training. NVIDIA Merlin implemented horovod to reduce required code changes and enables the users to scale their pipeline with minimal code changes.\n",
    "\n",
    "The example is based on [03-Exploring-different-models.ipynb](../03-Exploring-different-models.ipynb). We will focus on data parallel training and do not cover functionality from the notebook.\n",
    "\n",
    "**Learning objectives**\n",
    "- Scaling training pipeline to multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb261fa",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c458709",
   "metadata": {},
   "source": [
    "First, import some libraries and set some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027b6018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "from merlin.datasets.synthetic import generate_data\n",
    "\n",
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/raid/data/\")\n",
    "NUM_ROWS = os.environ.get(\"NUM_ROWS\", 1000000)\n",
    "SYNTHETIC_DATA = eval(os.environ.get(\"SYNTHETIC_DATA\", \"True\"))\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", 16384))\n",
    "NUM_GPUs = int(os.environ.get(\"NUM_GPUs\", 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f45d0",
   "metadata": {},
   "source": [
    "## Generating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb985f",
   "metadata": {},
   "source": [
    "We will use the same dataset and preprocessing with NVTabular as in [03-Exploring-different-models.ipynb](../03-Exploring-different-models.ipynb). Please review the example, if you have any questions.\n",
    "\n",
    "**There is one difference**:\n",
    "We set `out_files_per_proc` to `NUM_GPUs` in the `to_parquet` function. We want to have one output parquet file for each available GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e13d96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/io/dataset.py:862: UserWarning: Only created 1 files did not have enough partitions to create 2 files.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if SYNTHETIC_DATA:\n",
    "    train, valid = generate_data(\"aliccp-raw\", int(NUM_ROWS), set_sizes=(0.7, 0.3))\n",
    "    # save the datasets as parquet files\n",
    "    train.to_ddf().to_parquet(os.path.join(DATA_FOLDER, \"train\"))\n",
    "    valid.to_ddf().to_parquet(os.path.join(DATA_FOLDER, \"valid\"))\n",
    "\n",
    "train_path = os.path.join(DATA_FOLDER, \"train\", \"*.parquet\")\n",
    "valid_path = os.path.join(DATA_FOLDER, \"valid\", \"*.parquet\")\n",
    "output_path = os.path.join(DATA_FOLDER, \"processed\")\n",
    "\n",
    "category_temp_directory = os.path.join(DATA_FOLDER, \"categories\")\n",
    "user_id = [\"user_id\"] >> Categorify(out_path=category_temp_directory) >> TagAsUserID()\n",
    "item_id = [\"item_id\"] >> Categorify(out_path=category_temp_directory) >> TagAsItemID()\n",
    "targets = [\"click\"] >> AddMetadata(tags=[Tags.BINARY_CLASSIFICATION, \"target\"])\n",
    "\n",
    "item_features = [\"item_category\", \"item_shop\", \"item_brand\"] >> Categorify(out_path=category_temp_directory) >> TagAsItemFeatures()\n",
    "\n",
    "user_features = (\n",
    "    [\n",
    "        \"user_shops\",\n",
    "        \"user_profile\",\n",
    "        \"user_group\",\n",
    "        \"user_gender\",\n",
    "        \"user_age\",\n",
    "        \"user_consumption_2\",\n",
    "        \"user_is_occupied\",\n",
    "        \"user_geography\",\n",
    "        \"user_intentions\",\n",
    "        \"user_brands\",\n",
    "        \"user_categories\",\n",
    "    ]\n",
    "    >> Categorify(out_path=category_temp_directory)\n",
    "    >> TagAsUserFeatures()\n",
    ")\n",
    "\n",
    "outputs = user_id + item_id + item_features + user_features + targets\n",
    "\n",
    "workflow = nvt.Workflow(outputs)\n",
    "\n",
    "train_dataset = nvt.Dataset(train_path)\n",
    "valid_dataset = nvt.Dataset(valid_path)\n",
    "\n",
    "workflow.fit(train_dataset)\n",
    "workflow.transform(train_dataset).to_parquet(\n",
    "    output_path=output_path + \"/train/\",\n",
    "    out_files_per_proc = NUM_GPUs\n",
    ")\n",
    "workflow.transform(valid_dataset).to_parquet(\n",
    "    output_path=output_path + \"/valid/\",\n",
    "    out_files_per_proc = NUM_GPUs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7d97d2",
   "metadata": {},
   "source": [
    "## Data Parallel Training with Merlin Models and Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344a8fc",
   "metadata": {},
   "source": [
    "First, we can take a look on the training dataset. We can see that we have the same number of parquet files as we have GPUs available (set by `NUM_GPUs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a1f8eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_file_list.txt\t_metadata.json\tpart_1.parquet\r\n",
      "_metadata\tpart_0.parquet\tschema.pbtxt\r\n"
     ]
    }
   ],
   "source": [
    "!ls $output_path/train/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12aa11",
   "metadata": {},
   "source": [
    "We use [horovod](https://github.com/horovod/horovod) to schedule and distribute our training pipeline. Horovod requires multiple code changes, which Merlin Models handles automatically (see reference [Horovod Keras Example](https://horovod.readthedocs.io/en/stable/keras.html)).\n",
    "\n",
    "Let's take a look on the required code changes to our previous examples:\n",
    "- We need to write our training pipeline as a separate `.py` file, as we need to start the training run with `horovodrun`\n",
    "- We need to set `os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(MPI_RANK)`. `MPI_RANK` is the ID of the current worker (starting at 0). The line ensures that each worker can access only one GPU. The first worker (ID=0) can access GPU ID=0 and so on.\n",
    "- We need to select only one parquet file per worker (`os.path.join(args.path, \"train\", \"part_\" + str(MPI_RANK) + \".parquet\")`. Similar to assigning one GPU, we select only one `part_<ID>.parquet` file for each GPU. In that way, one epoch is only one pass through the full dataset.\n",
    "- We need to set `drop_last=True`. As the last batch is not complete and can cause errors.\n",
    "\n",
    "**Important: The individual parquet files require to have the same number of batches. If one worker has more batches than another, the training process will freeze. The worker with more batches waits for the gradients from the other worker, but it finished the training run.**\n",
    "\n",
    "We will print number of batches by using `print(\"Number batches: \" + str(len(train_loader)))`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86db86f",
   "metadata": {},
   "source": [
    "We can take a look what required code changes are applied automatically by Merlin Models. You do not need to care about them:\n",
    "- Horovod requires to initialize horovod (`hvd.init()`)\n",
    "- After the first batch, we need to broadcast the initial variables from one worker to all other ones. As we initialize the model randomly, every worker would have different parameters. In the beginning, we need to ensure every worker starts with the same state.\n",
    "- We need to scale the learning rate by number of workers\n",
    "- We need to average the evaluation metrics (Note: AUC metrics could slightly be different, as averaging AUC vs. calculating AUC on the full dataset is different)\n",
    "- We need to wrap the optimizer to use distributed optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656271a",
   "metadata": {},
   "source": [
    "Let's write the training pipeline to a `.py file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88820f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tf_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './tf_trainer.py'\n",
    "\n",
    "import os\n",
    "\n",
    "MPI_SIZE = int(os.getenv(\"OMPI_COMM_WORLD_SIZE\"))\n",
    "MPI_RANK = int(os.getenv(\"OMPI_COMM_WORLD_RANK\"))\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(MPI_RANK)\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Hyperparameters for model training'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--batch-size', \n",
    "    type=str,\n",
    "    help='Batch-Size per GPU worker'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--path', \n",
    "    type=str,\n",
    "    help='Directory with training and validation data'\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# define train and valid dataset objects\n",
    "train = Dataset(os.path.join(args.path, \"train\", \"part_\" + str(MPI_RANK) + \".parquet\"))\n",
    "valid = Dataset(os.path.join(args.path, \"valid\", \"part_\" + str(MPI_RANK) + \".parquet\"))\n",
    "\n",
    "# define schema object\n",
    "target_column = train.schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "\n",
    "train_loader = mm.Loader(\n",
    "    train,\n",
    "    schema=train.schema,\n",
    "    batch_size=int(args.batch_size),\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valid_loader = mm.Loader(\n",
    "    valid,\n",
    "    schema=valid.schema,\n",
    "    batch_size=int(args.batch_size),\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(\"Number batches: \" + str(len(train_loader)))\n",
    "\n",
    "model = mm.DLRMModel(\n",
    "    train.schema,\n",
    "    embedding_dim=16,\n",
    "    bottom_block=mm.MLPBlock([32, 16]),\n",
    "    top_block=mm.MLPBlock([32, 16]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column),\n",
    ")\n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "losses = model.fit(\n",
    "    train_loader\n",
    ")\n",
    "\n",
    "print(model.evaluate(valid, batch_size=int(args.batch_size), return_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7e63a",
   "metadata": {},
   "source": [
    "We can start the training run with\n",
    "\n",
    "```bash\n",
    "horovod -np <number of GPUs> python <script> [--args]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f74e84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,1]<stderr>:2022-11-03 13:24:49.132058: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "[1,1]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[1,0]<stderr>:2022-11-03 13:24:49.193852: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "[1,0]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[1,1]<stderr>:2022-11-03 13:24:59.091618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16255 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:0a:00.0, compute capability: 7.0\n",
      "[1,0]<stderr>:2022-11-03 13:24:59.639104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16255 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "[1,1]<stdout>:Number batches: 21\n",
      "[1,0]<stdout>:Number batches: 21\n",
      "21/21 [==============================] - 22s 42ms/step - loss: 0.6932 - auc: 0.5000 - regularization_loss: 0.0000e+00stdout>t>[1,0]<stdout>[1,1]<stdout>:[1,1]<stdout>[1,0]<stdout>[1,1]<stdout>[1,0]<stdout>[1,0]<stdout>[1,1]<stdout>[1,0]<stdout>[1,1]<stdout>[1,1]<stdout>\n",
      "21/21 [==============================] - 23s 42ms/step - loss: 0.6931 - auc: 0.4999 - regularization_loss: 0.0000e+00stdout>[1,1]<stdout>[1,1]<stdout>\n",
      "10/10 [==============================] - 4s 24ms/step - loss: 0.6931 - auc: 0.5002 - regularization_loss: 0.0000e+00]<stdout[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>\n",
      "[1,0]<stdout>:{'loss': 0.6931458711624146, 'auc': 0.5002372860908508, 'regularization_loss': 0.0}\n",
      "[1,0]<stderr>:/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "[1,0]<stderr>:  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "[1,0]<stderr>:/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "[1,0]<stderr>:  warnings.warn(\n",
      "[1,0]<stderr>:/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "[1,0]<stderr>:  warnings.warn(\n",
      "10/10 [==============================] - 4s 25ms/step - loss: 0.6931 - auc: 0.5000 - regularization_loss: 0.0000e+00<stdout>[1,1]<stdout>[1,1]<stdout>\n",
      "[1,1]<stdout>:{'loss': 0.6931441426277161, 'auc': 0.5000250339508057, 'regularization_loss': 0.0}\n",
      "[1,1]<stderr>:/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "[1,1]<stderr>:  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "[1,1]<stderr>:/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "[1,1]<stderr>:  warnings.warn(\n",
      "[1,1]<stderr>:/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "[1,1]<stderr>:  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!horovodrun -np $NUM_GPUs python tf_trainer.py --batch-size $BATCH_SIZE --path $output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cea5782-c88f-4a34-990f-9074bb387d63",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d3d88-979e-4587-a74a-bcf61db36b84",
   "metadata": {},
   "source": [
    "In this example notebook we learned how to scale a Merlin Models training pipeline using Horovod to multiple GPUs to reduce training time. In theory, you should be able to use horovod for multi-node training, as well.\n",
    "\n",
    "Check out our [example](https://github.com/NVIDIA-Merlin/NVTabular/blob/main/examples/03-Running-on-multiple-GPUs-or-on-CPU.ipynb), how to scale NVTabular workflows to multi-GPU feature engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c78a7de67f1468ee33d22a76790123f2989400fa0e73ac6b45f15b09432f615d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
