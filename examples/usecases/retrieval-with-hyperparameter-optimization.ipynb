{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a556f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions anda\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d1452",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_models-retrieval-with-hyperparameter-optimization/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Retrieval with hyperparameter optimization\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this use case we will perform hyperparameter optimization using [Optuna](https://optuna.org/). Optuna is an open source hyperparameter optimization framework which automates hyperparameter search and can be used across a wide set of scenarios.\n",
    "\n",
    "We will look at optimizing candidate retrieval on a dataset from a Kaggle competition, the [H&M Personalized Fashion Recommendations challenge](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations).\n",
    "\n",
    "Hyperparameter optimization can be arbitrarily complex -- in this use case, we will look at optimizing the learning rate and embedding dimensionality to achieve best results on candidate generation. We will train a Matrix Factorization model on user-item pairs and maximize the hit rate at 100 (how many of top 100 retrieved candidates were indeed purchased).\n",
    "\n",
    "## Why run an HPO (hyperparameter optimization) experiment in the first place?\n",
    "\n",
    "Deep learninng models are highly sophisticated. Each architecture exposes many knobs that we can tweak. We can also optimizte the training procedure or the way in which we process data.\n",
    "\n",
    "We usually have an idea of what a good range of values for any of these parameters would be, or at least we can make an educated guess. But it is impossible to pin point values that would give us adequate performance without ample experimentation.\n",
    "\n",
    "We could perform all this work manually. We could train our model multiple times, tweaking the hyperparameters from run to run. We could track the results and systematically explore the parameter space.\n",
    "\n",
    "This approach has its merits. We might learn about the effect each of the hyperparameter has and what they do in combination. We can learn a lot in this process.\n",
    "\n",
    "But for day to day work, handing off all this effort to an HPO framework like Optuna can save us a lot of time. We no longer have to track the experiments since they all will be logged. We no longer have to babysit our model and rerun the training.\n",
    "\n",
    "We write the code once, specify a range of hyperparameters for the experimentation framework to explore, and are off to the races. The experiment will run on its own and we can come back to it and review the results when it finishes.\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- How to run a hyperoptimization experiment\n",
    "- Candidate generation using Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cccd005",
   "metadata": {},
   "source": [
    "## Downloading and preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a7a100",
   "metadata": {},
   "source": [
    "Let's begin by downloading the dataset. Please find the data on Kaggle [here](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data). We will only use `transactions_train.csv` which lists items that were purchased and maps the transactions to customers.\n",
    "\n",
    "Please download the `transactions_train.csv` file and store it alongside the current notebook.\n",
    "\n",
    "Let us read in the file and look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d29fb60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')\n",
    "\n",
    "# from merlin.datasets.synthetic import generate_data\n",
    "\n",
    "# train_transformed = generate_data('user-item-interactions', 1000)\n",
    "# valid_transformed = generate_data('user-item-interactions', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1191c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from merlin.core.dispatch import get_lib\n",
    "# import numpy as np\n",
    "\n",
    "# transactions = get_lib().read_csv('transactions_train.csv', parse_dates=['t_dat'])\n",
    "# transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c25bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from merlin.core.dispatch import get_lib\n",
    "# import numpy as np\n",
    "\n",
    "# transactions = get_lib().read_csv('examples/usecases/transactions_train.csv', parse_dates=['t_dat'])\n",
    "# transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea5470",
   "metadata": {},
   "source": [
    "Let's assign the last week to our validation set and treat the rest of that data as our train set. Additionally, let us remove purchases in our train set that were performed by customers who do not appear in our validation set. This will speed up our experiments (we have less data to train on) and in other experiments that I ran, including all the customers doesn't lead to better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888db6d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transactions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m seven_days_ago \u001b[38;5;241m=\u001b[39m \u001b[43mtransactions\u001b[49m\u001b[38;5;241m.\u001b[39mt_dat\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mtimedelta64(\u001b[38;5;241m7\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m train_set \u001b[38;5;241m=\u001b[39m transactions[transactions\u001b[38;5;241m.\u001b[39mt_dat \u001b[38;5;241m<\u001b[39m seven_days_ago]\n\u001b[1;32m      4\u001b[0m validation_set \u001b[38;5;241m=\u001b[39m transactions[transactions\u001b[38;5;241m.\u001b[39mt_dat \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m seven_days_ago]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transactions' is not defined"
     ]
    }
   ],
   "source": [
    "seven_days_ago = transactions.t_dat.max() - np.timedelta64(7, 'D')\n",
    "\n",
    "train_set = transactions[transactions.t_dat < seven_days_ago]\n",
    "validation_set = transactions[transactions.t_dat >= seven_days_ago]\n",
    "\n",
    "validation_set_customers = validation_set.customer_id.unique()\n",
    "train_set = train_set[train_set.customer_id.isin(validation_set_customers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd07da",
   "metadata": {},
   "source": [
    "Before we can proceed with training we need to preprocess our data.\n",
    "\n",
    "We will only use `customer_id` and `article_id` pairs. Still, a neural network expects them to be represented as continuous integers. We need to go from how they are represented in our dataset to that desired representation.\n",
    "\n",
    "In order to do so, we will leverage `nvtabular` and the `Categorify` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a6a7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 02:13:02.073299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 02:13:02.073790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 02:13:02.073931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ID",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnvtabular\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tags\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchedDataset\n",
      "File \u001b[0;32m/workspace/merlin/models/tf/__init__.py:48\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenseResidualBlock, MLPBlock\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     LazyAdam,\n\u001b[1;32m     44\u001b[0m     MultiOptimizer,\n\u001b[1;32m     45\u001b[0m     OptimizerBlocks,\n\u001b[1;32m     46\u001b[0m     split_embeddings_on_size,\n\u001b[1;32m     47\u001b[0m )\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DualEncoderBlock, ItemRetrievalScorer\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrix_factorization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     MatrixFactorizationBlock,\n\u001b[1;32m     51\u001b[0m     QueryItemIdsEmbeddingsBlock,\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtwo_tower\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TwoTowerBlock\n",
      "File \u001b[0;32m/workspace/merlin/models/tf/blocks/retrieval/base.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelBlock\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabular\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabularAggregationType\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelBlock\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregularization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m L2Norm\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabularData\n",
      "File \u001b[0;32m/workspace/merlin/models/tf/models/base.py:38\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_all_instances_in_layers\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     call_layer,\n\u001b[1;32m     35\u001b[0m     get_sub_blocks,\n\u001b[1;32m     36\u001b[0m     maybe_serialize_keras_objects,\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unique_rows_by_features\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnSchema, Schema, Tags\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m/workspace/merlin/models/utils/dataset.py:55\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     targets \u001b[38;5;241m=\u001b[39m _to_numpy(df[target_column]) \u001b[38;5;28;01mif\u001b[39;00m target_column \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(userids))\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m coo_matrix((targets\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m), (userids, itemids)))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique_rows_by_features\u001b[39m(\n\u001b[0;32m---> 55\u001b[0m     dataset: Dataset, features_tag: Union[\u001b[38;5;28mstr\u001b[39m, Tags], grouping_tag: Union[\u001b[38;5;28mstr\u001b[39m, Tags] \u001b[38;5;241m=\u001b[39m \u001b[43mTags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mID\u001b[49m\n\u001b[1;32m     56\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    Select unique rows from a Dataset. Returns columns specified by `features_tag`\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m     that are unique based on the columns specified by the `grouping_tag`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m        Dataset\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`unique_rows_by_features` is deprecated and will be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `unique_by_tag` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     79\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3.8/enum.py:384\u001b[0m, in \u001b[0;36mEnumMeta.__getattr__\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_member_map_[name]\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(name) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: ID"
     ]
    }
   ],
   "source": [
    "from nvtabular import *\n",
    "from nvtabular import ops\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.models.tf.dataset import BatchedDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55697a",
   "metadata": {},
   "source": [
    "We represent our data as `Merlin` `Datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset(train_set)\n",
    "validation_set = Dataset(validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c996d487",
   "metadata": {},
   "source": [
    "We now define the operations we want to apply to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2211ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id = ['customer_id'] >> ops.Categorify() >> ops.TagAsUserID()\n",
    "article_id = ['article_id'] >> ops.Categorify() >> ops.TagAsItemID()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5880ef5",
   "metadata": {},
   "source": [
    "And we proceed with fitting the workflow and transforming both the train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3cf03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Workflow(customer_id + article_id)\n",
    "train_transformed = workflow.fit_transform(train_set)\n",
    "\n",
    "valid_transformed = workflow.transform(validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3cbd8e",
   "metadata": {},
   "source": [
    "We are now ready to train our model and perform hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9984c8d",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization using optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1507109",
   "metadata": {},
   "source": [
    "We will train a retrieval model. Customers have performed a number of purchases in the last week of data that we are using as our validation set.\n",
    "\n",
    "Our objective will be to train a retrieval model, Matrix Factorization, and to generate 100 candidates for each customer.\n",
    "\n",
    "The goal is to maximize the count of purchases that appear among our candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c3c04",
   "metadata": {},
   "source": [
    "### The 3 components of hyperparameter optimization with Optuna\n",
    "\n",
    "With Optuna, you create a `study`. A `study` is an optimization session with a number of trials. A `trial` is a single experiment, a call of the objective function.\n",
    "\n",
    "A `parameter` is a variable whose value we will optimize.\n",
    "\n",
    "This is a brief primer but should provide you with all the information necessary to get started with Optuna. You can find further information in Optuna's documentation [here](https://Optuna.readthedocs.io/en/stable/tutorial/index.html).\n",
    "\n",
    "Below we will run an Optuna `study`. The `parameters` we will optimize are:\n",
    "\n",
    "* embedding dimensionality\n",
    "* learning rate\n",
    "* number of epochs\n",
    "\n",
    "Embedding dimensionality is how we control the capacity of the model. We would like to provide it with just the right amount of expressive power. If we endow our model with too great of a capacity, our model will overfit to our training data and it's ability to generalize to unseen data will suffer.\n",
    "\n",
    "On the other hand, if we train too simple of a model, its performance will be limited as it will not be able to capture much of the signal in our train data.\n",
    "\n",
    "The learning rate and number of epochs deal with the technical aspects of training our model. As the model might train differently depending on its capacity, we will perform a grid search across all these parameters in order to find a combination that will perform best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3369212",
   "metadata": {},
   "source": [
    "### The setup of our study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be9a85f",
   "metadata": {},
   "source": [
    "Before we proceed further, please make sure you have `optuna` and `plotly` installed.\n",
    "\n",
    "You can do so by executing the following line from your terminal:\n",
    "\n",
    "`pip install optuna plotly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6a2fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.schema.tags import Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b73412",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ID",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmm\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/merlin/models/tf/__init__.py:48\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenseResidualBlock, MLPBlock\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     LazyAdam,\n\u001b[1;32m     44\u001b[0m     MultiOptimizer,\n\u001b[1;32m     45\u001b[0m     OptimizerBlocks,\n\u001b[1;32m     46\u001b[0m     split_embeddings_on_size,\n\u001b[1;32m     47\u001b[0m )\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DualEncoderBlock, ItemRetrievalScorer\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrix_factorization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     MatrixFactorizationBlock,\n\u001b[1;32m     51\u001b[0m     QueryItemIdsEmbeddingsBlock,\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtwo_tower\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TwoTowerBlock\n",
      "File \u001b[0;32m/workspace/merlin/models/tf/blocks/retrieval/base.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelBlock\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabular\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabularAggregationType\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelBlock\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregularization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m L2Norm\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabularData\n",
      "File \u001b[0;32m/workspace/merlin/models/tf/models/base.py:38\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_all_instances_in_layers\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     call_layer,\n\u001b[1;32m     35\u001b[0m     get_sub_blocks,\n\u001b[1;32m     36\u001b[0m     maybe_serialize_keras_objects,\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unique_rows_by_features\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnSchema, Schema, Tags\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m/workspace/merlin/models/utils/dataset.py:55\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     targets \u001b[38;5;241m=\u001b[39m _to_numpy(df[target_column]) \u001b[38;5;28;01mif\u001b[39;00m target_column \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(userids))\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m coo_matrix((targets\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m), (userids, itemids)))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique_rows_by_features\u001b[39m(\n\u001b[0;32m---> 55\u001b[0m     dataset: Dataset, features_tag: Union[\u001b[38;5;28mstr\u001b[39m, Tags], grouping_tag: Union[\u001b[38;5;28mstr\u001b[39m, Tags] \u001b[38;5;241m=\u001b[39m \u001b[43mTags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mID\u001b[49m\n\u001b[1;32m     56\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    Select unique rows from a Dataset. Returns columns specified by `features_tag`\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m     that are unique based on the columns specified by the `grouping_tag`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m        Dataset\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`unique_rows_by_features` is deprecated and will be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `unique_by_tag` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     79\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3.8/enum.py:384\u001b[0m, in \u001b[0;36mEnumMeta.__getattr__\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_member_map_[name]\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(name) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: ID"
     ]
    }
   ],
   "source": [
    "import merlin.models.tf as mm\n",
    "# import tensorflow as tf\n",
    "# import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3798f61",
   "metadata": {},
   "source": [
    "Below we define an `objective` function. Running it is a single `trial` in our optimization `study`.\n",
    "\n",
    "Each call of the `objective` function will train a model and evaluate its performance on the validation set. It will also return the count of candidates that were purchased across all the customers in the validation set. This is the value that we will strive to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5729b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate_at_100(model):\n",
    "    item_features = train_transformed.schema.select_by_tag(Tags.ITEM_ID).column_names\n",
    "    item_dataset = train_transformed.to_ddf()[item_features].drop_duplicates().compute()\n",
    "    \n",
    "    item_dataset = Dataset(item_dataset)\n",
    "    top_k_rec = model.to_top_k_recommender(item_dataset, 100)\n",
    "    \n",
    "    users_schema = train_transformed.schema.select_by_tag(Tags.USER_ID)\n",
    "    user_features = users_schema.column_names\n",
    "\n",
    "    unique_users = train_transformed.to_ddf()[user_features].drop_duplicates().compute()\n",
    "    users_dataset = Dataset(unique_users, schema=users_schema)\n",
    "    _, cls_idxs = top_k_rec.predict(BatchedDataset(users_dataset, 100, shuffle=False, schema=users_schema))\n",
    "    \n",
    "    customer_id_mapping = get_lib().read_parquet('.//categories/unique.customer_id.parquet')\n",
    "    customer_ids_preds = customer_id_mapping.to_pandas().customer_id.iloc[1:].values\n",
    "    article_id_mapping = get_lib().read_parquet('.//categories/unique.article_id.parquet')\n",
    "    \n",
    "    validation_set_df = validation_set.compute()\n",
    "    validation_set_df = validation_set_df.drop_duplicates(['customer_id', 'article_id'])\n",
    "    val_cust2purchases = validation_set_df.to_pandas().groupby('customer_id')['article_id'].apply(list)\n",
    "    \n",
    "    id2a = article_id_mapping.to_pandas().article_id.to_dict()\n",
    "    \n",
    "    hit_rate = 0\n",
    "    for i in range(cls_idxs.shape[0]):\n",
    "        current_cust = customer_ids_preds[i]\n",
    "        purchases = set(val_cust2purchases[current_cust])\n",
    "        candidates = set(int(id2a[c]) for c in cls_idxs[i])\n",
    "        hit_rate += len(purchases.intersection(candidates))\n",
    "        \n",
    "    return hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf8b01f",
   "metadata": {},
   "source": [
    "Below you will see me define `trial.suggest_float(...)` and `trial.suggest_int(...)`. In this notebook we will perform exhaustive grid search to explore the hyperparameter space. But Optuna comes with many more strategies for finding good hyperparameters.\n",
    "\n",
    "Instead of trying all the values, we could have Optuna intelligently converge to a good set of hyperparameters without having to try out all of their combinations.\n",
    "\n",
    "This is what the `trial.suggest...` syntax facilitates. Unfortunately, even if we want to do an exhaustive search over all the parameters, we stil need to follow Optuna's syntax and specify the permissible ranges a value can take.\n",
    "\n",
    "If you are looking for a follow up activity to this notebook to learn more about HPO, consider replacing the `optuna.samplers.GridSampler` with another hyperparameter search strategy in one of the cells below. What results do you get? How many trials did you have to perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 1, 100)\n",
    "    embedding_dim = trial.suggest_int('embedding_dim', 4, 128)\n",
    "    \n",
    "    model = mm.MatrixFactorizationModel(train_transformed.schema, embedding_dim)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, label_smoothing=0,\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer, loss=loss)\n",
    "\n",
    "    model.fit(train_transformed, validation_data=valid_transformed, batch_size=1024, epochs=num_epochs)\n",
    "    \n",
    "    return calculate_hit_rate_at_100(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c80fe0",
   "metadata": {},
   "source": [
    "For this example, we will use the `GridSampler`. We define parameter values we would like Optuna to perform an exhaustive search over in `search_space`.\n",
    "\n",
    "Optuna features several other samplers that can navigate the search space using various algorithms. You can read about them in the documentation [here](https://optuna.readthedocs.io/en/stable/reference/samplers/index.html).\n",
    "\n",
    "Still, exhaustive search is often the way to go. It guarantees the entire space will be explored and gives us an easy way to refine the experiments (for example, to focus on a specific area in the hyperparameter space we might want to learn more about)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb69fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'learning_rate': [5e-3, 1e-3, 1e-4],\n",
    "    'num_epochs': [3, 6, 9],\n",
    "    'embedding_dim': [16, 32, 48]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59689fb0",
   "metadata": {},
   "source": [
    "Our `study` will execute 27 `trials` (3 * 3 * 3) for us. I am capturing the output below as otherwise the notebook would become very hard to read (a lot information is produced during each training run).\n",
    "\n",
    "We will use a `GridSampler` that will iterate over all the possible combinations of the hyperaparameters we chose for our `study`. This is just one of the many ways supported by `Optuna` for exploring the hyperparameter space. There are many sampler and sampling strategies one might chose from.\n",
    "\n",
    "We set the `n_trials` parameter to 100. Still, `Optuna` will only run 27 runs and will terminate when it has tested all the possible hyperaparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space), direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32339a69",
   "metadata": {},
   "source": [
    "Now that we have performed hyperparameter optimization (a `study`), let's explore the results.\n",
    "\n",
    "We can query the `study` for the best parameters along with the best attained result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eafa7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d483f",
   "metadata": {},
   "source": [
    "The best attained result is displayed below. This is the count of candidates that were purchased in the validation week.\n",
    "\n",
    "It is the value that we want to drive up as high as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d339ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258311ab",
   "metadata": {},
   "source": [
    "We can also draw a parallel coordinate plot to take a look at a graphical depiction of the impact of the hyperparameters.\n",
    "\n",
    "The vertical axis on the left gives us the values our `objective` function returned. Along the horizontal axis we have the parameters whose values we optimized.\n",
    "\n",
    "These graphs are sometimes not easy to read, but here we can clearly see that higher values of the objective function are associated with greater embedding size and possibly lower learning rate (though that association seems to be less strong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e21400",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3312b07d",
   "metadata": {},
   "source": [
    "Contour plots can also often be helpful to understand better the interaction between two parameters.\n",
    "\n",
    "Here we are looking at the learning rate and number of epochs. The medium learning rate value (`0.001` out of the 3 possible values we provided) along with longer training seems to perform best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7cc492",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study,params=['learning_rate','num_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f05b67",
   "metadata": {},
   "source": [
    "Another very useful functionality is hyperparameter importance. This information needs to be taken with a grain of salt, but can help us identify hyperparameters to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c739508",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64864cf1",
   "metadata": {},
   "source": [
    "We can also take a look individually at the values our parameters took and the results we got.\n",
    "\n",
    "It is interesting to note to what extent learning rate of `1e-3` outperforms the other learning rate values across a broad set of other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94772014",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b7913",
   "metadata": {},
   "source": [
    "We can also query the `study` for the number of `trials` that have been run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c34e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(study.trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f44414",
   "metadata": {},
   "source": [
    "Last but not least, we can take a look at each individual run as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fad39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.trials[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd6806c",
   "metadata": {},
   "source": [
    "We can access various pieces of information of each run for custom analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157adef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.trials[0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "runtime_by_epoch_count = defaultdict(lambda: list())\n",
    "\n",
    "for i in range(len(study.trials)):\n",
    "    runtime_by_epoch_count[study.trials[i].params['num_epochs']].append(\n",
    "        (study.trials[i].datetime_complete - study.trials[i].datetime_start).seconds\n",
    "    )\n",
    "\n",
    "for epoch_count in [3, 6, 9]:\n",
    "    print(f'Mean runtime in seconds when training with {epoch_count} epochs: {np.mean(runtime_by_epoch_count[epoch_count]):.02f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8eb3a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have trained a retrieval model and optimized the parameters we used for training using `Optuna`. This approach offered a dramatic improvement in performance. Some hyperaparameter combinations resulted in our candidates containing as few as several hundred of purchased items.\n",
    "\n",
    "With tuned hyperparameters we were able to retrieve 7238 candidates that overlap with items that were actually purchased.\n",
    "\n",
    "`Optuna` is a powerful, open source hyperameter optimization framework that can be leveraged with minimum configuration to achieve good results. More elaborate strategies for searching the hyperparameter space can also be utilized, however performing an exhaustive grid search (how we did above) is a great approach to improving the performance of your model.\n",
    "\n",
    "Other strategies of searching the hyperparameter space can be interesting to experiment with, can reduce the total runtime of a study, can offer greater flexibility, and certainly should be attempted so that you can get a feel for how they fit into your workflow, but might not necessarily lead to better results than exhaustive search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
