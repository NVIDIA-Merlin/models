{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a556f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions anda\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d1452",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_models-transformers-net-item-prediction/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Transformer-based architecture for next-item prediction task\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this use case we will train a Transformer-based architecture for next-item prediction task.\n",
    "\n",
    "We will use the [booking.com dataset](https://github.com/bookingcom/ml-dataset-mdt) to train a session-based model. The dataset contains 1,166,835 of anonymized hotel reservations in the train set and 378,667 in the test set. Each reservation is a part of a customer's trip (identified by `utrip_id`) which includes consecutive reservations.\n",
    "\n",
    "We will reshape the data to organize it into 'sessions'. Each session will be a full customer itinerary in chronological order. The goal will be to predict the city_id of the final reservation of each trip.\n",
    "\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Training a Transformer-based architecture for next-item prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cccd005",
   "metadata": {},
   "source": [
    "## Downloading and preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b619b",
   "metadata": {},
   "source": [
    "We will download the dataset using a functionality provided by merlin models. The dataset can be found on GitHub [here](https://github.com/bookingcom/ml-dataset-mdt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8439829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/NVIDIA-Merlin/Models\n",
      " * branch                main       -> FETCH_HEAD\n",
      "Updating 8ac9090dc..ceed7c5b9\n",
      "Fast-forward\n",
      " .github/workflows/docs-preview-pr.yaml             |  118 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " .github/workflows/docs-sched-rebuild.yaml          |   24 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " .pre-commit-config.yaml                            |    1 \u001b[32m+\u001b[m\n",
      " Makefile                                           |    2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " ci/test_unit.sh                                    |    2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " docs/source/conf.py                                |    4 \u001b[32m+\u001b[m\n",
      " ...2-Merlin-Models-and-NVTabular-integration.ipynb |  238 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " examples/03-Exploring-different-models.ipynb       |  463 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " examples/04-Exporting-ranking-models.ipynb         |  165 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " examples/05-Retrieval-Model.ipynb                  |  893 \u001b[32m+\u001b[m\u001b[31m--\u001b[m\n",
      " ...-your-own-architecture-with-Merlin-Models.ipynb |  509 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " ...ing-of-large-embedding-tables-by-LazyAdam.ipynb |   11 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " .../entertainment-with-pretrained-embeddings.ipynb |   74 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " .../incremental-training-with-layer-freezing.ipynb |  915 \u001b[32m++++\u001b[m\n",
      " examples/usecases/multi-gpu/hvd_wrapper.sh         |   13 \u001b[32m+\u001b[m\n",
      " ...etrieval-with-hyperparameter-optimization.ipynb | 5763 \u001b[32m++++++++++++++++++++\u001b[m\n",
      " merlin/datasets/ecommerce/__init__.py              |    3 \u001b[32m+\u001b[m\n",
      " merlin/datasets/ecommerce/booking/__init__.py      |    0\n",
      " merlin/datasets/ecommerce/booking/dataset.py       |  335 \u001b[32m++\u001b[m\n",
      " .../ecommerce/booking/transformed/__init__.py      |    0\n",
      " .../ecommerce/booking/transformed/schema.pbtxt     |  212 \u001b[32m+\u001b[m\n",
      " merlin/datasets/ecommerce/transactions/__init__.py |   15 \u001b[32m+\u001b[m\n",
      " .../datasets/ecommerce/transactions/schema.pbtxt   |   65 \u001b[32m+\u001b[m\n",
      " merlin/datasets/synthetic.py                       |    2 \u001b[32m+\u001b[m\n",
      " merlin/models/api.py                               |   76 \u001b[32m+\u001b[m\n",
      " merlin/models/io.py                                |   47 \u001b[32m+\u001b[m\n",
      " merlin/models/tf/__init__.py                       |    5 \u001b[32m+\u001b[m\n",
      " merlin/models/tf/blocks/mlp.py                     |  105 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/blocks/optimizer.py               |   18 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/core/aggregation.py               |    4 \u001b[31m-\u001b[m\n",
      " merlin/models/tf/core/base.py                      |    2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/core/combinators.py               |   19 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/core/encoder.py                   |   64 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/core/prediction.py                |   32 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/core/tabular.py                   |   12 \u001b[31m-\u001b[m\n",
      " merlin/models/tf/distributed/__init__.py           |    0\n",
      " merlin/models/tf/distributed/backend.py            |   13 \u001b[32m+\u001b[m\n",
      " merlin/models/tf/inputs/base.py                    |    3 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/inputs/embedding.py               |   52 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/loader.py                         |   49 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/metrics/topk.py                   |   26 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/models/base.py                    |  187 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/models/retrieval.py               |  280 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/outputs/base.py                   |   11 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/outputs/contrastive.py            |   27 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/outputs/topk.py                   |   47 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/prediction_tasks/base.py          |    7 \u001b[32m+\u001b[m\n",
      " merlin/models/tf/transformers/block.py             |    6 \u001b[32m+\u001b[m\n",
      " merlin/models/tf/transformers/transforms.py        |   37 \u001b[32m+\u001b[m\n",
      " merlin/models/tf/transforms/bias.py                |  109 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/transforms/features.py            |   75 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/transforms/tensor.py              |   99 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/tf/utils/tf_utils.py                 |    8 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/utils/example_utils.py               |    8 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " merlin/models/xgb/__init__.py                      |    9 \u001b[32m+\u001b[m\n",
      " pyproject.toml                                     |    1 \u001b[32m+\u001b[m\n",
      " requirements/dev.txt                               |   12 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " requirements/horovod.txt                           |    1 \u001b[32m+\u001b[m\n",
      " setup.py                                           |    3 \u001b[32m+\u001b[m\n",
      " tests/common/tf/retrieval/retrieval_config.py      |   83 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " .../common/tf/retrieval/retrieval_tests_common.py  |    8 \u001b[32m+\u001b[m\n",
      " tests/common/tf/retrieval/retrieval_utils.py       |  259 \u001b[32m+\u001b[m\n",
      " tests/conftest.py                                  |    2 \u001b[32m+\u001b[m\n",
      " .../tf/retrieval/test_integration_retrieval.py     |    8 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/integration/tf/test_ci_01_getting_started.py |    1 \u001b[32m+\u001b[m\n",
      " tests/integration/tf/test_ci_02_dataschema.py      |    1 \u001b[32m+\u001b[m\n",
      " .../tf/test_ci_03_exploring_different_models.py    |    5 \u001b[32m+\u001b[m\n",
      " .../tf/test_ci_06_advanced_own_architecture.py     |    1 \u001b[32m+\u001b[m\n",
      " tests/unit/datasets/test_ecommerce.py              |   25 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/datasets/test_synthetic.py              |    2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/blocks/retrieval/test_two_tower.py   |    3 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/blocks/test_dlrm.py                  |    3 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/blocks/test_interactions.py          |    2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/blocks/test_mlp.py                   |   85 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/core/test_combinators.py             |   32 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/core/test_encoder.py                 |   43 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/examples/test_01_getting_started.py  |    1 \u001b[32m+\u001b[m\n",
      " tests/unit/tf/examples/test_02_dataschema.py       |   13 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " .../examples/test_03_exploring_different_models.py |   13 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " .../tf/examples/test_04_export_ranking_models.py   |   10 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " .../tf/examples/test_05_export_retrieval_model.py  |   14 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " .../examples/test_06_advanced_own_architecture.py  |   10 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " ...test_usecase_accelerate_training_by_lazyadam.py |    2 \u001b[32m+\u001b[m\n",
      " .../test_usecase_ecommerce_session_based.py        |    2 \u001b[32m+\u001b[m\n",
      " ..._usecase_incremental_training_layer_freezing.py |   35 \u001b[32m+\u001b[m\n",
      " .../examples/test_usecase_pretrained_embeddings.py |    4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " .../tf/examples/test_usecase_retrieval_with_hpo.py |   34 \u001b[32m+\u001b[m\n",
      " tests/unit/tf/horovod/__init__.py                  |   18 \u001b[32m+\u001b[m\n",
      " tests/unit/tf/horovod/test_horovod.py              |  160 \u001b[32m+\u001b[m\n",
      " tests/unit/tf/inputs/test_base.py                  |   36 \u001b[32m+\u001b[m\n",
      " tests/unit/tf/inputs/test_continuous.py            |   29 \u001b[32m+\u001b[m\n",
      " tests/unit/tf/inputs/test_embedding.py             |   70 \u001b[32m+\u001b[m\n",
      " tests/unit/tf/metrics/test_metrics_topk.py         |   11 \u001b[32m+\u001b[m\n",
      " tests/unit/tf/models/test_base.py                  |   61 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/models/test_ranking.py               |    3 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/models/test_retrieval.py             |  585 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/outputs/test_base.py                 |   41 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/outputs/test_classification.py       |    2 \u001b[32m+\u001b[m\n",
      " tests/unit/tf/outputs/test_contrastive.py          |   30 \u001b[32m+\u001b[m\n",
      " tests/unit/tf/outputs/test_regression.py           |    1 \u001b[32m+\u001b[m\n",
      " tests/unit/tf/prediction_tasks/test_multi_task.py  |    4 \u001b[32m+\u001b[m\n",
      " tests/unit/tf/transformers/test_block.py           |  100 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/transformers/test_transforms.py      |    8 \u001b[32m+\u001b[m\n",
      " tests/unit/tf/transforms/test_bias.py              |   22 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/transforms/test_features.py          |   56 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/tf/transforms/test_sequence.py          |   10 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " tests/unit/torch/features/test_embedding.py        |    1 \u001b[32m+\u001b[m\n",
      " tox.ini                                            |   53 \u001b[32m+\u001b[m\n",
      " 108 files changed, 11943 insertions(+), 1340 deletions(-)\n",
      " create mode 100644 examples/usecases/incremental-training-with-layer-freezing.ipynb\n",
      " create mode 100644 examples/usecases/multi-gpu/hvd_wrapper.sh\n",
      " create mode 100644 examples/usecases/retrieval-with-hyperparameter-optimization.ipynb\n",
      " create mode 100644 merlin/datasets/ecommerce/booking/__init__.py\n",
      " create mode 100644 merlin/datasets/ecommerce/booking/dataset.py\n",
      " create mode 100644 merlin/datasets/ecommerce/booking/transformed/__init__.py\n",
      " create mode 100644 merlin/datasets/ecommerce/booking/transformed/schema.pbtxt\n",
      " create mode 100644 merlin/datasets/ecommerce/transactions/__init__.py\n",
      " create mode 100644 merlin/datasets/ecommerce/transactions/schema.pbtxt\n",
      " create mode 100644 merlin/models/api.py\n",
      " create mode 100644 merlin/models/io.py\n",
      " create mode 100644 merlin/models/tf/distributed/__init__.py\n",
      " create mode 100644 merlin/models/tf/distributed/backend.py\n",
      " create mode 100644 requirements/horovod.txt\n",
      " create mode 100644 tests/unit/tf/examples/test_usecase_incremental_training_layer_freezing.py\n",
      " create mode 100644 tests/unit/tf/examples/test_usecase_retrieval_with_hpo.py\n",
      " create mode 100644 tests/unit/tf/horovod/__init__.py\n",
      " create mode 100644 tests/unit/tf/horovod/test_horovod.py\n",
      " create mode 100644 tests/unit/tf/inputs/test_base.py\n",
      " create mode 100644 tox.ini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /models\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: merlin-core>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-models==0.9.0+44.gceed7c5b9) (0.8.0)\n",
      "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (3.19.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (4.64.1)\n",
      "Requirement already satisfied: distributed>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (2022.5.1)\n",
      "Requirement already satisfied: dask>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (2022.5.1)\n",
      "Requirement already satisfied: betterproto<2.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.2.5)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (7.0.0)\n",
      "Requirement already satisfied: fsspec==2022.5.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (2022.5.0)\n",
      "Requirement already satisfied: pandas<1.4.0dev0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.3.5)\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (0.56.3)\n",
      "Requirement already satisfied: tensorflow-metadata>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (3.0.9)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (2.2.0)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.26.12)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (2.2.0)\n",
      "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (6.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (6.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (3.1.2)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (0.12.0)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (2.4.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.0.4)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (8.1.3)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.7.0)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (5.9.3)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.3.0)\n",
      "Requirement already satisfied: grpclib in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (0.4.3)\n",
      "Requirement already satisfied: stringcase in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from pyarrow>=5.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.22.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (2.8.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (45.2.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (0.39.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (5.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.56.4)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.3.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (2.1.1)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (4.1.0)\n",
      "Requirement already satisfied: multidict in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (6.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (1.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata; python_version < \"3.9\"->numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (3.10.0)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (4.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+44.gceed7c5b9) (6.0.1)\n",
      "Building wheels for collected packages: merlin-models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for merlin-models (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for merlin-models: filename=merlin_models-0.9.0+44.gceed7c5b9-py3-none-any.whl size=364967 sha256=a90747c5808af4158496f1ec8323c7dd845d22638135eea35ce95580b82bd5e1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fp9i6g7q/wheels/4d/e8/98/0493db55fff90dc9af123f55a9455b96f7f8166c912a02c8a6\n",
      "Successfully built merlin-models\n",
      "Installing collected packages: merlin-models\n",
      "  Attempting uninstall: merlin-models\n",
      "    Found existing installation: merlin-models 0.9.0\n",
      "    Uninstalling merlin-models-0.9.0:\n",
      "      Successfully uninstalled merlin-models-0.9.0\n",
      "Successfully installed merlin-models-0.9.0+44.gceed7c5b9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# !cd /models && git pull origin main && pip install .\n",
    "# # cd /nvtabular && git pull && pip install .\n",
    "# # cd /core && git pull && pip install .\n",
    "# # cd /systems && git pull && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9dccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-11-16 02:14:54.749335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-16 02:14:54.749755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-16 02:14:54.749957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-16 02:14:55.039058: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-16 02:14:55.039956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-16 02:14:55.040168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-16 02:14:55.040326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-16 02:14:55.781809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-16 02:14:55.782019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-16 02:14:55.782177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-16 02:14:55.782307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24576 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:08:00.0, compute capability: 7.5\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.SESSION_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.SESSION: 'session'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from merlin.core.dispatch import get_lib\n",
    "from merlin.datasets.ecommerce import get_booking\n",
    "import numpy as np\n",
    "\n",
    "from nvtabular import *\n",
    "from nvtabular import ops\n",
    "from merlin.models.tf import Loader\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "get_booking('/workspace/data')\n",
    "train = get_lib().read_csv('/workspace/data/train_set.csv', parse_dates=['checkin', 'checkout'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b7224",
   "metadata": {},
   "source": [
    "Each reservation has a unique `utrip_id`. During each trip a customer vists several destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192a5727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000027</td>\n",
       "      <td>2016-08-13</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>8183</td>\n",
       "      <td>desktop</td>\n",
       "      <td>7168</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>1000027_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000027</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>2016-08-16</td>\n",
       "      <td>15626</td>\n",
       "      <td>desktop</td>\n",
       "      <td>7168</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>1000027_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000027</td>\n",
       "      <td>2016-08-16</td>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>60902</td>\n",
       "      <td>desktop</td>\n",
       "      <td>7168</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>1000027_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000027</td>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>30628</td>\n",
       "      <td>desktop</td>\n",
       "      <td>253</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>1000027_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000033</td>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>2016-04-11</td>\n",
       "      <td>38677</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>1000033_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id    checkin   checkout  city_id device_class  affiliate_id  \\\n",
       "0  1000027 2016-08-13 2016-08-14     8183      desktop          7168   \n",
       "1  1000027 2016-08-14 2016-08-16    15626      desktop          7168   \n",
       "2  1000027 2016-08-16 2016-08-18    60902      desktop          7168   \n",
       "3  1000027 2016-08-18 2016-08-21    30628      desktop           253   \n",
       "4  1000033 2016-04-09 2016-04-11    38677       mobile           359   \n",
       "\n",
       "  booker_country hotel_country   utrip_id  \n",
       "0        Elbonia        Gondal  1000027_1  \n",
       "1        Elbonia        Gondal  1000027_1  \n",
       "2        Elbonia        Gondal  1000027_1  \n",
       "3        Elbonia        Gondal  1000027_1  \n",
       "4         Gondal  Cobra Island  1000033_1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc2d94",
   "metadata": {},
   "source": [
    "We will train on sequences of `city_id` and `booker_country` and based on this information, our model will attempt to predict the next `city_id` (the next hop in the journey).\n",
    "\n",
    "We will train a transformer model that can work with sequences of variable length within a batch. This functionality is provided to us out of the box and doesn't require any changes to the architecture. Thanks to it we do not have to pad or trim our sequences to any particular length -- our model can make effective use of all of the data!\n",
    "\n",
    "With one exception. For a masked language model that we will be training, we need to discard sequences that are shorter than two hops. This makes sense as there is nothing our model could learn if it was only presented with an itinerary with a single destination on it!\n",
    "\n",
    "Let us begin by splitting the data into a train and validation set based on trip ID.\n",
    "\n",
    "Let's see how many unique trips there are in the dataset. Also, let us shuffle the trips along the way so that our validation set consists of a random sample of our train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23bef6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217686"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utrip_ids = train.sample(frac=1).utrip_id.unique()\n",
    "len(utrip_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eca1f6",
   "metadata": {},
   "source": [
    "Now let's assign data to our train and validation sets. Furthermore, we sort the data by `utrip_id` and `checkin`. This way we ensure our sequences of visited `city_ids` will be in proper order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7754847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_utrip_ids = utrip_ids[:int(0.8 * utrip_ids.shape[0])]\n",
    "validation_set_utrip_ids = utrip_ids[int(0.8 * utrip_ids.shape[0]):]\n",
    "\n",
    "train_set = train[train.utrip_id.isin(train_set_utrip_ids)].sort_values(['utrip_id', 'checkin'])\n",
    "validation_set = train[train.utrip_id.isin(validation_set_utrip_ids)].sort_values(['utrip_id', 'checkin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc3992",
   "metadata": {},
   "source": [
    "We can now begin with data preprocessing.\n",
    "\n",
    "We will combine trips into \"sessions\", discard trips that are too short and calculate total trip length.\n",
    "\n",
    "We will use NVTabular for this work. It offers optimized tabular data preprocessing operators that run on the GPU. If you would like to learn more about the NVTabular library, please take a look [here](https://github.com/NVIDIA-Merlin/NVTabular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3435af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_dataset = Dataset(train_set)\n",
    "validation_set_dataset = Dataset(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60bd5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_checkin = (\n",
    "    [\"checkin\"]\n",
    "    >> ops.LambdaOp(lambda col: col.dt.weekday)\n",
    "    >> ops.Rename(name=\"weekday_checkin\")\n",
    ")\n",
    "\n",
    "weekday_checkout = (\n",
    "    [\"checkout\"]\n",
    "    >> ops.LambdaOp(lambda col: col.dt.weekday)\n",
    "    >> ops.Rename(name=\"weekday_checkout\")\n",
    ")\n",
    "\n",
    "\n",
    "groupby_features = ['city_id', 'booker_country', 'utrip_id', 'hotel_country', 'checkin'] + weekday_checkin + weekday_checkout >> ops.Groupby(\n",
    "    groupby_cols=['utrip_id'],\n",
    "    aggs={\n",
    "        'city_id': ['list', 'count'],\n",
    "        'booker_country': ['list'],\n",
    "        'hotel_country': ['list'],\n",
    "        'weekday_checkin': ['list'],\n",
    "        'weekday_checkout': ['list']\n",
    "    },\n",
    "    sort_cols=\"checkin\"\n",
    ")\n",
    "\n",
    "groupby_features_city = groupby_features['city_id_list'] >> ops.Categorify() >> ops.AddTags([Tags.SEQUENCE, Tags.ITEM, Tags.ITEM_ID])\n",
    "groupby_features_country = (\n",
    "    groupby_features['booker_country_list', 'hotel_country_list', 'weekday_checkin_list', 'weekday_checkout_list']\n",
    "    >> ops.Categorify() >> ops.AddTags([Tags.SEQUENCE, Tags.ITEM])\n",
    ")\n",
    "city_id_count = groupby_features['city_id_count'] >> ops.AddTags([Tags.CONTEXT, Tags.ITEM, Tags.CONTINUOUS])\n",
    "\n",
    "# Filter out sessions with less than 2 interactions \n",
    "MINIMUM_SESSION_LENGTH = 2\n",
    "filtered_sessions = groupby_features_city + groupby_features_country + city_id_count >> ops.Filter(f=lambda df: df[\"city_id_count\"] >= MINIMUM_SESSION_LENGTH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6105767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = Workflow(filtered_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edcbcdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = wf.fit_transform(train_set_dataset)\n",
    "validation_set_processed = wf.transform(validation_set_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a6675",
   "metadata": {},
   "source": [
    "Our data consists of a sequence of visited `city_ids`, a sequence of `booker_countries` (represented as integer categories) and a `city_id_count` column (which contains the count of visited cities in a trip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dee6b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id_list</th>\n",
       "      <th>booker_country_list</th>\n",
       "      <th>hotel_country_list</th>\n",
       "      <th>weekday_checkin_list</th>\n",
       "      <th>weekday_checkout_list</th>\n",
       "      <th>city_id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8240, 156, 2278, 2097]</td>\n",
       "      <td>[3, 3, 3, 3]</td>\n",
       "      <td>[3, 3, 3, 3]</td>\n",
       "      <td>[5, 7, 4, 3]</td>\n",
       "      <td>[7, 4, 2, 7]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[63, 1160, 87, 619, 63]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[5, 1, 4, 3, 5]</td>\n",
       "      <td>[6, 4, 2, 5, 4]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[7, 6, 24, 1051, 65, 52, 3]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>[2, 2, 2, 16, 16, 3, 3]</td>\n",
       "      <td>[5, 1, 2, 6, 5, 7, 4]</td>\n",
       "      <td>[6, 3, 1, 5, 7, 4, 3]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1032, 757, 140, 3]</td>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "      <td>[19, 19, 19, 3]</td>\n",
       "      <td>[1, 4, 2, 3]</td>\n",
       "      <td>[4, 3, 2, 5]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3603, 262, 662, 250, 359]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[30, 30, 30, 30, 30]</td>\n",
       "      <td>[1, 3, 6, 5, 1]</td>\n",
       "      <td>[2, 1, 5, 6, 3]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  city_id_list    booker_country_list  \\\n",
       "0      [8240, 156, 2278, 2097]           [3, 3, 3, 3]   \n",
       "1      [63, 1160, 87, 619, 63]        [1, 1, 1, 1, 1]   \n",
       "2  [7, 6, 24, 1051, 65, 52, 3]  [2, 2, 2, 2, 2, 2, 2]   \n",
       "3          [1032, 757, 140, 3]           [2, 2, 2, 2]   \n",
       "4   [3603, 262, 662, 250, 359]        [1, 1, 1, 1, 1]   \n",
       "\n",
       "        hotel_country_list   weekday_checkin_list  weekday_checkout_list  \\\n",
       "0             [3, 3, 3, 3]           [5, 7, 4, 3]           [7, 4, 2, 7]   \n",
       "1          [1, 1, 1, 1, 1]        [5, 1, 4, 3, 5]        [6, 4, 2, 5, 4]   \n",
       "2  [2, 2, 2, 16, 16, 3, 3]  [5, 1, 2, 6, 5, 7, 4]  [6, 3, 1, 5, 7, 4, 3]   \n",
       "3          [19, 19, 19, 3]           [1, 4, 2, 3]           [4, 3, 2, 5]   \n",
       "4     [30, 30, 30, 30, 30]        [1, 3, 6, 5, 1]        [2, 1, 5, 6, 3]   \n",
       "\n",
       "   city_id_count  \n",
       "0              4  \n",
       "1              5  \n",
       "2              7  \n",
       "3              4  \n",
       "4              5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_processed.compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89cc3a0",
   "metadata": {},
   "source": [
    "We are now ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7ee9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import merlin.models.tf as mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95c794",
   "metadata": {},
   "source": [
    "Let's identify two schemas. The first one for sequential features, the other for context features (`city_id_count`) that we will broadcast to the entire sequence.\n",
    "\n",
    "Here is the schema of the data that our model will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4813456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>city_id_list</td>\n",
       "      <td>(Tags.ITEM_ID, Tags.CATEGORICAL, Tags.ID, Tags...</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.city_id_list.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>37202</td>\n",
       "      <td>city_id_list</td>\n",
       "      <td>37203</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>booker_country_list</td>\n",
       "      <td>(Tags.SEQUENCE, Tags.CATEGORICAL, Tags.ITEM)</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.booker_country_list.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>booker_country_list</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hotel_country_list</td>\n",
       "      <td>(Tags.SEQUENCE, Tags.CATEGORICAL, Tags.ITEM)</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.hotel_country_list.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "      <td>hotel_country_list</td>\n",
       "      <td>195</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weekday_checkin_list</td>\n",
       "      <td>(Tags.SEQUENCE, Tags.CATEGORICAL, Tags.ITEM)</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.weekday_checkin_list.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>weekday_checkin_list</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekday_checkout_list</td>\n",
       "      <td>(Tags.SEQUENCE, Tags.CATEGORICAL, Tags.ITEM)</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.weekday_checkout_list.par...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>weekday_checkout_list</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'city_id_list', 'tags': {<Tags.ITEM_ID: 'item_id'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ID: 'id'>, <Tags.ITEM: 'item'>, <Tags.SEQUENCE: 'sequence'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.city_id_list.parquet', 'domain': {'min': 0, 'max': 37202, 'name': 'city_id_list'}, 'embedding_sizes': {'cardinality': 37203, 'dimension': 512}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': True}, {'name': 'booker_country_list', 'tags': {<Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.booker_country_list.parquet', 'domain': {'min': 0, 'max': 5, 'name': 'booker_country_list'}, 'embedding_sizes': {'cardinality': 6, 'dimension': 16}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': True}, {'name': 'hotel_country_list', 'tags': {<Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.hotel_country_list.parquet', 'domain': {'min': 0, 'max': 194, 'name': 'hotel_country_list'}, 'embedding_sizes': {'cardinality': 195, 'dimension': 31}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': True}, {'name': 'weekday_checkin_list', 'tags': {<Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.weekday_checkin_list.parquet', 'domain': {'min': 0, 'max': 7, 'name': 'weekday_checkin_list'}, 'embedding_sizes': {'cardinality': 8, 'dimension': 16}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': True}, {'name': 'weekday_checkout_list', 'tags': {<Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.weekday_checkout_list.parquet', 'domain': {'min': 0, 'max': 7, 'name': 'weekday_checkout_list'}, 'embedding_sizes': {'cardinality': 8, 'dimension': 16}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': True}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_schema = train_set_processed.schema.select_by_tag(Tags.SEQUENCE)\n",
    "seq_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d422833",
   "metadata": {},
   "source": [
    "Let's also identify the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e34a3a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'city_id_list'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = train_set_processed.schema.select_by_tag(Tags.SEQUENCE).column_names[0]\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8adad",
   "metadata": {},
   "source": [
    "## Constructing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c465e01e",
   "metadata": {},
   "source": [
    "We begin by defining our `Loader`. It will be responsible for batching our data and passing it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba9e1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(train_set_processed, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf02dc",
   "metadata": {},
   "source": [
    "And now onto model construction.\n",
    "\n",
    "We can specify various hyperparameters, such as the number of heads and number of layers to use.\n",
    "\n",
    "For the transformer portion of our model, we will use the `XLNet` architecture. Additionally, we are passing `mm.ReplaceMaskedEmbeddings()` as our `pre` block. We will be training a masked language model and this parameter is responsible for the masking of our sequences.\n",
    "\n",
    "Later, when we run the `fit` method on our model, we will specify the `masking_probability` of `0.3`. Through the combination of these parameters, our model will train on sequences where any given timestep will be masked with a probability of 0.3 and it will be our model's training task to infer the target value for that step!\n",
    "\n",
    "To summarize, Masked Language Modeling is implemented by using two blocks in combination:\n",
    "\n",
    "* `SequenceMaskRandom()` - Used as a pre for `model.fit()`, it randomly selects items from the sequence to be masked for prediction as targets, by using Keras masking.\n",
    "* `ReplaceMaskedEmbeddings()` - Used as a pre for a `TransformerBlock`, it replaces the input embeddings at masked positions for prediction by a dummy trainable embedding, to avoid leakage of the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cddfd424",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.Model(\n",
    "    mm.InputBlockV2(\n",
    "        seq_schema,\n",
    "        embeddings=mm.Embeddings(\n",
    "            train_set_processed.schema.select_by_tag(Tags.CATEGORICAL), sequence_combiner=None\n",
    "        ),\n",
    "    ),\n",
    "    mm.XLNetBlock(d_model=64, n_head=4, n_layer=2, pre=mm.ReplaceMaskedEmbeddings()),\n",
    "    mm.CategoricalOutput(\n",
    "        train_set_processed.schema.select_by_name(target),\n",
    "        default_loss=\"categorical_crossentropy\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac975cd",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65d28c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 02:16:54.558116: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['model/embeddings:0', 'model/embeddings:0', 'model/embeddings:0', 'model/embeddings:0', 'model/embeddings:0', 'model/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/xl_net_block/prepare_transformer_inputs_4/RaggedToTensor/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/xl_net_block/prepare_transformer_inputs_4/RaggedToTensor/boolean_mask/GatherV2:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/xl_net_block/prepare_transformer_inputs_4/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/xl_net_block/replace_masked_embeddings/RaggedWhere/Reshape_3:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/model/xl_net_block/replace_masked_embeddings/RaggedWhere/Reshape_2:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/xl_net_block/replace_masked_embeddings/RaggedWhere/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/xl_net_block/replace_masked_embeddings/RaggedWhere/RaggedTile_2/Reshape_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/concat_features/RaggedConcat/Slice_1:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/concat_features/RaggedConcat/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/xl_net_block/replace_masked_embeddings/RaggedWhere/RaggedTile_2/Reshape_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/concat_features/RaggedConcat/Slice_3:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/concat_features/RaggedConcat/Shape_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/xl_net_block/replace_masked_embeddings/RaggedWhere/RaggedTile_2/Reshape_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/concat_features/RaggedConcat/Slice_5:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/concat_features/RaggedConcat/Shape_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/xl_net_block/replace_masked_embeddings/RaggedWhere/RaggedTile_2/Reshape_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/concat_features/RaggedConcat/Slice_7:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/concat_features/RaggedConcat/Shape_3:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/xl_net_block/replace_masked_embeddings/RaggedWhere/RaggedTile_2/Reshape_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/concat_features/RaggedConcat/Slice_9:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/concat_features/RaggedConcat/Shape_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['model/embeddings:0', 'model/embeddings:0', 'model/embeddings:0', 'model/embeddings:0', 'model/embeddings:0', 'model/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 02:17:02.465694: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: model/xl_net_block/replace_masked_embeddings/RaggedWhere/Assert/AssertGuard/branch_executed/_9\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2720/2720 [==============================] - 113s 37ms/step - loss: 0.9611 - recall_at_10: 0.0930 - mrr_at_10: 0.0420 - ndcg_at_10: 0.0540 - map_at_10: 0.0420 - precision_at_10: 0.0093 - regularization_loss: 0.0000e+00 - loss_batch: 0.9612\n",
      "Epoch 2/5\n",
      "2720/2720 [==============================] - 106s 37ms/step - loss: 0.7824 - recall_at_10: 0.1500 - mrr_at_10: 0.0710 - ndcg_at_10: 0.0896 - map_at_10: 0.0710 - precision_at_10: 0.0150 - regularization_loss: 0.0000e+00 - loss_batch: 0.7824\n",
      "Epoch 3/5\n",
      "2720/2720 [==============================] - 106s 38ms/step - loss: 0.7318 - recall_at_10: 0.1602 - mrr_at_10: 0.0765 - ndcg_at_10: 0.0962 - map_at_10: 0.0765 - precision_at_10: 0.0160 - regularization_loss: 0.0000e+00 - loss_batch: 0.7319\n",
      "Epoch 4/5\n",
      "2720/2720 [==============================] - 106s 37ms/step - loss: 0.7130 - recall_at_10: 0.1691 - mrr_at_10: 0.0812 - ndcg_at_10: 0.1019 - map_at_10: 0.0812 - precision_at_10: 0.0169 - regularization_loss: 0.0000e+00 - loss_batch: 0.7131\n",
      "Epoch 5/5\n",
      "2720/2720 [==============================] - 107s 38ms/step - loss: 0.6977 - recall_at_10: 0.1770 - mrr_at_10: 0.0857 - ndcg_at_10: 0.1073 - map_at_10: 0.0857 - precision_at_10: 0.0177 - regularization_loss: 0.0000e+00 - loss_batch: 0.6978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff8f8330f10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(run_eagerly=False, optimizer='adam', loss=\"categorical_crossentropy\")\n",
    "model.fit(loader, epochs=5, pre=mm.SequenceMaskRandom(schema=seq_schema, target=target, masking_prob=0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24699106",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d87d27",
   "metadata": {},
   "source": [
    "We have trained our model.\n",
    "\n",
    "But in training the metrics come from a masked language modelling task. A portion of steps in the sequence was masked for each example. The metrics were calculated on this task.\n",
    "\n",
    "In reality, we probably care how well our model does on the next item prediction task (as it mimics the scenario in which the model would be likely to be used).\n",
    "\n",
    "Let's measure the performance of the model on a task where it attempts to predict the last item in a sequence.\n",
    "\n",
    "We will mask the last item using `SequenceMaskLast` and run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c662af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_eval = Loader(validation_set_processed, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb3c6358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 02:25:57.739742: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: model/xl_net_block/replace_masked_embeddings/RaggedWhere/Assert/AssertGuard/branch_executed/_9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 21s 52ms/step - loss: 0.3228 - recall_at_10: 0.5691 - mrr_at_10: 0.3137 - ndcg_at_10: 0.3746 - map_at_10: 0.3137 - precision_at_10: 0.0569 - regularization_loss: 0.0000e+00 - loss_batch: 0.3227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3228282928466797,\n",
       " 0.5691077709197998,\n",
       " 0.31373801827430725,\n",
       " 0.3745550513267517,\n",
       " 0.31373801827430725,\n",
       " 0.05691079422831535,\n",
       " 0.2738809883594513,\n",
       " 0.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(loader_eval, batch_size=128, pre=mm.SequenceMaskLast(schema=validation_set_processed.schema, target=target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
