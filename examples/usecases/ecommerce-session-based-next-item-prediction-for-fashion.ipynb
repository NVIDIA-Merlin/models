{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef5df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854815b0",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_models_ecommerce-session-based-next-item-prediction-for-fashion/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Session-Based Next Item Prediction for Fashion E-Commerce\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container. \n",
    "\n",
    "## Overview\n",
    "\n",
    "NVIDIA-Merlin team participated in [Recsys2022 challenge](http://www.recsyschallenge.com/2022/index.html) and secured 3rd position. This notebook contains the various techniques used in the solution.\n",
    "\n",
    "### Learning Objective\n",
    "\n",
    "In this notebook, we will apply important concepts that improve recommender systems. We leveraged them for our RecSys solution:\n",
    "- MultiClass next item prediction head with Merlin Models\n",
    "- Sequential input features representing user sessions\n",
    "- Label Smoothing \n",
    "- Temperature Scaling\n",
    "- Weight Tying\n",
    "- Learning Rate Scheduler\n",
    "\n",
    "### Brief Description of the Concepts\n",
    "\n",
    "##### Label smoothing\n",
    "In recommender systems, we often have noisy datasets. A user cannot view all items to make the best decision. Noisy examples can result in high gradients and confuse the model. Label smoothing addresses the problem of noisy examples by smoothing the porbabilities to avoid high confident predictions.\n",
    "\n",
    "$$  \\begin{array}{l}\n",
    "y_{l} \\ =\\ ( 1\\ -\\ \\alpha \\ ) \\ *\\ y_{o} \\ +\\ ( \\alpha \\ /\\ L)\\\\\n",
    "\\alpha :\\ Label\\ smoothing\\ hyper-parameter\\ ( 0 \\leq \\alpha \\leq 1 ) \\\\\n",
    "L:\\ Total\\ number\\ of\\ label\\ classes\\\\\n",
    "y_{o} :\\ One-hot\\ encoded\\ label\\ vector\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "When α is 0, we have the original one-hot encoded labels, and as α increases, we move towards smoothed labels. Read [this](https://arxiv.org/abs/1906.02629) paper to learn more about it.\n",
    "\n",
    "\n",
    "##### Temperature Scaling\n",
    "Similar to Label Smoothing, Temperature Scaling is done to reduce the overconfidence of a model. In this, we divide the logits (inputs to the softmax function) by a scalar parameter (T) . For more information on Temperature Scaling read [this](https://arxiv.org/pdf/1706.04599.pdf) paper.\n",
    "$$ softmax\\ =\\ \\frac{e\\ ^{( z_{i} \\ /\\ \\ T)}}{\\sum _{j} \\ e^{( z_{j} \\ /\\ T)} \\ } $$\n",
    "\n",
    "\n",
    "##### Weight Tying\n",
    "Weight Tying can be applied for Multi-Class Classification problems, when we try to predict items and have previous viewed items as an input. The final output layer (without activation function) is multiplied with the same item embeddings table to represent the input items, resulting in a vector with a logit for each item id. The advantage is that the gradients flow to the item embeddings are short. For more information read [this](https://arxiv.org/pdf/1608.05859v3.pdf) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af678b4c",
   "metadata": {},
   "source": [
    "## Downloading and preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee802a",
   "metadata": {},
   "source": [
    "We will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0e1975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-11-15 23:36:41.283384: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 23:36:42.472778: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-11-15 23:36:42.472873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "import glob\n",
    "\n",
    "import nvtabular as nvt\n",
    "from merlin.io import Dataset\n",
    "from merlin.schema import Schema, Tags\n",
    "from nvtabular.ops import (\n",
    "    AddMetadata,\n",
    ")\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.models.tf import InputBlock\n",
    "from merlin.models.tf.models.base import Model\n",
    "from merlin.models.tf.transforms.bias import LogitsTemperatureScaler\n",
    "from merlin.models.tf.prediction_tasks.next_item import ItemsPredictionWeightTying\n",
    "\n",
    "from merlin.core.dispatch import get_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92ec55",
   "metadata": {},
   "source": [
    "###  Dressipi\n",
    "[Dressipi](http://www.recsyschallenge.com/2022/dataset.html) hosted the [Recsys2022 challenge](http://www.recsyschallenge.com/2022/index.html) and provided an anonymized dataset. It contains 1.1 M online retail sessions that resulted in a purchase. It provides details about items that were viewed in a session, the item purchased at the end of the session and numerous features of those items. The item features are categorical IDs and are not interpretable.\n",
    "\n",
    "The task of this competition was, given a sequence of items predict which item will be purchased at the end of a session.\n",
    "\n",
    "<img src=\"http://www.recsyschallenge.com/2022/images/session_purchase_data.jpeg\" alt=\"dressipi_dataset\" style=\"width: 400px; float: center;\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afab83f",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We provide a function `get_dressipi2022` which preprocess the dataset. Currently, we can't download this dataset automatically so this needs to be downloaded manually. To use this function, prepare the data by following these 3 steps:\n",
    "1. Sign up and download the data from [dressipi-recsys2022.com](https://www.dressipi-recsys2022.com/).\n",
    "2. Unzip the raw data to a directory.\n",
    "3. Define `DATA_FOLDER` to the directory\n",
    "\n",
    "In case you do not want to use this dataset to run our examples, you can also opt for synthetic data. Synthetic data can be generated by running::\n",
    "\n",
    "```python\n",
    "    from merlin.datasets.synthetic import generate_data\n",
    "    train, valid = generate_data(\"dressipi2022-preprocessed\", num_rows=10000, set_sizes=(0.8, 0.2))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd734f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.datasets.ecommerce import get_dressipi2022\n",
    "\n",
    "DATA_FOLDER = os.environ.get(\n",
    "    \"DATA_FOLDER\", \n",
    "    '/workspace/data/dressipi_recsys2022'\n",
    ")\n",
    "\n",
    "train, valid = get_dressipi2022(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6edf8",
   "metadata": {},
   "source": [
    "The dataset contains:\n",
    "- `session_id`, id of a session, in which a user viewed and purchased an item. \n",
    "- `item_id` which was viewed at a given `timestamp` in a session\n",
    "- `purchase_id` which is the id of item bought at the end of the session \n",
    "\n",
    "In addition to `timestamp`, we have `day` and `date` features for representing the chronological order in which items were viewed.\n",
    "\n",
    "The items in the Dresspi dataset had a many features out of which we took 22 most important features, namely \n",
    "`f_3 ,f_4 ,f_5 ,f_7 ,f_17 ,f_24 ,f_30 ,f_45 ,f_46 ,f_47 ,f_50 ,f_53 ,f_55 ,f_56 ,f_58 ,f_61 ,f_63 ,f_65 ,f_68 ,f_69 ,f_72 ,f_73`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e362d59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_45</th>\n",
       "      <th>f_47</th>\n",
       "      <th>...</th>\n",
       "      <th>f_61</th>\n",
       "      <th>f_63</th>\n",
       "      <th>f_65</th>\n",
       "      <th>f_68</th>\n",
       "      <th>f_69</th>\n",
       "      <th>f_72</th>\n",
       "      <th>f_73</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>day</th>\n",
       "      <th>purchase_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21970</td>\n",
       "      <td>26077</td>\n",
       "      <td>2020-04-27 06:39:19.329</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>619</td>\n",
       "      <td>378</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>706</td>\n",
       "      <td>861</td>\n",
       "      <td>521</td>\n",
       "      <td>662</td>\n",
       "      <td>592</td>\n",
       "      <td>263</td>\n",
       "      <td>544</td>\n",
       "      <td>1587969559329</td>\n",
       "      <td>117</td>\n",
       "      <td>24868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21970</td>\n",
       "      <td>6140</td>\n",
       "      <td>2020-04-27 06:39:46.460</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>619</td>\n",
       "      <td>378</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>706</td>\n",
       "      <td>861</td>\n",
       "      <td>521</td>\n",
       "      <td>383</td>\n",
       "      <td>592</td>\n",
       "      <td>655</td>\n",
       "      <td>544</td>\n",
       "      <td>1587969586460</td>\n",
       "      <td>117</td>\n",
       "      <td>24868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21970</td>\n",
       "      <td>19770</td>\n",
       "      <td>2020-04-27 06:40:13.598</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>536</td>\n",
       "      <td>378</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>706</td>\n",
       "      <td>861</td>\n",
       "      <td>521</td>\n",
       "      <td>97</td>\n",
       "      <td>592</td>\n",
       "      <td>7</td>\n",
       "      <td>544</td>\n",
       "      <td>1587969613598</td>\n",
       "      <td>117</td>\n",
       "      <td>24868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21970</td>\n",
       "      <td>6140</td>\n",
       "      <td>2020-04-27 06:41:03.256</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>619</td>\n",
       "      <td>378</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>706</td>\n",
       "      <td>861</td>\n",
       "      <td>521</td>\n",
       "      <td>383</td>\n",
       "      <td>592</td>\n",
       "      <td>655</td>\n",
       "      <td>544</td>\n",
       "      <td>1587969663256</td>\n",
       "      <td>117</td>\n",
       "      <td>24868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21970</td>\n",
       "      <td>22885</td>\n",
       "      <td>2020-04-27 06:41:40.288</td>\n",
       "      <td>793</td>\n",
       "      <td>605</td>\n",
       "      <td>798</td>\n",
       "      <td>378</td>\n",
       "      <td>-1</td>\n",
       "      <td>559</td>\n",
       "      <td>549</td>\n",
       "      <td>...</td>\n",
       "      <td>706</td>\n",
       "      <td>861</td>\n",
       "      <td>521</td>\n",
       "      <td>745</td>\n",
       "      <td>592</td>\n",
       "      <td>75</td>\n",
       "      <td>544</td>\n",
       "      <td>1587969700288</td>\n",
       "      <td>117</td>\n",
       "      <td>24868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id  item_id                    date  f_3  f_5  f_7  f_17  f_24  \\\n",
       "0       21970    26077 2020-04-27 06:39:19.329   -1   -1  619   378    -1   \n",
       "1       21970     6140 2020-04-27 06:39:46.460   -1   -1  619   378    -1   \n",
       "2       21970    19770 2020-04-27 06:40:13.598   -1   -1  536   378    -1   \n",
       "3       21970     6140 2020-04-27 06:41:03.256   -1   -1  619   378    -1   \n",
       "4       21970    22885 2020-04-27 06:41:40.288  793  605  798   378    -1   \n",
       "\n",
       "   f_45  f_47  ...  f_61  f_63  f_65  f_68  f_69  f_72  f_73      timestamp  \\\n",
       "0    -1    96  ...   706   861   521   662   592   263   544  1587969559329   \n",
       "1    -1    96  ...   706   861   521   383   592   655   544  1587969586460   \n",
       "2    -1    96  ...   706   861   521    97   592     7   544  1587969613598   \n",
       "3    -1    96  ...   706   861   521   383   592   655   544  1587969663256   \n",
       "4   559   549  ...   706   861   521   745   592    75   544  1587969700288   \n",
       "\n",
       "   day  purchase_id  \n",
       "0  117        24868  \n",
       "1  117        24868  \n",
       "2  117        24868  \n",
       "3  117        24868  \n",
       "4  117        24868  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.to_ddf().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4235e",
   "metadata": {},
   "source": [
    "## Feature Engineering with NVTabular\n",
    "\n",
    "We use NVTabular for Feature Engineering. If you want to learn more about NVTabular, we recommend the [examples in the NVTabular GitHub Repository](https://github.com/NVIDIA-Merlin/NVTabular/tree/main/examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5e2b7",
   "metadata": {},
   "source": [
    "### Categorify\n",
    "\n",
    "We want to use embedding layers for our categorical features. First, we need to Categorify them, that they are contiguous integers. \n",
    "\n",
    "The features `item_id` and `purchase_id` belongs to the same category. If `item_id` is 8432 and `purchase_id` is 8432, they are the same item. When we want to apply Categorify, we want to keep the connection. We can achieve this by encoding them jointly by providing them as a list in the list `[['item_id', 'purchase_id']]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389a708",
   "metadata": {},
   "source": [
    "We will use only 2 of the categorical item features in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f834311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 86 µs, sys: 31 µs, total: 117 µs\n",
      "Wall time: 121 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "item_features_names = ['f_' + str(col) for col in [47, 68]]\n",
    "cat_features = [['item_id', 'purchase_id']] + item_features_names >> nvt.ops.Categorify(start_index=1, dtype='int32')\n",
    "\n",
    "features = ['session_id', 'timestamp', 'date'] + cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10a791",
   "metadata": {},
   "source": [
    "### GroupBy the data by sessions.\n",
    "\n",
    "Currently, every row is a viewed item in the dataset. Our goal is to predict the item purchased after the last view in a session. Therefore, we groupby the dataset by `session_id` to have one row for each prediction.\n",
    "\n",
    "Each row will have a sequence of encoded items ids with which a user interacted. The last item of a session has special importance as it is closer to the user's intention. We will keep the viewed item as a separate feature.\n",
    "\n",
    "The NVTabular `GroupBy` op enables the transformation. \n",
    "\n",
    "First, we define how the different columns should be aggregates:\n",
    "- Keep the first occurrence of `date`\n",
    "- Keep the last item and concatenate all items to a list (results are 2 features)\n",
    "- Keep the first occurrence of `purchase_id` (purchase_id should be the same for all rows of one session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de40cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_aggregate = {}\n",
    "to_aggregate['date'] = [\"first\"]\n",
    "to_aggregate['item_id'] = [\"last\", \"list\"]\n",
    "to_aggregate['purchase_id'] = [\"first\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453abf34",
   "metadata": {},
   "source": [
    "In addition, we concatenate each item features to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d6450e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in item_features_names: \n",
    "    to_aggregate[name] = ['list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6963a0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': ['first'],\n",
       " 'item_id': ['last', 'list'],\n",
       " 'purchase_id': ['first'],\n",
       " 'f_47': ['list'],\n",
       " 'f_68': ['list']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caead45e",
   "metadata": {},
   "source": [
    "We want to sort the dataframe by `date` and groupby the columns by `session_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad84c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_features = features >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    sort_cols=[\"date\"],\n",
    "    aggs= to_aggregate,\n",
    "    name_sep=\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375e1d5",
   "metadata": {},
   "source": [
    "Merlin Models can infer the neural network architecture from the dataset schema. We will Tag the columns accordingly based on the type of each column. If you want to learn more, we recommend our [Dataset Schema Example](https://github.com/NVIDIA-Merlin/models/blob/main/examples/02-Merlin-Models-and-NVTabular-integration.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8732e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_last = (\n",
    "    groupby_features['item_id_last'] >> \n",
    "    AddMetadata(tags=[Tags.ITEM, Tags.ITEM_ID])\n",
    ")\n",
    "item_list = (\n",
    "    groupby_features['item_id_list'] >> \n",
    "    AddMetadata(\n",
    "        tags=[Tags.ITEM, Tags.ITEM_ID, Tags.LIST, Tags.SEQUENCE]\n",
    "    )\n",
    ")\n",
    "feature_list = (\n",
    "    groupby_features[[name+'_list' for name in item_features_names]] >> \n",
    "    AddMetadata(\n",
    "        tags=[Tags.SEQUENCE, Tags.ITEM, Tags.LIST]\n",
    "    )\n",
    ")\n",
    "target_feature = (\n",
    "    groupby_features['purchase_id_first'] >> \n",
    "    AddMetadata(tags=[Tags.TARGET])\n",
    ")\n",
    "other_features = groupby_features['session_id', 'date_first']\n",
    "\n",
    "groupby_features = item_last + item_list + feature_list + other_features +  groupby_features['purchase_id_first']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a9832",
   "metadata": {},
   "source": [
    "### Truncate for a Maximum Sequence Length\n",
    "\n",
    "We want to truncate and pad the sequential features. We define the columns, which are sequential features and the non-sequential ones. We truncate the sequence by keeping the last 3 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e65dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = [name+'_list' for name in item_features_names] + ['item_id_list']\n",
    "nonlist_features = ['session_id', 'date_first', 'item_id_last', 'purchase_id_first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0d338d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSIONS_MAX_LENGTH = 3\n",
    "truncated_features = groupby_features[list_features] >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH) >> nvt.ops.Rename(postfix = '_seq')\n",
    "\n",
    "final_features = groupby_features[nonlist_features] + truncated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2caa1",
   "metadata": {},
   "source": [
    "We initialize our NVTabular workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74b030ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow(final_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6954007",
   "metadata": {},
   "source": [
    "We call fit and transform similar to the scikit learn API.\n",
    "\n",
    "Categorify will map item_ids (and purchase_ids), which does not occur in the train dataset, to a special category `0` in the validation dataset. This can bias the validation metrics. In our example, almost all item_ids in validation are available in train and we neglect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f5c222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# fit data\n",
    "workflow.fit(train)\n",
    "\n",
    "# transform and save data\n",
    "workflow.transform(train).to_parquet(os.path.join(DATA_FOLDER, \"train/\"), output_files=2)\n",
    "workflow.transform(valid).to_parquet(os.path.join(DATA_FOLDER, \"valid/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d222c0e",
   "metadata": {},
   "source": [
    "### Sort the Training Dataset by Time\n",
    "\n",
    "The train dataset contains the data from Jan 2020 to April 2021 and the validation dataset is May 2021. As the data is split by time, we noticed that we achieve higher validation scores, when we sort the training data by time and do not apply shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ba81642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_lib().read_parquet(\n",
    "    glob.glob(\n",
    "        os.path.join(DATA_FOLDER, \"train/*.parquet\")\n",
    "    )\n",
    ")\n",
    "df = df.sort_values('date_first').reset_index(drop=True)\n",
    "df.to_parquet(os.path.join(DATA_FOLDER, \"train_sorted.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c04dc",
   "metadata": {},
   "source": [
    "Let's review the transformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03de5beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>date_first</th>\n",
       "      <th>item_id_last</th>\n",
       "      <th>purchase_id_first</th>\n",
       "      <th>f_47_list_seq</th>\n",
       "      <th>f_68_list_seq</th>\n",
       "      <th>item_id_list_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3747794</td>\n",
       "      <td>2020-01-01 00:00:01.359</td>\n",
       "      <td>7920</td>\n",
       "      <td>13757</td>\n",
       "      <td>[3, 6, 14]</td>\n",
       "      <td>[2, 10, 8]</td>\n",
       "      <td>[538, 4177, 7920]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3458777</td>\n",
       "      <td>2020-01-01 00:00:21.440</td>\n",
       "      <td>11594</td>\n",
       "      <td>17900</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[10, 45, 7]</td>\n",
       "      <td>[14809, 7840, 11594]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4350716</td>\n",
       "      <td>2020-01-01 00:00:48.505</td>\n",
       "      <td>5192</td>\n",
       "      <td>14217</td>\n",
       "      <td>[14]</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[5192]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2579761</td>\n",
       "      <td>2020-01-01 00:06:37.801</td>\n",
       "      <td>9675</td>\n",
       "      <td>12251</td>\n",
       "      <td>[9, 8, 9]</td>\n",
       "      <td>[15, 4, 4]</td>\n",
       "      <td>[19431, 16369, 9675]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2048031</td>\n",
       "      <td>2020-01-01 00:08:19.297</td>\n",
       "      <td>14877</td>\n",
       "      <td>12751</td>\n",
       "      <td>[2, 6, 2]</td>\n",
       "      <td>[6, 2, 10]</td>\n",
       "      <td>[779, 15326, 14877]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id              date_first  item_id_last  purchase_id_first  \\\n",
       "0     3747794 2020-01-01 00:00:01.359          7920              13757   \n",
       "1     3458777 2020-01-01 00:00:21.440         11594              17900   \n",
       "2     4350716 2020-01-01 00:00:48.505          5192              14217   \n",
       "3     2579761 2020-01-01 00:06:37.801          9675              12251   \n",
       "4     2048031 2020-01-01 00:08:19.297         14877              12751   \n",
       "\n",
       "  f_47_list_seq f_68_list_seq      item_id_list_seq  \n",
       "0    [3, 6, 14]    [2, 10, 8]     [538, 4177, 7920]  \n",
       "1     [2, 2, 2]   [10, 45, 7]  [14809, 7840, 11594]  \n",
       "2          [14]           [9]                [5192]  \n",
       "3     [9, 8, 9]    [15, 4, 4]  [19431, 16369, 9675]  \n",
       "4     [2, 6, 2]    [6, 2, 10]   [779, 15326, 14877]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04948a31",
   "metadata": {},
   "source": [
    "## Training an MLP with sequential input with Merlin Models\n",
    "\n",
    "We train a Sequential-Multi-Layer Perceptron model, which averages the sequential input features (e.g. `item_id_list_seq`) and concatenate the resulting embeddings with the categorical embeddings (e.g. `item_id_last`). We visualize the architecture in the figure below.\n",
    "\n",
    "<img src=\"../images/mlp_ecommerce.png\"  width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d6a7d",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "We initialize the dataloaders to train the neural network models. First, we define NVTabular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f673e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = Dataset(os.path.join(DATA_FOLDER, 'train_sorted.parquet'))\n",
    "valid = Dataset(os.path.join(DATA_FOLDER, 'valid/*.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546df60",
   "metadata": {},
   "source": [
    "As we loaded, sorted and saved the train dataset without using NVTabular, the parquet file doesn't contain a schema, anymore. We can copy the schema from valid to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f89a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.schema = valid.schema\n",
    "schema_model = train.schema.select_by_name(\n",
    "        ['item_id_list_seq', 'item_id_last','f_47_list_seq', 'f_68_list_seq']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92782bbb-5715-49b6-9750-fda93fb36a5d",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "We use the following hyperparameters, we found during experimentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45d40de0-138c-4410-83df-cda5a34867c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = int(os.environ.get(\n",
    "    \"EPOCHS\", \n",
    "    '3'\n",
    "))\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.01\n",
    "DROPOUT = 0.2 \n",
    "LABEL_SMOOTHING = 0.2\n",
    "TEMPERATURE_SCALING = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d529122a-f535-4eae-8e32-d02e0da4fec1",
   "metadata": {},
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9044ee0-cabe-4ea0-8a16-dca3ca5b796e",
   "metadata": {},
   "source": [
    "The default dataloader does shuffle by default. We will initialize the Loader for the training dataset, and set the shuffle to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d55e758-28b9-42ec-8766-f724aa4f2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = mm.Loader(train, batch_size=BATCH_SIZE, transform=mm.ToTarget(train.schema, \"purchase_id_first\", one_hot=True),  shuffle = False)\n",
    "val_loader = mm.Loader(valid, batch_size=BATCH_SIZE, transform=mm.ToTarget(train.schema, \"purchase_id_first\", one_hot=True),  shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e197311",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build the Sequential MLP with Merlin Models\n",
    "\n",
    "Now we will create an InputBlock which takes sequential features, concatenate them and return the sequence of interaction embeddings. Note that we define the embedding dimensions, manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc98a39-ff6c-4eae-be53-d3f4d3f8a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_dims = {\n",
    "    'item_id_list_seq': 256, \n",
    "    'item_id_last': 256,\n",
    "    'f_47_list_seq': 16,\n",
    "    'f_68_list_seq': 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4dce190",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_block = mm.InputBlockV2(\n",
    "        schema_model,\n",
    "        categorical=mm.Embeddings(schema_model,\n",
    "                                 dim=manual_dims\n",
    "                                )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5241379",
   "metadata": {},
   "source": [
    "Before the loss is calculated, we want to transform the model output:\n",
    "\n",
    "1. We apply `weight-tying` and multiply the model output with the embedding weights from ITEM_ID. The embedding dimensions and the model output dimensions have to be the same (256 in our example).\n",
    "2. We transform the ground truth into OneHot representation\n",
    "3. We apply Temperature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85994ba-49d7-4630-b1c8-c4cc520b49e7",
   "metadata": {},
   "source": [
    "Now, we will build a model with a 2-layer MLPBlock, `input_block` as the input and `prediction_task` as the task. The output dimension of MLPBlock should match with the embedding dimension of the `item_id_list_seq` since we are using weight tying technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3abeac63-876a-428a-bfaa-72bcba966646",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = mm.MLPBlock(\n",
    "        [128, 256], \n",
    "        no_activation_last_layer=True, \n",
    "        dropout=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75d6a85d-1d7e-4158-ade6-ba1386f001bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'item_id_purchase_id'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id_name = train.schema.select_by_tag(Tags.ITEM_ID).first.properties['domain']['name']\n",
    "item_id_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5cbe51-a5cb-4964-9bb2-7ecf53ee36ec",
   "metadata": {},
   "source": [
    "Next, we define the prediction task. Our objective is multi-class classification - which is the item purchased at the end of the session. Therefore, this is a multi-class classification task, and the default_loss in the `CategoricalOutput` class is  \"categorical_crossentropy\". [CategoricalOutput](https://github.com/NVIDIA-Merlin/models/blob/main/merlin/models/tf/outputs/classification.py#L112) class has the functionality to do `weight-tying`, when we provide the `EmbeddingTable` related to the target feature in the `to_call` method. Note that in our example we feed the embedding table for the `item_id_purchase_id` domain name, since it reflects the fact that the `item_id_list_seq` and `item_id_last` input columns were jointly encoded and they share the same embedding table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b053041",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_task= mm.CategoricalOutput(\n",
    "        to_call=input_block[\"categorical\"][item_id_name],\n",
    "        logits_temperature=TEMPERATURE_SCALING,\n",
    "        target='purchase_id_first',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76f47160-f2df-4e6b-bff2-4b8b3e9bef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp = mm.Model(input_block, mlp_block, prediction_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54efec72",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be5866-f7f3-4111-b461-7b23eea7d7c2",
   "metadata": {},
   "source": [
    "\n",
    "We initialize the optimizer with `ExponentialDecay` learning rate scheduler and compile the model - similar to other TensorFlow Keras API. The competition was evaluated based on MRR@100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "203030aa-71d5-4167-b96c-96e99b6d9c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = LEARNING_RATE\n",
    "\n",
    "exp_decay_lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cd66c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=exp_decay_lr_scheduler,\n",
    ")\n",
    "\n",
    "model_mlp.compile(\n",
    "    optimizer=optimizer,\n",
    "    run_eagerly=False,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, \n",
    "        label_smoothing=LABEL_SMOOTHING\n",
    "    ),\n",
    "    metrics=mm.TopKMetricsAggregator.default_metrics(top_ks=[100])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e82bee",
   "metadata": {},
   "source": [
    "We call `.fit` to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f6deb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "900/900 [==============================] - 124s 128ms/step - loss: 7.9929 - recall_at_100: 0.3589 - mrr_at_100: 0.0747 - ndcg_at_100: 0.1279 - map_at_100: 0.0747 - precision_at_100: 0.0036 - regularization_loss: 0.0000e+00 - loss_batch: 7.9920 - val_loss: 7.9838 - val_recall_at_100: 0.4741 - val_mrr_at_100: 0.0948 - val_ndcg_at_100: 0.1668 - val_map_at_100: 0.0948 - val_precision_at_100: 0.0047 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 7.7966\n",
      "Epoch 2/3\n",
      "900/900 [==============================] - 114s 125ms/step - loss: 7.6865 - recall_at_100: 0.4475 - mrr_at_100: 0.0972 - ndcg_at_100: 0.1636 - map_at_100: 0.0972 - precision_at_100: 0.0045 - regularization_loss: 0.0000e+00 - loss_batch: 7.6851 - val_loss: 8.0098 - val_recall_at_100: 0.4653 - val_mrr_at_100: 0.0916 - val_ndcg_at_100: 0.1626 - val_map_at_100: 0.0916 - val_precision_at_100: 0.0047 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 7.8087\n",
      "Epoch 3/3\n",
      "900/900 [==============================] - 114s 125ms/step - loss: 7.6071 - recall_at_100: 0.4806 - mrr_at_100: 0.1042 - ndcg_at_100: 0.1758 - map_at_100: 0.1042 - precision_at_100: 0.0048 - regularization_loss: 0.0000e+00 - loss_batch: 7.6051 - val_loss: 8.0929 - val_recall_at_100: 0.4580 - val_mrr_at_100: 0.0905 - val_ndcg_at_100: 0.1601 - val_map_at_100: 0.0905 - val_precision_at_100: 0.0046 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 7.9016\n",
      "CPU times: user 4min 46s, sys: 2min 18s, total: 7min 4s\n",
      "Wall time: 5min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model_mlp.fit(\n",
    "    loader,\n",
    "    validation_data=val_loader,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160f023",
   "metadata": {},
   "source": [
    "We can evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f114178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 9s 108ms/step - loss: 8.0929 - recall_at_100: 0.4582 - mrr_at_100: 0.0904 - ndcg_at_100: 0.1601 - map_at_100: 0.0904 - precision_at_100: 0.0046 - regularization_loss: 0.0000e+00 - loss_batch: 8.0888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09053944051265717"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_mlp = model_mlp.evaluate(val_loader, batch_size=BATCH_SIZE, return_dict=True)\n",
    "metrics_mlp['mrr_at_100']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8dbe8",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this section, we train a Bi-LSTM model, which concatenates the embedding vectors for all sequential features (`item_id_list_seq`, `f_47_list_seq`, `f_68_list_seq`) per step (e.g. here 3). The concatenated vectors are processed by a BiLSTM. The hidden state of the BiLSTM is concatenated with the embedding vectors of the categorical features (`item_id_last`). Then we connect it with a Multi-Layer Perceptron Block. We visualize the architecture in the figure below.\n",
    "\n",
    "<img src=\"../images/bi-lstm_ecommerce.png\"  width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081cf607",
   "metadata": {},
   "source": [
    "### Build Bi-LSTM model\n",
    "\n",
    "Now we will create a Bi-LSTM model by using `tf.keras.layers.Bidirectional` api. We connect the sequence input block for sequential features with `BiLSTM` block. First, we  create two input blocks which takes sequential and categorical features, respectively, concatenate them and return the interaction embeddings.\n",
    "\n",
    "#### Hyperparameters\n",
    "We only update the `LEARNING_RATE`, as in our experimentations we found Bi-LSTM gave better results with lower learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d74c292-22c9-4870-b225-3d41e7526152",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70108db8-235a-496f-9787-f41d6193318d",
   "metadata": {},
   "source": [
    "We define the embedding dimensions, manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9275b012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_inputs = mm.InputBlockV2(\n",
    "        schema_model.select_by_name(\n",
    "            ['item_id_list_seq', 'f_47_list_seq', 'f_68_list_seq']\n",
    "        ),\n",
    "        categorical=mm.Embeddings(\n",
    "            schema_model.select_by_name(\n",
    "            ['item_id_list_seq', 'f_47_list_seq', 'f_68_list_seq']\n",
    "        ),\n",
    "            sequence_combiner=None,\n",
    "            dim=manual_dims\n",
    "                                \n",
    "        )\n",
    ")\n",
    "\n",
    "cat_inputs = mm.InputBlockV2(\n",
    "        schema_model.select_by_name(\n",
    "            ['item_id_last']\n",
    "        ),\n",
    "        categorical=mm.Embeddings(\n",
    "            schema_model.select_by_name(\n",
    "            ['item_id_last']\n",
    "        ),\n",
    "            dim=manual_dims\n",
    "                                \n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b452a14-bb07-4299-9c6a-bf026ec1cfbd",
   "metadata": {},
   "source": [
    "Connect the sequential input block to the BiLSTM model. We leverage `tf.keras.layers.Bidirectional` api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33820698-9303-44bc-8798-b74e753e6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_block =seq_inputs.connect(tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(64,\n",
    "        return_sequences=False, \n",
    "        dropout=0.05,\n",
    "        kernel_regularizer=regularizers.l2(1e-4),\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280055cb",
   "metadata": {},
   "source": [
    "Now, we combine dense block with input block of categorical features by concatenating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcd660b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "concats = mm.ParallelBlock(\n",
    "    {'dense_block': dense_block, \n",
    "     'cat_inputs': cat_inputs},\n",
    "    aggregation='concat'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380ca59",
   "metadata": {},
   "source": [
    "Next, we build a 2-layer MLPBlock which is used as a projection layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "deea812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = mm.MLPBlock(\n",
    "                [128,256],\n",
    "                activation='relu',\n",
    "                no_activation_last_layer=True,\n",
    "                dropout=DROPOUT,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef2de27",
   "metadata": {},
   "source": [
    "Now, we define the prediction task. As we saw above in our MLP implementation, our objective is multi-class classification.\n",
    "\n",
    "Before the loss is calculated, we want to transform the model output:\n",
    "\n",
    "- We apply  `weight-tying` (see in the beginning) and multiply the model output with the embedding weights from ITEM_ID. The embedding dimensions and the model output dimensions have to be the same (256 in our example).\n",
    "- We transform the ground truth into OneHot representation\n",
    "- We apply Temperature Scaling\n",
    "\n",
    "Again same as above, the pipeline is called prediction_call and we add it as the pre parameter to MultiClassClassificationTask. The pipeline is executed before the loss is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "921df537-5577-46f4-a656-c2c420483751",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_name = train.schema.select_by_tag(Tags.ITEM_ID).first.properties['domain']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fa8d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_task= mm.CategoricalOutput(\n",
    "    to_call=seq_inputs[\"categorical\"][item_id_name],\n",
    "    logits_temperature=TEMPERATURE_SCALING,\n",
    "    target='purchase_id_first',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05106839",
   "metadata": {},
   "source": [
    "Now, we will build a model by chaining the `concats`, the `mlp_block` and the `prediction_task` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17e9c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi_lstm = Model(concats, mlp_block, prediction_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e6adf",
   "metadata": {},
   "source": [
    "### Fit the Model\n",
    "We initialize the optimizer and compile the model - similar to other TensorFlow Keras API. The competition was evaluated based on MRR@100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7673905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "model_bi_lstm.compile(\n",
    "    optimizer=optimizer,\n",
    "    run_eagerly=False,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, \n",
    "        label_smoothing=LABEL_SMOOTHING\n",
    "    ),\n",
    "    metrics=mm.TopKMetricsAggregator.default_metrics(top_ks=[100])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d845114e",
   "metadata": {},
   "source": [
    "Now, we train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7dfa4858-d87c-43db-a6fa-c02a55e83941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 22:43:59.797594: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"Adam/gradients/concat_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_1/parallel_block_3/sequential_block_4/concat_features_1/RaggedConcat/Slice_1:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_1/parallel_block_3/sequential_block_4/concat_features_1/RaggedConcat/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"Adam/gradients/concat_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_1/parallel_block_3/sequential_block_4/concat_features_1/RaggedConcat/Slice_3:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_1/parallel_block_3/sequential_block_4/concat_features_1/RaggedConcat/Shape_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"Adam/gradients/concat_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_1/parallel_block_3/sequential_block_4/concat_features_1/RaggedConcat/Slice_5:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_1/parallel_block_3/sequential_block_4/concat_features_1/RaggedConcat/Shape_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900/900 [==============================] - 130s 135ms/step - loss: 7.9822 - recall_at_100: 0.3279 - mrr_at_100: 0.0716 - ndcg_at_100: 0.1194 - map_at_100: 0.0716 - precision_at_100: 0.0033 - regularization_loss: 0.0176 - loss_batch: 7.9809 - val_loss: 7.8566 - val_recall_at_100: 0.5026 - val_mrr_at_100: 0.1221 - val_ndcg_at_100: 0.1951 - val_map_at_100: 0.1221 - val_precision_at_100: 0.0050 - val_regularization_loss: 0.0224 - val_loss_batch: 7.7104\n",
      "Epoch 2/3\n",
      "900/900 [==============================] - 119s 130ms/step - loss: 7.4523 - recall_at_100: 0.4771 - mrr_at_100: 0.1224 - ndcg_at_100: 0.1904 - map_at_100: 0.1224 - precision_at_100: 0.0048 - regularization_loss: 0.0198 - loss_batch: 7.4508 - val_loss: 7.8624 - val_recall_at_100: 0.4941 - val_mrr_at_100: 0.1284 - val_ndcg_at_100: 0.1989 - val_map_at_100: 0.1284 - val_precision_at_100: 0.0049 - val_regularization_loss: 0.0205 - val_loss_batch: 7.6917\n",
      "Epoch 3/3\n",
      "900/900 [==============================] - 119s 131ms/step - loss: 7.2655 - recall_at_100: 0.5503 - mrr_at_100: 0.1494 - ndcg_at_100: 0.2268 - map_at_100: 0.1494 - precision_at_100: 0.0055 - regularization_loss: 0.0207 - loss_batch: 7.2632 - val_loss: 7.9479 - val_recall_at_100: 0.4859 - val_mrr_at_100: 0.1278 - val_ndcg_at_100: 0.1968 - val_map_at_100: 0.1278 - val_precision_at_100: 0.0049 - val_regularization_loss: 0.0205 - val_loss_batch: 7.7458\n"
     ]
    }
   ],
   "source": [
    "history = model_bi_lstm.fit(\n",
    "    loader,\n",
    "    validation_data=val_loader,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb684f",
   "metadata": {},
   "source": [
    "We can evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2676f668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 9s 110ms/step - loss: 7.9479 - recall_at_100: 0.4849 - mrr_at_100: 0.1270 - ndcg_at_100: 0.1960 - map_at_100: 0.1270 - precision_at_100: 0.0048 - regularization_loss: 0.0205 - loss_batch: 7.9436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12781542539596558"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_bi_lstm = model_bi_lstm.evaluate(val_loader, batch_size=BATCH_SIZE, return_dict=True)\n",
    "metrics_bi_lstm['mrr_at_100']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c9a164-df68-417f-a634-c2966abe52a0",
   "metadata": {},
   "source": [
    "Note that final score `mrr_at_100` we printed out above is the average over all steps, whereas the value we get from progress bar shows the score of the last n_steps (by default =1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525cdc3",
   "metadata": {},
   "source": [
    "## Training a Transformer-based Model\n",
    "\n",
    "In recent years, several deep learning-based algorithms have been proposed for recommendation systems while its adoption in industry deployments have been steeply growing. In particular, NLP inspired approaches have been successfully adapted for sequential and session-based recommendation problems, which are important for many domains like e-commerce, news and streaming media. Session-Based Recommender Systems (SBRS) have been proposed to model the sequence of interactions within the current user session, where a session is a short sequence of user interactions typically bounded by user inactivity. They have recently gained popularity due to their ability to capture short-term or contextual user preferences towards items.\n",
    "\n",
    "The field of NLP has evolved significantly within the last decade, particularly due to the increased usage of deep learning. As a result, state of the art NLP approaches have inspired RecSys practitioners and researchers to adapt those architectures, especially for sequential and session-based recommendation problems. Here, we use one of the state-of-the-art Transformer-based architecture, [XLNet](https://arxiv.org/abs/1906.08237) with Causal Language Modeling (CLM) training technique for multi-class classification task. For this, we leverage the popular HuggingFace’s Transformers NLP library and make it possible to experiment with cutting-edge implementation of such architectures for sequential and session-based recommendation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deb8987-9ee7-4301-be2a-cc2efe2f9497",
   "metadata": {},
   "source": [
    "Now, we replace the BiLSTM model with a transformer-based architecture. We train an `XLNet` model which concatenates the embedding vectors for all sequential features (`item_id_list_seq`, `f_47_list_seq`, `f_68_list_seq`) per step in the sequential input block, and uses self-attention mechanism. To learn more about the self-attention mechanism you can take a look at this [paper](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) and this [post](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "699cc21e-56d1-4ea5-a028-fd82becc5c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_name = train.schema.select_by_tag(Tags.ITEM_ID).first.properties['domain']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d1d918-fc3b-45f0-aee0-876b30897bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_inputs = mm.InputBlockV2(\n",
    "        schema_model.select_by_name(\n",
    "            ['item_id_list_seq', 'f_47_list_seq', 'f_68_list_seq']\n",
    "        ),\n",
    "        categorical=mm.Embeddings(\n",
    "            schema_model.select_by_name(\n",
    "            ['item_id_list_seq', 'f_47_list_seq', 'f_68_list_seq']\n",
    "        ),\n",
    "            sequence_combiner=None,\n",
    "            dim=manual_dims\n",
    "                                \n",
    "        )\n",
    ")\n",
    "\n",
    "cat_inputs = mm.InputBlockV2(\n",
    "        schema_model.select_by_name(\n",
    "            ['item_id_last']\n",
    "        ),\n",
    "        categorical=mm.Embeddings(\n",
    "            schema_model.select_by_name(\n",
    "            ['item_id_last']\n",
    "        ),\n",
    "            dim=manual_dims\n",
    "                                \n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34277fc8-48d6-4658-ae9b-c9d1beb8ab57",
   "metadata": {},
   "source": [
    "We can check the output from the sequential input block and its dimension. We obtain a 3-D sequence representation (batch_size, sequence_length, sum_of_emb_dim_of_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d523493-2fa0-4fbb-bc1c-52a522532f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = mm.sample_batch(train, batch_size=128, include_targets=False, to_ragged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4a65115-560f-4509-8a24-8b57a8071339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, None, 288])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_inputs(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75fc5a5c-dca6-47f4-94f2-28ac139df902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_inputs(batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ddc6d9-2044-4a83-9257-17ed35b8037e",
   "metadata": {},
   "source": [
    "The sequence_length dimension is printed out as None, because it is a variable length given a batch. That's why we get the sequence_length dim printed as `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fbf470-8124-4a30-ac19-924ad313a151",
   "metadata": {},
   "source": [
    "Let's create a sequential block where we connect sequential inputs block (i.e., a SequentialLayer represents a sequence of Keras layers) with MLPBlock and then XLNetBlock. MLPBlock is used as a projection block to match the output dimensions of the seq_inputs block with the transformer block. In otherwords, due to residual connection in the Transformer model, we add an MLPBlock in the model pipeline. The output dim of the input block should match with the hidden dimension of the XLNetBlock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01b8243b-22de-4c68-a277-76216bad0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = mm.MLPBlock(\n",
    "                [32,256],\n",
    "                activation='relu',\n",
    "                no_activation_last_layer=True,\n",
    "                dropout=DROPOUT,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdef6ac0-011f-42ac-bd70-0b4bcced6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_block =mm.SequentialBlock(\n",
    "    seq_inputs,\n",
    "    mlp_block,\n",
    "    mm.XLNetBlock(\n",
    "        d_model=256,\n",
    "        n_head=4,\n",
    "        n_layer=2,\n",
    "        post='sequence_mean',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae233975-c534-4fa8-b20f-858167ef7bee",
   "metadata": {},
   "source": [
    "The output of XLNetBlock is a 2D tensor `(batch_size, d_model)`, and it is then fed to final output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33f62939-b2af-498c-8885-44712923811a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 23:37:20.451144: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_block(batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d50de3-3643-4fd0-b7de-618796ed2edd",
   "metadata": {},
   "source": [
    "Let's concatenate the output of transformer block with the output of the categorical block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2ac9fc9-4e11-413b-ac0f-f18a60fca428",
   "metadata": {},
   "outputs": [],
   "source": [
    "concats = mm.ParallelBlock(\n",
    "    {'dense_block': dense_block, \n",
    "     'cat_inputs': cat_inputs},\n",
    "    aggregation='concat'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a632b9-3c0f-4cd9-9af6-6ffea547a86c",
   "metadata": {},
   "source": [
    "The concat layer shape would be the total output dimension (256 + 256) of two layers that are concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc42a786-17fb-4321-97c9-493f1ecdb958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concats(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48c793fe-8bed-452b-bc50-c2f9df67d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_task= mm.CategoricalOutput(\n",
    "    to_call=seq_inputs[\"categorical\"][item_id_name],\n",
    "    logits_temperature=TEMPERATURE_SCALING,\n",
    "    target='purchase_id_first',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afbc3f18-a792-47ab-9fa9-10306ad43203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer = mm.Model(concats, mlp_block, prediction_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc474897-1fe4-46cb-9c15-dd45f2ea7d22",
   "metadata": {},
   "source": [
    "We train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b2be155-a8ed-4a7b-bed4-6a9e7663d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "model_transformer.compile(\n",
    "    optimizer=optimizer,\n",
    "    run_eagerly=False,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, \n",
    "        label_smoothing=LABEL_SMOOTHING\n",
    "    ),\n",
    "    metrics=mm.TopKMetricsAggregator.default_metrics(top_ks=[100])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43f5a3a0-6218-4162-ad8b-319d0581602a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"private__dense_4\" (type _Dense).\n\nInput 0 of layer \"dense_4\" is incompatible with the layer: expected axis -1 of input shape to have value 288, but received input with shape (1024, 512)\n\nCall arguments received by layer \"private__dense_4\" (type _Dense):\n  • inputs=tf.Tensor(shape=(1024, 512), dtype=float32)\n  • kwargs={'training': 'False', 'features': {'f_47_list_seq': '<tf.RaggedTensor [[[3],\\n  [6],\\n  [14]], [[2],\\n          [2],\\n          [2]], [[14]], ..., [[4],\\n                              [4],\\n                              [4]], [[2],\\n                                     [2],\\n                                     [4]], [[14]]]>', 'f_68_list_seq': '<tf.RaggedTensor [[[2],\\n  [10],\\n  [8]] , [[10],\\n          [45],\\n          [7]] , [[9]], ..., [[28],\\n                              [23],\\n                              [2]] , [[28],\\n                                      [11],\\n                                      [2]] , [[10]]]>', 'item_id_list_seq': '<tf.RaggedTensor [[[538],\\n  [4177],\\n  [7920]], [[14809],\\n            [7840],\\n            [11594]], [[5192]], ..., [[11823],\\n                                      [19212],\\n                                      [6142]] , [[4396],\\n                                                 [11014],\\n                                                 [10622]], [[3855]]]>', 'item_id_last': 'tf.Tensor(shape=(1024, 1), dtype=int32)'}, 'testing': 'False'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/models/base.py:910\u001b[0m, in \u001b[0;36mBaseModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, train_metrics_steps, pre, **kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_compile_cache()\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pre \u001b[38;5;241m=\u001b[39m pre\n\u001b[0;32m--> 910\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre:\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pre\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/models/base.py:1190\u001b[0m, in \u001b[0;36mModel.call\u001b[0;34m(self, inputs, targets, training, testing, output_context)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     outputs, context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_child(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre, outputs, context)\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m-> 1190\u001b[0m     outputs, context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost:\n\u001b[1;32m   1193\u001b[0m     outputs, context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_child(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost, outputs, context)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/models/base.py:1219\u001b[0m, in \u001b[0;36mModel._call_child\u001b[0;34m(self, child, inputs, context)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(sub, ModelBlock) \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m child\u001b[38;5;241m.\u001b[39msubmodules):\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1219\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, Prediction):\n\u001b[1;32m   1221\u001b[0m     targets \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;28;01mif\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m context\u001b[38;5;241m.\u001b[39mtargets\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/utils/tf_utils.py:437\u001b[0m, in \u001b[0;36mcall_layer\u001b[0;34m(layer, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m         call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m    435\u001b[0m         filtered_kwargs \u001b[38;5;241m=\u001b[39m filter_kwargs(filtered_kwargs, call_fn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_k)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/config/schema.py:58\u001b[0m, in \u001b[0;36mSchemaMixin.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_schema()\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/core/combinators.py:269\u001b[0m, in \u001b[0;36mSequentialBlock.call\u001b[0;34m(self, inputs, training, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_sequentially\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/core/combinators.py:836\u001b[0m, in \u001b[0;36mcall_sequentially\u001b[0;34m(layers, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m outputs \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m--> 836\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/utils/tf_utils.py:437\u001b[0m, in \u001b[0;36mcall_layer\u001b[0;34m(layer, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m         call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m    435\u001b[0m         filtered_kwargs \u001b[38;5;241m=\u001b[39m filter_kwargs(filtered_kwargs, call_fn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_k)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/blocks/mlp.py:244\u001b[0m, in \u001b[0;36m_Dense.call\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tabular_aggregation_registry\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_aggregation)(inputs)\n\u001b[1;32m    243\u001b[0m filtered_kwargs \u001b[38;5;241m=\u001b[39m filter_kwargs(kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense)\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"private__dense_4\" (type _Dense).\n\nInput 0 of layer \"dense_4\" is incompatible with the layer: expected axis -1 of input shape to have value 288, but received input with shape (1024, 512)\n\nCall arguments received by layer \"private__dense_4\" (type _Dense):\n  • inputs=tf.Tensor(shape=(1024, 512), dtype=float32)\n  • kwargs={'training': 'False', 'features': {'f_47_list_seq': '<tf.RaggedTensor [[[3],\\n  [6],\\n  [14]], [[2],\\n          [2],\\n          [2]], [[14]], ..., [[4],\\n                              [4],\\n                              [4]], [[2],\\n                                     [2],\\n                                     [4]], [[14]]]>', 'f_68_list_seq': '<tf.RaggedTensor [[[2],\\n  [10],\\n  [8]] , [[10],\\n          [45],\\n          [7]] , [[9]], ..., [[28],\\n                              [23],\\n                              [2]] , [[28],\\n                                      [11],\\n                                      [2]] , [[10]]]>', 'item_id_list_seq': '<tf.RaggedTensor [[[538],\\n  [4177],\\n  [7920]], [[14809],\\n            [7840],\\n            [11594]], [[5192]], ..., [[11823],\\n                                      [19212],\\n                                      [6142]] , [[4396],\\n                                                 [11014],\\n                                                 [10622]], [[3855]]]>', 'item_id_last': 'tf.Tensor(shape=(1024, 1), dtype=int32)'}, 'testing': 'False'}"
     ]
    }
   ],
   "source": [
    "model_transformer.fit(loader, \n",
    "                      validation_data=val_loader,\n",
    "                      epochs=EPOCHS\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc3788fa-3a47-48b8-a9e9-02787f29818b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.summary of Model(\n",
       "  (blocks): _TupleWrapper((ParallelBlock(\n",
       "    (_aggregation): ConcatFeatures(\n",
       "      (_feature_shapes): Dict(\n",
       "        (f_47_list_seq): TensorShape([1024, None])\n",
       "        (f_68_list_seq): TensorShape([1024, None])\n",
       "        (item_id_list_seq): TensorShape([1024, None])\n",
       "        (item_id_last): TensorShape([1024, 1])\n",
       "      )\n",
       "      (_feature_dtypes): Dict(\n",
       "        (f_47_list_seq): tf.int32\n",
       "        (f_68_list_seq): tf.int32\n",
       "        (item_id_list_seq): tf.int32\n",
       "        (item_id_last): tf.int32\n",
       "      )\n",
       "    )\n",
       "    (parallel_layers): Dict(\n",
       "      (dense_block): SequentialBlock(\n",
       "        (layers): List(\n",
       "          (0): ParallelBlock(\n",
       "            (_aggregation): ConcatFeatures(\n",
       "              (_feature_shapes): Dict(\n",
       "                (f_47_list_seq): TensorShape([1024, None])\n",
       "                (f_68_list_seq): TensorShape([1024, None])\n",
       "                (item_id_list_seq): TensorShape([1024, None])\n",
       "                (item_id_last): TensorShape([1024, 1])\n",
       "              )\n",
       "              (_feature_dtypes): Dict(\n",
       "                (f_47_list_seq): tf.int32\n",
       "                (f_68_list_seq): tf.int32\n",
       "                (item_id_list_seq): tf.int32\n",
       "                (item_id_last): tf.int32\n",
       "              )\n",
       "            )\n",
       "            (parallel_layers): Dict(\n",
       "              (categorical): ParallelBlock(\n",
       "                (parallel_layers): Dict(\n",
       "                  (item_id_purchase_id): EmbeddingTable(\n",
       "                    (features): Dict(\n",
       "                      (item_id_list_seq): ColumnSchema(name='item_id_list_seq', tags={<Tags.ITEM: 'item'>, <Tags.ITEM_ID: 'item_id'>, <Tags.ID: 'id'>, <Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.LIST: 'list'>}, properties={'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 1.0, 'cat_path': './/categories/unique.item_id_purchase_id.parquet', 'embedding_sizes': {'cardinality': 23272.0, 'dimension': 446.0}, 'domain': {'min': 0, 'max': 23271, 'name': 'item_id_purchase_id'}}, dtype=dtype('int32'), is_list=True, is_ragged=True)\n",
       "                    )\n",
       "                    (table): Embedding(\n",
       "                      (embeddings): <tf.Variable 'parallel_block/embeddings:0' shape=(23272, 256) dtype=float32, numpy=\n",
       "                      array([[ 0.04811797, -0.00899345, -0.04444077, ..., -0.00330721,\n",
       "                               0.00299748,  0.03726442],\n",
       "                             [-0.01568881,  0.04338299, -0.04912223, ..., -0.03949897,\n",
       "                               0.02617436, -0.0135996 ],\n",
       "                             [-0.00171857,  0.0334307 , -0.00416709, ..., -0.03254211,\n",
       "                              -0.00979654, -0.01041394],\n",
       "                             ...,\n",
       "                             [ 0.04733298, -0.01479564, -0.04049425, ..., -0.04113694,\n",
       "                              -0.03093792,  0.00712723],\n",
       "                             [-0.04123167,  0.00964994,  0.01854951, ..., -0.01597718,\n",
       "                              -0.04847977,  0.00324798],\n",
       "                             [-0.04678471,  0.02663937,  0.0143769 , ...,  0.01008096,\n",
       "                              -0.00546101,  0.03424305]], dtype=float32)>\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (_feature_shapes): Dict(\n",
       "                      (f_47_list_seq): TensorShape([1024, None])\n",
       "                      (f_68_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_last): TensorShape([1024, 1])\n",
       "                    )\n",
       "                    (_feature_dtypes): Dict(\n",
       "                      (f_47_list_seq): tf.int32\n",
       "                      (f_68_list_seq): tf.int32\n",
       "                      (item_id_list_seq): tf.int32\n",
       "                      (item_id_last): tf.int32\n",
       "                    )\n",
       "                  )\n",
       "                  (f_47): EmbeddingTable(\n",
       "                    (features): Dict(\n",
       "                      (f_47_list_seq): ColumnSchema(name='f_47_list_seq', tags={<Tags.LIST: 'list'>, <Tags.SEQUENCE: 'sequence'>, <Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>}, properties={'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 1.0, 'cat_path': './/categories/unique.f_47.parquet', 'embedding_sizes': {'cardinality': 19.0, 'dimension': 16.0}, 'domain': {'min': 0, 'max': 18, 'name': 'f_47'}}, dtype=dtype('int32'), is_list=True, is_ragged=True)\n",
       "                    )\n",
       "                    (table): Embedding(\n",
       "                      (embeddings): <tf.Variable 'parallel_block/embeddings:0' shape=(19, 16) dtype=float32, numpy=\n",
       "                      array([[-3.1522170e-02, -6.7860000e-03,  2.1104526e-02,  2.8435078e-02,\n",
       "                              -1.4613640e-02, -3.1449161e-02,  2.2166643e-02, -9.0212114e-03,\n",
       "                              -4.5578219e-02, -1.6068004e-02,  6.3238628e-03, -4.0992927e-02,\n",
       "                               1.6521219e-02, -4.0432502e-02, -4.5590069e-02,  3.7889827e-02],\n",
       "                             [ 4.4306707e-02, -3.0084670e-02, -3.1434134e-02,  2.0095136e-02,\n",
       "                               4.7484908e-02, -1.3830971e-02, -3.9973069e-02, -1.4361106e-02,\n",
       "                               4.8051108e-02,  1.2076903e-02, -2.7262568e-02, -3.1376496e-02,\n",
       "                               1.2294184e-02, -8.8098049e-03, -4.4810660e-03,  3.9692249e-02],\n",
       "                             [-4.7303140e-02,  4.9706921e-03,  3.2631610e-02, -3.0921901e-02,\n",
       "                              -4.0334128e-02,  4.9812522e-02, -4.2754807e-02, -4.8311103e-02,\n",
       "                              -1.6594540e-02,  3.0685429e-02, -1.2048364e-02,  4.4147406e-02,\n",
       "                              -4.2432319e-02, -1.9881165e-02,  4.3064963e-02,  4.6978060e-02],\n",
       "                             [-2.1736979e-02, -2.7215648e-02, -2.7879560e-02,  3.7945334e-02,\n",
       "                               4.3024693e-02,  2.8778911e-03, -2.3558652e-02, -1.9218398e-02,\n",
       "                               4.8696671e-02, -4.9523987e-02, -4.1343346e-03, -2.5634944e-02,\n",
       "                               2.3452044e-03,  6.1483271e-03, -4.7932755e-02,  4.3705497e-02],\n",
       "                             [-4.9249198e-02,  3.4215722e-02, -3.0377079e-02, -3.4671389e-02,\n",
       "                              -4.7594596e-02, -8.0955848e-03, -3.3843506e-02, -2.7408516e-02,\n",
       "                              -3.3186663e-02, -4.5529306e-02,  2.1542441e-02, -1.7325878e-02,\n",
       "                               2.5805149e-02, -1.7993078e-03,  3.4184828e-03,  4.8671689e-02],\n",
       "                             [ 4.7533657e-02,  2.0459805e-02, -2.1536613e-02,  2.0752978e-02,\n",
       "                               3.7989106e-02, -2.4027897e-02,  1.0137070e-02,  4.3101300e-02,\n",
       "                               7.8128092e-03, -6.8389662e-03,  4.5159649e-02,  1.9411091e-02,\n",
       "                               2.4053965e-02, -3.4823306e-03, -1.5439056e-02, -3.2290414e-02],\n",
       "                             [-4.3899454e-02, -5.0976388e-03,  4.6570335e-02,  4.1969504e-02,\n",
       "                              -1.9339537e-02, -2.0319689e-02, -6.9067627e-04, -2.0597950e-03,\n",
       "                               1.9459639e-02, -2.9522145e-02, -3.2676533e-02,  4.1860405e-02,\n",
       "                              -4.1291643e-02,  3.9467398e-02,  2.0632651e-02, -2.8141333e-02],\n",
       "                             [ 1.7838325e-02,  9.6583851e-03, -3.6108147e-02, -1.2836982e-02,\n",
       "                              -3.8887165e-02, -4.9492717e-02,  4.4571113e-02,  3.4518242e-03,\n",
       "                               3.5047520e-02,  2.8220925e-02,  1.9021187e-02, -1.1831723e-02,\n",
       "                              -1.1889696e-02,  3.9492734e-03,  1.4600601e-02, -4.0835928e-02],\n",
       "                             [-8.2389265e-04,  2.7948607e-02, -2.9312944e-02,  4.1756760e-02,\n",
       "                              -1.3414849e-02, -2.3066377e-02, -1.1816539e-02, -3.6410581e-02,\n",
       "                               3.0056689e-02, -2.2465194e-02,  5.1252842e-03,  1.9913707e-02,\n",
       "                              -2.1551896e-02, -4.9837112e-02,  2.6439402e-02, -3.9676011e-02],\n",
       "                             [ 8.2105994e-03, -7.1586743e-03,  3.3630203e-02,  9.8300204e-03,\n",
       "                               2.8491680e-02, -3.7432242e-02,  4.9318563e-02,  4.6413932e-02,\n",
       "                              -4.1654766e-02, -4.3747853e-02, -2.5497938e-02,  8.0718286e-03,\n",
       "                               2.3330953e-02, -4.4960964e-02, -4.9217690e-02,  3.9584413e-03],\n",
       "                             [-3.8205825e-02,  4.6005819e-02,  3.1227652e-02, -2.4105359e-02,\n",
       "                               2.3211364e-02,  4.0688638e-02,  4.3537468e-04, -1.8401254e-02,\n",
       "                              -2.9176487e-02, -3.5365105e-02,  3.8153481e-02, -3.5698093e-02,\n",
       "                               3.2772869e-04,  4.1641966e-03,  2.8027184e-03,  1.4547706e-03],\n",
       "                             [-4.2794585e-02,  4.1346323e-02,  2.9204939e-02,  9.2610940e-03,\n",
       "                               2.1749105e-02, -1.8829679e-02,  1.8094305e-02, -3.1776540e-03,\n",
       "                               3.8001571e-02,  4.3382678e-02, -1.2156416e-02,  3.1847063e-02,\n",
       "                              -2.0466102e-02, -8.8207014e-03, -3.6073040e-02,  1.8131938e-02],\n",
       "                             [ 3.7700389e-02,  1.8425848e-02, -3.1244636e-02, -1.5586447e-02,\n",
       "                               2.6951861e-02,  1.8069182e-02, -3.2067254e-02, -1.5722405e-02,\n",
       "                               2.1183256e-02, -2.7867019e-02,  1.5805695e-02,  2.6654709e-02,\n",
       "                              -9.8016262e-03,  3.3970762e-02,  2.4871062e-02,  1.9305531e-02],\n",
       "                             [-3.6458232e-02, -2.3615181e-02,  2.5357042e-02,  1.5141476e-02,\n",
       "                               3.1799674e-03, -3.0782163e-02, -5.2477494e-03, -1.2635529e-02,\n",
       "                               7.0557483e-03,  1.0308754e-02, -1.8429302e-02,  1.5687119e-02,\n",
       "                              -1.6915090e-03,  2.3839269e-02,  2.1902170e-02,  3.2891147e-03],\n",
       "                             [ 4.5693740e-03,  4.2645637e-02, -3.0108914e-03, -3.3338524e-02,\n",
       "                               5.2528605e-03, -2.3421014e-02,  5.8085546e-03,  2.5879193e-02,\n",
       "                              -1.9936418e-02,  2.9983167e-02, -6.5595880e-03,  4.9843941e-02,\n",
       "                               3.7497852e-02, -3.0419052e-02,  3.6049638e-02, -3.0876577e-02],\n",
       "                             [-2.6589597e-02, -4.4733394e-02,  4.7713604e-02, -3.0423403e-03,\n",
       "                               2.8473306e-02,  1.8052571e-03,  2.9200528e-02, -2.0314539e-02,\n",
       "                               2.3306776e-02,  3.0900512e-02,  4.5635555e-02, -4.3734469e-02,\n",
       "                               7.6391809e-03, -1.2049973e-02,  1.6967062e-02,  3.6725607e-02],\n",
       "                             [-1.6954444e-02,  3.6811221e-02, -5.8860295e-03, -2.7086234e-02,\n",
       "                               2.2105370e-02, -4.9285937e-02,  4.9230818e-02, -1.1765920e-02,\n",
       "                              -2.9650653e-02, -3.2660771e-02,  6.7867339e-05,  2.2356931e-02,\n",
       "                               2.6984215e-03, -2.6016688e-02, -3.2426335e-02, -3.0279661e-02],\n",
       "                             [-3.1254433e-02,  4.6797879e-03,  2.8802529e-03, -2.3492206e-02,\n",
       "                              -6.8021044e-03,  1.0545529e-02, -1.4114104e-02,  9.2402585e-03,\n",
       "                              -3.5829507e-02, -7.0851929e-03, -3.4593642e-02,  4.5845959e-02,\n",
       "                              -1.2140498e-03,  2.8254125e-02, -4.2047419e-02,  3.6925826e-02],\n",
       "                             [ 1.3638232e-02, -3.1137658e-02, -3.5996616e-02,  4.2587195e-02,\n",
       "                               1.1077322e-02,  3.2289732e-02, -3.9327513e-02,  3.1024169e-02,\n",
       "                               4.9885225e-02, -3.3177007e-02, -2.6050890e-02, -5.0515905e-03,\n",
       "                               4.5234274e-02,  4.4075038e-02, -1.7013773e-03, -2.7524328e-02]],\n",
       "                            dtype=float32)>\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (_feature_shapes): Dict(\n",
       "                      (f_47_list_seq): TensorShape([1024, None])\n",
       "                      (f_68_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_last): TensorShape([1024, 1])\n",
       "                    )\n",
       "                    (_feature_dtypes): Dict(\n",
       "                      (f_47_list_seq): tf.int32\n",
       "                      (f_68_list_seq): tf.int32\n",
       "                      (item_id_list_seq): tf.int32\n",
       "                      (item_id_last): tf.int32\n",
       "                    )\n",
       "                  )\n",
       "                  (f_68): EmbeddingTable(\n",
       "                    (features): Dict(\n",
       "                      (f_68_list_seq): ColumnSchema(name='f_68_list_seq', tags={<Tags.LIST: 'list'>, <Tags.SEQUENCE: 'sequence'>, <Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>}, properties={'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 1.0, 'cat_path': './/categories/unique.f_68.parquet', 'embedding_sizes': {'cardinality': 51.0, 'dimension': 16.0}, 'domain': {'min': 0, 'max': 50, 'name': 'f_68'}}, dtype=dtype('int32'), is_list=True, is_ragged=True)\n",
       "                    )\n",
       "                    (table): Embedding(\n",
       "                      (embeddings): <tf.Variable 'parallel_block/embeddings:0' shape=(51, 16) dtype=float32, numpy=\n",
       "                      array([[ 1.34843625e-02,  3.82635258e-02, -5.39183617e-04,\n",
       "                              -3.97730842e-02, -1.36745572e-02,  3.53767015e-02,\n",
       "                               2.82293595e-02, -1.61471367e-02,  1.03976242e-02,\n",
       "                               2.29437016e-02, -2.34415885e-02,  3.71317379e-02,\n",
       "                               1.83819644e-02,  4.27755155e-02, -2.18245741e-02,\n",
       "                               7.30168819e-03],\n",
       "                             [-4.66648452e-02,  5.30866534e-03,  6.87919930e-03,\n",
       "                              -3.99975888e-02, -2.70095821e-02,  4.23934944e-02,\n",
       "                              -3.63313183e-02,  3.66002358e-02,  1.40010156e-02,\n",
       "                               3.12228315e-02, -2.57383827e-02, -4.08231840e-02,\n",
       "                              -1.63332000e-02,  4.49685715e-02,  1.19707957e-02,\n",
       "                              -1.14061832e-02],\n",
       "                             [-3.32984924e-02,  3.84786390e-02,  3.46089862e-02,\n",
       "                              -3.67510803e-02,  7.88167864e-03, -4.81099263e-02,\n",
       "                               9.46056098e-04,  1.05292909e-02,  1.07492916e-02,\n",
       "                               4.62672226e-02, -2.69675385e-02,  2.27743872e-02,\n",
       "                               2.59685554e-02,  4.88203801e-02, -3.14919837e-02,\n",
       "                               4.63850982e-02],\n",
       "                             [ 4.57676500e-03, -4.32411321e-02,  3.92524488e-02,\n",
       "                               1.67583264e-02, -3.82953770e-02,  3.85453589e-02,\n",
       "                               4.04609106e-02, -2.06045639e-02,  4.23055179e-02,\n",
       "                              -2.30185688e-05, -1.04808807e-03,  3.96629237e-02,\n",
       "                              -2.16040760e-03, -4.12321910e-02, -4.65494990e-02,\n",
       "                               2.30890624e-02],\n",
       "                             [-1.13436691e-02,  6.21684641e-03,  7.60059431e-03,\n",
       "                               1.78206824e-02,  3.09948958e-02,  1.89560764e-02,\n",
       "                               3.50039341e-02,  4.32677530e-02,  4.34473641e-02,\n",
       "                               5.43079525e-03,  3.82849239e-02,  4.94482629e-02,\n",
       "                               8.60946253e-03,  4.89948131e-02,  2.54011154e-03,\n",
       "                               4.10043113e-02],\n",
       "                             [-2.53242254e-03, -1.57241710e-02, -7.52792507e-03,\n",
       "                               1.24283209e-02, -1.39983408e-02,  2.09658258e-02,\n",
       "                              -2.83430107e-02,  3.45745571e-02, -4.74308245e-02,\n",
       "                               4.42264564e-02,  3.66547816e-02,  3.98915894e-02,\n",
       "                               3.86193059e-02,  4.03820165e-02, -2.09933762e-02,\n",
       "                               3.17172669e-02],\n",
       "                             [-3.90659943e-02,  3.87154557e-02, -1.09645501e-02,\n",
       "                               1.61235593e-02,  3.05331834e-02,  2.81528942e-02,\n",
       "                               2.01070569e-02, -1.69300437e-02, -4.34810519e-02,\n",
       "                               3.55172791e-02,  3.46856602e-02, -8.49530846e-03,\n",
       "                               4.26533073e-03, -1.27292871e-02, -1.22952089e-02,\n",
       "                              -2.65826825e-02],\n",
       "                             [ 4.27637808e-02,  2.21295692e-02, -1.80444233e-02,\n",
       "                               3.82358767e-02, -3.83360274e-02,  4.61340211e-02,\n",
       "                              -3.38064805e-02, -3.94991413e-02,  4.79830764e-02,\n",
       "                              -4.71142679e-03, -2.48034596e-02, -4.30870913e-02,\n",
       "                               3.45464610e-02, -3.76703516e-02, -1.89378858e-02,\n",
       "                              -3.22767347e-03],\n",
       "                             [-4.36762832e-02,  2.72424929e-02,  4.87146266e-02,\n",
       "                               3.47782411e-02,  2.39034779e-02,  1.91303827e-02,\n",
       "                              -2.15576421e-02,  1.57798454e-03, -3.06833144e-02,\n",
       "                               1.45834573e-02, -2.11214665e-02, -3.37206200e-03,\n",
       "                               3.59748341e-02,  2.81052478e-02,  2.03177221e-02,\n",
       "                              -3.93351167e-03],\n",
       "                             [-2.73008477e-02,  2.55384184e-02,  3.40231992e-02,\n",
       "                              -2.31230147e-02,  4.43863012e-02,  3.40882652e-02,\n",
       "                              -3.85117531e-02,  5.63188642e-03, -6.46704435e-03,\n",
       "                               4.56256792e-03, -4.20082808e-02,  4.52457741e-03,\n",
       "                              -4.28493284e-02,  2.97471024e-02,  3.44858654e-02,\n",
       "                               4.02372219e-02],\n",
       "                             [-4.78003882e-02,  1.80590414e-02, -1.58600099e-02,\n",
       "                               3.00967954e-02, -3.28577757e-02,  3.44127305e-02,\n",
       "                              -2.76039243e-02,  3.90466563e-02,  1.85817145e-02,\n",
       "                              -4.15582657e-02, -3.02508362e-02, -1.98626518e-03,\n",
       "                               3.68889607e-02, -2.08342075e-02, -3.39423195e-02,\n",
       "                               2.45944895e-02],\n",
       "                             [ 1.32203139e-02,  3.24630253e-02,  3.72808315e-02,\n",
       "                               4.40534204e-03, -4.06958684e-02, -1.70152299e-02,\n",
       "                               9.96589661e-03, -3.73722427e-02,  3.02388705e-02,\n",
       "                              -1.18185394e-02,  2.94336788e-02,  3.22706811e-02,\n",
       "                              -4.51325253e-03, -4.30159234e-02,  3.51369046e-02,\n",
       "                              -1.03367344e-02],\n",
       "                             [ 1.31105892e-02,  1.76720135e-02, -2.53162868e-02,\n",
       "                               1.03701465e-02,  2.24966928e-03,  4.29930352e-02,\n",
       "                              -1.57155097e-04,  3.57366242e-02, -2.21670996e-02,\n",
       "                               3.03983688e-04,  4.54125144e-02, -1.53251998e-02,\n",
       "                               3.33106779e-02, -4.34520245e-02, -4.82973456e-02,\n",
       "                              -6.14262745e-03],\n",
       "                             [ 4.87432368e-02,  3.74684222e-02, -1.75097100e-02,\n",
       "                               4.74694408e-02,  1.80125274e-02,  1.21469870e-02,\n",
       "                              -3.78260128e-02,  1.04582533e-02,  3.36970948e-02,\n",
       "                               4.50794771e-03, -3.09085138e-02, -4.08728048e-03,\n",
       "                               4.73231114e-02,  8.34993273e-03,  3.37512754e-02,\n",
       "                               6.52836636e-03],\n",
       "                             [ 6.56425953e-03, -1.73602700e-02,  3.45602967e-02,\n",
       "                              -2.50354763e-02,  4.69104089e-02,  3.63861211e-02,\n",
       "                               4.04857509e-02, -2.28690859e-02,  1.39317028e-02,\n",
       "                              -1.04078427e-02,  3.01499031e-02,  4.44341637e-02,\n",
       "                              -3.60735171e-02, -1.85988918e-02,  4.25010063e-02,\n",
       "                              -3.81607525e-02],\n",
       "                             [-1.75648816e-02,  3.27126272e-02, -2.28439812e-02,\n",
       "                              -1.66889913e-02, -3.71095650e-02, -4.00838144e-02,\n",
       "                               3.38854901e-02, -4.77090590e-02, -4.16335464e-02,\n",
       "                              -2.18661074e-02,  2.32479610e-02, -3.81774195e-02,\n",
       "                              -1.03592165e-02, -9.86734778e-03, -3.94434705e-02,\n",
       "                               3.39321047e-03],\n",
       "                             [ 1.17359869e-02,  2.76564248e-02, -1.63940415e-02,\n",
       "                              -3.42401490e-02,  1.92870535e-02,  1.45240910e-02,\n",
       "                               9.17142630e-03, -3.25759873e-02, -3.37520987e-02,\n",
       "                               2.44171955e-02,  1.09054334e-02, -2.44889390e-02,\n",
       "                               4.33943756e-02,  4.39464785e-02, -4.43175808e-02,\n",
       "                               1.62668489e-02],\n",
       "                             [ 3.25835086e-02, -4.88726273e-02,  1.93856098e-02,\n",
       "                               1.78762786e-02, -1.65102705e-02,  2.43836679e-02,\n",
       "                              -1.99449658e-02, -4.22854312e-02, -8.48765299e-03,\n",
       "                               3.33035327e-02,  4.11841609e-02,  1.12534054e-02,\n",
       "                              -1.86888091e-02,  1.34524144e-02, -2.77636182e-02,\n",
       "                               3.17719318e-02],\n",
       "                             [-3.76252420e-02, -2.47296225e-02,  1.42575763e-02,\n",
       "                               4.22121920e-02,  3.90768982e-02,  4.16404940e-02,\n",
       "                               2.16184966e-02,  3.33586372e-02,  3.67782228e-02,\n",
       "                               1.98582560e-03, -2.97217611e-02,  1.89380683e-02,\n",
       "                               2.92961262e-02,  2.78389789e-02, -2.99611222e-02,\n",
       "                              -2.26606056e-03],\n",
       "                             [ 3.07974555e-02, -1.99101698e-02, -2.69055720e-02,\n",
       "                               3.53785194e-02,  4.32004370e-02, -7.19944388e-03,\n",
       "                               4.77014817e-02,  9.79840755e-03, -4.75416780e-02,\n",
       "                               2.39209197e-02, -4.37933914e-02,  4.67995889e-02,\n",
       "                              -3.53670940e-02, -3.53588685e-02, -3.43282968e-02,\n",
       "                              -1.05999410e-04],\n",
       "                             [-6.50211424e-03,  1.12127662e-02, -1.47604235e-02,\n",
       "                               1.44070387e-03,  2.26433016e-02,  4.26157750e-02,\n",
       "                              -4.22391780e-02,  4.57954146e-02,  3.83264311e-02,\n",
       "                               4.70567979e-02,  1.71417035e-02,  4.26681526e-02,\n",
       "                              -1.38120763e-02,  2.78788246e-02,  1.51424520e-02,\n",
       "                              -3.77945900e-02],\n",
       "                             [-3.79609093e-02,  3.61407734e-02,  4.31062467e-02,\n",
       "                               4.48610894e-02,  2.40392946e-02, -2.93204188e-02,\n",
       "                               2.58157961e-02,  3.88072170e-02, -3.12298536e-03,\n",
       "                               2.41224803e-02,  4.13534530e-02, -3.49547155e-02,\n",
       "                               3.20888199e-02, -4.28595431e-02, -2.61677261e-02,\n",
       "                              -1.00521445e-02],\n",
       "                             [-1.39857307e-02,  4.01411094e-02,  3.87235172e-02,\n",
       "                               2.09720992e-02,  9.84460115e-03, -3.70077118e-02,\n",
       "                               3.95039357e-02,  4.68881167e-02, -3.02981585e-04,\n",
       "                              -4.46885340e-02,  4.79592793e-02, -3.20232995e-02,\n",
       "                              -2.89606582e-02,  3.58283520e-03,  1.25246122e-03,\n",
       "                               1.50444545e-02],\n",
       "                             [ 3.06942500e-02,  1.78031661e-02, -3.52561474e-02,\n",
       "                               2.44337805e-02, -3.11551336e-02,  2.89220847e-02,\n",
       "                               2.94320472e-02, -1.65639147e-02, -8.42224807e-04,\n",
       "                              -2.09208261e-02, -2.27703936e-02, -2.69187223e-02,\n",
       "                               3.42835225e-02,  3.21128257e-02, -3.36927399e-02,\n",
       "                              -4.07630317e-02],\n",
       "                             [-6.14262745e-03,  4.55865152e-02,  2.36130469e-02,\n",
       "                               4.65313233e-02, -7.69281387e-03, -4.35402989e-02,\n",
       "                              -2.97092274e-03,  2.81658284e-02, -3.35934907e-02,\n",
       "                              -1.29385814e-02,  4.52966355e-02,  3.51662524e-02,\n",
       "                               3.09978016e-02,  4.59890254e-02, -2.82669794e-02,\n",
       "                               2.32386477e-02],\n",
       "                             [-3.48695889e-02, -3.60831395e-02,  4.41682450e-02,\n",
       "                              -4.61533666e-02,  2.24564783e-02, -2.34475266e-02,\n",
       "                              -3.91072743e-02,  3.17530893e-02, -4.09824774e-03,\n",
       "                               3.04491781e-02, -2.93600801e-02,  2.07296871e-02,\n",
       "                               2.56143846e-02,  3.44729461e-02, -2.38786936e-02,\n",
       "                              -3.68031263e-02],\n",
       "                             [-2.67045740e-02, -1.15316510e-02, -3.61009240e-02,\n",
       "                              -3.51166241e-02, -2.87923813e-02, -3.77403609e-02,\n",
       "                              -5.81551343e-04, -2.84126401e-02, -3.90807390e-02,\n",
       "                              -3.70235369e-03, -1.58350579e-02, -4.59921136e-02,\n",
       "                               4.56687473e-02,  1.09633058e-03, -3.29896063e-03,\n",
       "                              -1.96737405e-02],\n",
       "                             [ 3.82256992e-02, -4.62790504e-02, -3.72333303e-02,\n",
       "                              -3.83807346e-03, -4.52382080e-02,  2.91854031e-02,\n",
       "                              -1.24201179e-02,  4.77087535e-02,  3.15722488e-02,\n",
       "                              -4.77100499e-02,  7.61616230e-03, -4.71750870e-02,\n",
       "                               2.15524472e-02, -2.47835051e-02, -2.92172786e-02,\n",
       "                              -2.61870977e-02],\n",
       "                             [-2.01711059e-02,  1.30499266e-02, -2.65298020e-02,\n",
       "                              -4.95704301e-02,  1.20413788e-02, -2.46855747e-02,\n",
       "                              -2.19823960e-02, -4.89089154e-02,  6.41817972e-03,\n",
       "                              -3.72389071e-02,  4.40209620e-02, -2.68083941e-02,\n",
       "                              -8.95728916e-03, -4.26522866e-02,  1.58233568e-03,\n",
       "                               3.24520357e-02],\n",
       "                             [-2.43668314e-02, -2.80741937e-02,  2.96450593e-02,\n",
       "                              -1.59963742e-02,  1.33986212e-02, -3.57475653e-02,\n",
       "                               4.50608172e-02, -1.67110786e-02, -1.80001371e-02,\n",
       "                              -2.67272722e-02, -2.14831959e-02,  1.45810731e-02,\n",
       "                               7.81662390e-03,  2.82843001e-02,  2.39113010e-02,\n",
       "                              -7.86172226e-03],\n",
       "                             [ 7.46824592e-03,  4.06820811e-02, -2.71258000e-02,\n",
       "                              -7.74290413e-03,  3.29043902e-02, -2.79748924e-02,\n",
       "                               2.72664167e-02, -9.66269895e-03,  1.41647942e-02,\n",
       "                              -2.46168375e-02,  1.82497501e-03,  5.05484268e-03,\n",
       "                               3.69591601e-02,  3.39133181e-02, -3.16654928e-02,\n",
       "                               2.78338231e-02],\n",
       "                             [ 3.49823385e-03,  5.41601330e-03, -8.49940628e-03,\n",
       "                              -3.66141424e-02,  1.98088624e-02, -2.81957146e-02,\n",
       "                               2.59939097e-02,  2.99882405e-02,  3.84227075e-02,\n",
       "                               4.52225283e-03,  4.02219333e-02,  2.03981139e-02,\n",
       "                              -2.22650301e-02,  3.55178751e-02, -2.60327943e-02,\n",
       "                              -3.31314430e-02],\n",
       "                             [ 2.05045976e-02,  4.41200994e-02,  1.21164545e-02,\n",
       "                               3.39189284e-02, -1.39702484e-03,  3.03031243e-02,\n",
       "                              -9.89258289e-03,  1.88702010e-02,  2.30435617e-02,\n",
       "                               1.74547769e-02,  2.37593800e-03, -1.96459144e-03,\n",
       "                               4.37440984e-02, -1.02091655e-02, -4.93681096e-02,\n",
       "                               4.07752655e-02],\n",
       "                             [ 3.41232158e-02, -3.66923958e-03, -4.82519269e-02,\n",
       "                              -4.58405018e-02,  3.76373418e-02,  1.41344406e-02,\n",
       "                               3.96065004e-02,  4.11824100e-02,  2.57375576e-02,\n",
       "                               4.21223305e-02,  2.17926539e-02,  8.31720978e-03,\n",
       "                               4.76366319e-02,  3.40184905e-02, -3.71433869e-02,\n",
       "                              -2.19098330e-02],\n",
       "                             [ 4.36252616e-02,  4.33960296e-02, -3.82075422e-02,\n",
       "                               1.66882537e-02, -1.50437355e-02, -2.59594079e-02,\n",
       "                              -3.47702131e-02, -2.39191204e-03, -1.41195767e-02,\n",
       "                              -7.00629875e-03, -2.04042345e-03,  3.09768431e-02,\n",
       "                               2.59099342e-02,  3.28177474e-02,  9.83043760e-03,\n",
       "                              -7.81346112e-04],\n",
       "                             [-3.79485860e-02,  2.06872337e-02, -4.11319137e-02,\n",
       "                              -5.48766926e-03, -2.67573595e-02,  1.21836178e-02,\n",
       "                               2.86867358e-02,  1.20993629e-02, -2.00841315e-02,\n",
       "                              -1.20293871e-02,  4.96869124e-02,  8.52276012e-03,\n",
       "                              -4.27204370e-02, -4.96093892e-02, -4.04398553e-02,\n",
       "                              -2.19582077e-02],\n",
       "                             [-1.34769082e-02, -1.05868354e-02,  4.75177281e-02,\n",
       "                               1.83230303e-02, -7.11171702e-03,  3.30636017e-02,\n",
       "                              -3.23723704e-02,  3.95664088e-02,  4.09130342e-02,\n",
       "                               2.32596137e-02,  4.84883897e-02,  2.94926204e-02,\n",
       "                              -6.49813563e-03,  1.00693814e-02, -4.77066748e-02,\n",
       "                               4.46169041e-02],\n",
       "                             [ 6.10508025e-04,  2.46655606e-02, -5.35843521e-03,\n",
       "                               3.82242538e-02,  3.74087803e-02,  3.55466641e-02,\n",
       "                               7.28631020e-03, -4.57932837e-02,  3.51215638e-02,\n",
       "                               2.67135613e-02,  3.15953828e-02, -6.30903989e-04,\n",
       "                              -3.70150208e-02,  2.98987962e-02,  2.42337845e-02,\n",
       "                              -4.70187180e-02],\n",
       "                             [ 1.85321309e-02, -4.91906330e-03,  1.21308081e-02,\n",
       "                               3.05362232e-02,  5.57832792e-03, -8.93469900e-03,\n",
       "                              -3.87732610e-02,  4.15591262e-02,  3.86555679e-02,\n",
       "                              -4.37140353e-02,  1.73551701e-02, -3.85805853e-02,\n",
       "                               1.58158205e-02, -8.83785635e-03,  2.73872539e-03,\n",
       "                              -4.46992517e-02],\n",
       "                             [-7.63043016e-03, -4.13299799e-02,  4.72582504e-03,\n",
       "                               5.69605827e-03, -2.11243518e-02,  1.54638551e-02,\n",
       "                              -3.76971141e-02,  4.85827662e-02,  4.49309386e-02,\n",
       "                              -7.50967115e-03, -4.47829477e-02,  2.15841196e-02,\n",
       "                               3.22995521e-02, -3.07324529e-02,  4.72389720e-02,\n",
       "                              -5.12333959e-03],\n",
       "                             [-3.30203772e-02, -3.78973857e-02, -4.45995592e-02,\n",
       "                               3.90296690e-02,  1.28454827e-02, -3.20012718e-02,\n",
       "                              -3.31694484e-02,  5.91342524e-03, -8.45094770e-03,\n",
       "                               2.01726072e-02,  2.85832994e-02,  1.42230727e-02,\n",
       "                               3.95983122e-02,  3.22385170e-02, -4.51079272e-02,\n",
       "                              -1.26475915e-02],\n",
       "                             [ 1.86056830e-02, -1.58663988e-02, -2.18714476e-02,\n",
       "                               6.37442991e-03,  4.06199209e-02, -1.49963871e-02,\n",
       "                               2.10608281e-02,  3.84516232e-02, -3.92010696e-02,\n",
       "                              -2.64019724e-02,  4.60704230e-02, -4.92250919e-03,\n",
       "                              -2.54661962e-03, -5.71355969e-03, -4.10380960e-02,\n",
       "                              -2.66050342e-02],\n",
       "                             [ 1.64549835e-02,  2.01288201e-02, -1.29478574e-02,\n",
       "                              -3.95735502e-02,  3.55544128e-02, -4.87927459e-02,\n",
       "                              -8.06521252e-03,  4.53821570e-03,  2.62637064e-03,\n",
       "                              -4.62518707e-02,  1.72206499e-02, -2.72146221e-02,\n",
       "                              -4.20062654e-02, -3.53755951e-02, -2.97266245e-03,\n",
       "                               3.85798551e-02],\n",
       "                             [-4.28844355e-02, -2.23906767e-02, -2.76354309e-02,\n",
       "                              -5.93007728e-03, -1.91094764e-02,  2.83840932e-02,\n",
       "                               2.11479403e-02, -3.76235247e-02, -2.81176455e-02,\n",
       "                              -3.64957228e-02,  4.96454872e-02, -1.00357458e-03,\n",
       "                               4.48178910e-02,  9.54272598e-03, -3.53879854e-03,\n",
       "                              -4.36266661e-02],\n",
       "                             [ 3.77815478e-02, -3.82453315e-02, -3.64950411e-02,\n",
       "                              -1.72551386e-02, -4.73101847e-02,  1.95403360e-02,\n",
       "                               4.22099270e-02, -2.46018171e-02, -1.52505040e-02,\n",
       "                               4.67592478e-03, -3.36706787e-02, -3.69091853e-02,\n",
       "                               4.32022475e-02, -4.05625328e-02,  3.65560390e-02,\n",
       "                              -2.08758600e-02],\n",
       "                             [-4.31455150e-02,  3.56043614e-02, -4.05172482e-02,\n",
       "                              -5.19633293e-03,  3.27655114e-02,  1.52599253e-02,\n",
       "                               4.27660458e-02, -4.08694036e-02,  2.99397446e-02,\n",
       "                               4.44505475e-02,  3.88478972e-02,  1.47516839e-02,\n",
       "                               2.97432654e-02, -2.37526894e-02,  3.85285355e-02,\n",
       "                               9.76942852e-03],\n",
       "                             [-3.57367769e-02,  3.87284793e-02, -4.15229574e-02,\n",
       "                              -4.22921069e-02,  4.90978025e-02,  2.75131203e-02,\n",
       "                              -4.92048971e-02,  4.09960747e-03,  4.04051431e-02,\n",
       "                              -4.98577468e-02,  2.40735896e-02, -4.75533865e-02,\n",
       "                              -2.65752319e-02, -2.02692505e-02,  2.42641829e-02,\n",
       "                               2.54425518e-02],\n",
       "                             [-1.60892829e-02,  4.80470397e-02, -3.10418960e-02,\n",
       "                              -4.85063568e-02, -2.61012074e-02,  1.30824782e-02,\n",
       "                               1.39889009e-02,  3.31614874e-02, -1.29374973e-02,\n",
       "                              -4.38015535e-03,  3.77265848e-02,  1.67535804e-02,\n",
       "                              -5.56477159e-03,  1.58783048e-03,  1.97122730e-02,\n",
       "                               3.00792940e-02],\n",
       "                             [ 4.97431494e-02, -1.60847679e-02,  4.21752073e-02,\n",
       "                              -2.57684123e-02,  7.79167563e-03, -4.19334061e-02,\n",
       "                               6.15750626e-03, -4.72307578e-02, -3.20059881e-02,\n",
       "                               1.20948181e-02, -4.95893024e-02,  3.88602167e-03,\n",
       "                              -4.08336893e-02, -3.50224860e-02,  2.17093863e-02,\n",
       "                              -3.17517519e-02],\n",
       "                             [ 1.32688023e-02, -8.20638984e-03, -7.18538836e-03,\n",
       "                              -2.17568874e-02,  4.02320176e-04, -3.71019132e-02,\n",
       "                               1.93337537e-02,  3.25096138e-02, -4.34704311e-02,\n",
       "                              -1.39274225e-02, -1.87074766e-02, -1.80604681e-02,\n",
       "                               3.91183607e-02,  3.07383575e-02,  7.69286230e-03,\n",
       "                               3.19105722e-02],\n",
       "                             [-4.77574468e-02, -3.73177156e-02,  3.12137865e-02,\n",
       "                              -1.27069950e-02, -4.17580232e-02, -1.21236816e-02,\n",
       "                               2.03378536e-02,  4.38289382e-02, -4.60016727e-03,\n",
       "                               5.13159111e-03,  2.39057280e-02,  1.82409175e-02,\n",
       "                              -4.46599722e-02,  1.55853741e-02,  3.17043066e-03,\n",
       "                              -4.68515232e-03]], dtype=float32)>\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (_feature_shapes): Dict(\n",
       "                      (f_47_list_seq): TensorShape([1024, None])\n",
       "                      (f_68_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_last): TensorShape([1024, 1])\n",
       "                    )\n",
       "                    (_feature_dtypes): Dict(\n",
       "                      (f_47_list_seq): tf.int32\n",
       "                      (f_68_list_seq): tf.int32\n",
       "                      (item_id_list_seq): tf.int32\n",
       "                      (item_id_last): tf.int32\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (_feature_shapes): Dict(\n",
       "                  (f_47_list_seq): TensorShape([1024, None])\n",
       "                  (f_68_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_last): TensorShape([1024, 1])\n",
       "                )\n",
       "                (_feature_dtypes): Dict(\n",
       "                  (f_47_list_seq): tf.int32\n",
       "                  (f_68_list_seq): tf.int32\n",
       "                  (item_id_list_seq): tf.int32\n",
       "                  (item_id_last): tf.int32\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (_feature_shapes): Dict(\n",
       "              (f_47_list_seq): TensorShape([1024, None])\n",
       "              (f_68_list_seq): TensorShape([1024, None])\n",
       "              (item_id_list_seq): TensorShape([1024, None])\n",
       "              (item_id_last): TensorShape([1024, 1])\n",
       "            )\n",
       "            (_feature_dtypes): Dict(\n",
       "              (f_47_list_seq): tf.int32\n",
       "              (f_68_list_seq): tf.int32\n",
       "              (item_id_list_seq): tf.int32\n",
       "              (item_id_last): tf.int32\n",
       "            )\n",
       "          )\n",
       "          (1): MLPBlock(\n",
       "            (layers): List(\n",
       "              (0): _Dense(\n",
       "                (dense): Dense(\n",
       "                  32, activation=relu, use_bias=True\n",
       "                  (kernel): <tf.Variable 'sequential_block_4/sequential_block_3/private__dense_4/dense_4/kernel:0' shape=(288, 32) dtype=float32, numpy=\n",
       "                  array([[ 0.13668561, -0.00331074,  0.05509979, ..., -0.1048767 ,\n",
       "                          -0.06435522,  0.1269579 ],\n",
       "                         [-0.02135976,  0.11579561,  0.09275207, ..., -0.08785652,\n",
       "                           0.03408624, -0.13315791],\n",
       "                         [ 0.0110897 ,  0.12874144, -0.07588747, ...,  0.09341864,\n",
       "                          -0.12620835, -0.0447849 ],\n",
       "                         ...,\n",
       "                         [-0.03476546,  0.08247851, -0.08052737, ...,  0.11434636,\n",
       "                          -0.09653316,  0.07015914],\n",
       "                         [ 0.05088949, -0.09901311,  0.1276901 , ..., -0.12818041,\n",
       "                           0.05082168,  0.13061997],\n",
       "                         [ 0.02416109, -0.09868706,  0.06914937, ...,  0.0439043 ,\n",
       "                           0.00019096, -0.11566934]], dtype=float32)>\n",
       "                  (bias): <tf.Variable 'sequential_block_4/sequential_block_3/private__dense_4/dense_4/bias:0' shape=(32,) dtype=float32, numpy=\n",
       "                  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                        dtype=float32)>\n",
       "                  (_feature_shapes): Dict(\n",
       "                    (f_47_list_seq): TensorShape([1024, None])\n",
       "                    (f_68_list_seq): TensorShape([1024, None])\n",
       "                    (item_id_list_seq): TensorShape([1024, None])\n",
       "                    (item_id_last): TensorShape([1024, 1])\n",
       "                  )\n",
       "                  (_feature_dtypes): Dict(\n",
       "                    (f_47_list_seq): tf.int32\n",
       "                    (f_68_list_seq): tf.int32\n",
       "                    (item_id_list_seq): tf.int32\n",
       "                    (item_id_last): tf.int32\n",
       "                  )\n",
       "                )\n",
       "                (_feature_shapes): Dict(\n",
       "                  (f_47_list_seq): TensorShape([1024, None])\n",
       "                  (f_68_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_last): TensorShape([1024, 1])\n",
       "                )\n",
       "                (_feature_dtypes): Dict(\n",
       "                  (f_47_list_seq): tf.int32\n",
       "                  (f_68_list_seq): tf.int32\n",
       "                  (item_id_list_seq): tf.int32\n",
       "                  (item_id_last): tf.int32\n",
       "                )\n",
       "              )\n",
       "              (1): Dropout(\n",
       "                (_feature_shapes): Dict(\n",
       "                  (f_47_list_seq): TensorShape([1024, None])\n",
       "                  (f_68_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_last): TensorShape([1024, 1])\n",
       "                )\n",
       "                (_feature_dtypes): Dict(\n",
       "                  (f_47_list_seq): tf.int32\n",
       "                  (f_68_list_seq): tf.int32\n",
       "                  (item_id_list_seq): tf.int32\n",
       "                  (item_id_last): tf.int32\n",
       "                )\n",
       "              )\n",
       "              (2): _Dense(\n",
       "                (dense): Dense(\n",
       "                  256, activation=linear, use_bias=True\n",
       "                  (kernel): <tf.Variable 'sequential_block_4/sequential_block_3/private__dense_5/dense_5/kernel:0' shape=(32, 256) dtype=float32, numpy=\n",
       "                  array([[ 0.05131534, -0.05413289, -0.08483787, ...,  0.13316393,\n",
       "                           0.0393841 ,  0.12440467],\n",
       "                         [-0.03066889,  0.01768437,  0.02352177, ..., -0.11354895,\n",
       "                           0.12201235, -0.13605091],\n",
       "                         [-0.13135311,  0.05547984,  0.0597591 , ..., -0.0390357 ,\n",
       "                          -0.01086327,  0.03687899],\n",
       "                         ...,\n",
       "                         [-0.03804816, -0.12745136, -0.01011722, ..., -0.07244129,\n",
       "                          -0.02058902, -0.02280251],\n",
       "                         [-0.06398636,  0.09930643,  0.07152282, ..., -0.07264003,\n",
       "                           0.1368024 ,  0.10540514],\n",
       "                         [ 0.11821625,  0.09571522, -0.05523643, ...,  0.10063526,\n",
       "                          -0.13430037, -0.06427357]], dtype=float32)>\n",
       "                  (bias): <tf.Variable 'sequential_block_4/sequential_block_3/private__dense_5/dense_5/bias:0' shape=(256,) dtype=float32, numpy=\n",
       "                  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                         0.], dtype=float32)>\n",
       "                  (_feature_shapes): Dict(\n",
       "                    (f_47_list_seq): TensorShape([1024, None])\n",
       "                    (f_68_list_seq): TensorShape([1024, None])\n",
       "                    (item_id_list_seq): TensorShape([1024, None])\n",
       "                    (item_id_last): TensorShape([1024, 1])\n",
       "                  )\n",
       "                  (_feature_dtypes): Dict(\n",
       "                    (f_47_list_seq): tf.int32\n",
       "                    (f_68_list_seq): tf.int32\n",
       "                    (item_id_list_seq): tf.int32\n",
       "                    (item_id_last): tf.int32\n",
       "                  )\n",
       "                )\n",
       "                (_feature_shapes): Dict(\n",
       "                  (f_47_list_seq): TensorShape([1024, None])\n",
       "                  (f_68_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_last): TensorShape([1024, 1])\n",
       "                )\n",
       "                (_feature_dtypes): Dict(\n",
       "                  (f_47_list_seq): tf.int32\n",
       "                  (f_68_list_seq): tf.int32\n",
       "                  (item_id_list_seq): tf.int32\n",
       "                  (item_id_last): tf.int32\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (_feature_shapes): Dict(\n",
       "              (f_47_list_seq): TensorShape([1024, None])\n",
       "              (f_68_list_seq): TensorShape([1024, None])\n",
       "              (item_id_list_seq): TensorShape([1024, None])\n",
       "              (item_id_last): TensorShape([1024, 1])\n",
       "            )\n",
       "            (_feature_dtypes): Dict(\n",
       "              (f_47_list_seq): tf.int32\n",
       "              (f_68_list_seq): tf.int32\n",
       "              (item_id_list_seq): tf.int32\n",
       "              (item_id_last): tf.int32\n",
       "            )\n",
       "          )\n",
       "          (2): XLNetBlock(\n",
       "            (transformer): TFXLNetMainLayer(\n",
       "              (word_embedding): TFSharedEmbeddings(\n",
       "                (_feature_shapes): Dict(\n",
       "                  (f_47_list_seq): TensorShape([1024, None])\n",
       "                  (f_68_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_last): TensorShape([1024, 1])\n",
       "                )\n",
       "                (_feature_dtypes): Dict(\n",
       "                  (f_47_list_seq): tf.int32\n",
       "                  (f_68_list_seq): tf.int32\n",
       "                  (item_id_list_seq): tf.int32\n",
       "                  (item_id_last): tf.int32\n",
       "                )\n",
       "              )\n",
       "              (layer): List(\n",
       "                (0): TFXLNetLayer(\n",
       "                  (rel_attn): TFXLNetRelativeAttention(\n",
       "                    (layer_norm): LayerNormalization(\n",
       "                      (axis): List(\n",
       "                        (0): 2\n",
       "                      )\n",
       "                      (gamma): <tf.Variable 'transformer/layer_._0/rel_attn/layer_norm/gamma:0' shape=(256,) dtype=float32, numpy=\n",
       "                      array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1.], dtype=float32)>\n",
       "                      (beta): <tf.Variable 'transformer/layer_._0/rel_attn/layer_norm/beta:0' shape=(256,) dtype=float32, numpy=\n",
       "                      array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0.], dtype=float32)>\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (dropout): Dropout(\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (q): <tf.Variable 'transformer/layer_._0/rel_attn/q:0' shape=(256, 4, 64) dtype=float32, numpy=\n",
       "                    array([[[-8.5993465e-03, -7.9680569e-03, -1.9624291e-03, ...,\n",
       "                             -7.5437836e-03, -8.3370134e-03,  2.1238422e-03],\n",
       "                            [-5.9914848e-05,  7.8822542e-03,  1.2844286e-02, ...,\n",
       "                              5.5996468e-04,  7.1342173e-03,  1.0834885e-02],\n",
       "                            [ 1.4226026e-02, -1.5224957e-02, -4.9746088e-03, ...,\n",
       "                             -4.4839117e-03,  3.9232010e-03,  6.5151174e-03],\n",
       "                            [-3.0023803e-03, -7.1162037e-03,  5.3381491e-03, ...,\n",
       "                             -1.2756519e-03,  1.5803218e-02,  2.9123812e-03]],\n",
       "                    \n",
       "                           [[ 1.6327221e-02, -2.4835153e-03, -1.3368381e-03, ...,\n",
       "                             -1.6067492e-02, -3.3301141e-05,  1.6208725e-02],\n",
       "                            [-1.0944002e-03, -5.6921053e-03, -1.6245826e-03, ...,\n",
       "                             -1.6034747e-02,  1.8568536e-03,  2.0612173e-03],\n",
       "                            [-1.6984986e-02,  9.1565977e-04,  3.0053561e-04, ...,\n",
       "                             -3.8968602e-03,  2.5450534e-03, -1.8774377e-02],\n",
       "                            [ 1.0353476e-02,  1.1028045e-02, -9.2366599e-03, ...,\n",
       "                              1.3485487e-02, -6.8897597e-04,  1.0296000e-02]],\n",
       "                    \n",
       "                           [[ 6.2175114e-03,  9.2915213e-03, -1.1306177e-02, ...,\n",
       "                             -4.2365417e-03, -3.1543979e-03,  7.0324978e-03],\n",
       "                            [-2.1316772e-03, -8.2532456e-04,  1.6435146e-02, ...,\n",
       "                              1.5939696e-02, -4.0325671e-03, -5.1962868e-03],\n",
       "                            [ 8.9826966e-03, -8.2367100e-03,  7.9980586e-03, ...,\n",
       "                             -1.0616026e-02,  4.9848468e-03, -1.3174464e-02],\n",
       "                            [ 2.5818474e-03,  4.5310655e-03,  6.5711210e-03, ...,\n",
       "                             -6.8674963e-03,  1.7122226e-02,  1.2336150e-02]],\n",
       "                    \n",
       "                           ...,\n",
       "                    \n",
       "                           [[-5.3799814e-03,  1.9309138e-03, -1.1535334e-02, ...,\n",
       "                              8.1620738e-03,  1.4647537e-02, -1.8273722e-02],\n",
       "                            [ 4.7563836e-03, -1.1140522e-02, -1.1460185e-02, ...,\n",
       "                             -1.4846291e-02, -5.3937160e-03,  2.6699281e-03],\n",
       "                            [-9.8920558e-03,  1.3984780e-02,  3.8541900e-03, ...,\n",
       "                              6.3223462e-03,  5.8976060e-04,  8.2568023e-03],\n",
       "                            [ 1.4933714e-03, -8.6160153e-03, -1.4423371e-03, ...,\n",
       "                              1.6210029e-03,  1.0985809e-02, -5.8664638e-03]],\n",
       "                    \n",
       "                           [[ 1.9935656e-02, -9.3983896e-03, -2.2584380e-04, ...,\n",
       "                             -2.4839686e-03, -1.1207479e-02, -2.4234310e-03],\n",
       "                            [ 6.3799950e-03,  1.5893022e-02,  1.3708702e-02, ...,\n",
       "                             -3.8545527e-03, -7.9360148e-03,  1.8947706e-02],\n",
       "                            [ 6.5250783e-03, -4.7771428e-03, -5.8012120e-03, ...,\n",
       "                             -8.1256116e-03, -1.0173514e-02,  2.5234381e-03],\n",
       "                            [ 1.1740135e-02,  2.6516728e-03,  4.9202926e-03, ...,\n",
       "                              9.3871970e-03, -1.3050292e-02, -1.9553959e-02]],\n",
       "                    \n",
       "                           [[-1.5957717e-02,  1.0519799e-02,  6.2056957e-03, ...,\n",
       "                             -1.2200440e-02,  1.4400984e-02, -1.1605148e-02],\n",
       "                            [-8.2406476e-03,  9.7415932e-03, -1.2696829e-02, ...,\n",
       "                             -4.0908507e-03,  7.4595897e-03, -2.5696482e-03],\n",
       "                            [ 1.3082922e-02,  3.4075990e-04,  1.3218409e-02, ...,\n",
       "                             -2.5325554e-04,  9.1855498e-03,  1.5666317e-02],\n",
       "                            [ 8.0410158e-03, -9.1789430e-03,  7.5574457e-03, ...,\n",
       "                              1.7140737e-02,  2.9953229e-03, -8.5610832e-04]]], dtype=float32)>\n",
       "                    (k): <tf.Variable 'transformer/layer_._0/rel_attn/k:0' shape=(256, 4, 64) dtype=float32, numpy=\n",
       "                    array([[[-1.39371410e-03, -6.15052041e-03, -1.04734916e-02, ...,\n",
       "                              8.37781467e-03,  5.23563847e-03, -3.77532467e-03],\n",
       "                            [-3.97763029e-03, -3.51049844e-03, -1.23210810e-02, ...,\n",
       "                             -6.54758886e-04, -9.32285271e-04,  4.04560845e-03],\n",
       "                            [-1.34632587e-02, -2.90269614e-03,  8.01128568e-04, ...,\n",
       "                              6.62560249e-03,  1.42143352e-03, -6.47103181e-03],\n",
       "                            [ 6.84854155e-03, -1.06971823e-02, -1.80465877e-02, ...,\n",
       "                              4.54720389e-03, -3.16136493e-03,  1.24646639e-02]],\n",
       "                    \n",
       "                           [[-1.11047495e-02,  3.80536169e-03,  1.53214615e-02, ...,\n",
       "                              1.57903899e-02,  1.84555463e-02,  8.28169659e-03],\n",
       "                            [-1.79805397e-03,  1.41822558e-03,  1.43396424e-03, ...,\n",
       "                             -1.04206684e-03, -2.58574216e-03,  1.10137016e-02],\n",
       "                            [ 1.94882989e-04, -4.12425585e-03, -1.62948314e-02, ...,\n",
       "                              5.71983261e-03, -1.04917986e-02,  1.43630458e-02],\n",
       "                            [ 6.81841839e-03, -8.02450441e-03, -1.46883177e-02, ...,\n",
       "                             -1.57082956e-02,  6.86632062e-04,  5.16891992e-03]],\n",
       "                    \n",
       "                           [[ 6.11054013e-03, -3.45610315e-03, -1.07782055e-02, ...,\n",
       "                              1.86972402e-03, -8.69830325e-03, -3.63970431e-03],\n",
       "                            [ 5.47495391e-03, -7.11345673e-03, -3.06403590e-03, ...,\n",
       "                             -8.01168382e-03,  5.13383478e-04, -2.96621793e-03],\n",
       "                            [ 3.24535859e-03, -1.31942676e-02,  1.57087650e-02, ...,\n",
       "                              3.00341053e-03,  3.87904304e-03, -1.24524767e-02],\n",
       "                            [-1.81007129e-03, -5.77153871e-04, -1.71667989e-02, ...,\n",
       "                              1.27117644e-04,  8.09722301e-03,  1.69645995e-03]],\n",
       "                    \n",
       "                           ...,\n",
       "                    \n",
       "                           [[ 6.12953538e-03, -5.95584000e-03,  1.89717673e-02, ...,\n",
       "                              1.26224346e-02,  4.66334773e-03,  6.59534521e-03],\n",
       "                            [ 3.23283509e-03, -1.34118246e-02, -7.06847105e-03, ...,\n",
       "                              4.55294596e-03, -7.53125595e-03,  1.12492889e-02],\n",
       "                            [-1.01698674e-02,  2.55843799e-04, -1.77093723e-04, ...,\n",
       "                             -1.43499384e-02, -6.75335806e-03, -6.75298134e-03],\n",
       "                            [ 7.77769368e-03, -7.06243934e-03,  5.90674535e-05, ...,\n",
       "                              1.41258002e-03,  7.01917626e-04, -4.04600892e-03]],\n",
       "                    \n",
       "                           [[ 4.49778279e-03,  3.90843628e-03,  3.91670503e-03, ...,\n",
       "                             -6.14375062e-03, -3.88936652e-03, -1.36224367e-03],\n",
       "                            [ 1.30184162e-02, -7.59180030e-03, -5.00700716e-03, ...,\n",
       "                             -1.68702882e-02,  6.89143362e-03, -8.62208474e-03],\n",
       "                            [-9.48227849e-03,  6.30487339e-04,  3.14842630e-03, ...,\n",
       "                             -1.20650278e-02,  1.06235221e-03,  3.84999468e-04],\n",
       "                            [-1.08763631e-02, -9.48132575e-03,  4.01127245e-03, ...,\n",
       "                             -1.00594768e-02,  2.99844495e-03, -2.18640571e-03]],\n",
       "                    \n",
       "                           [[-1.33981276e-02,  4.12872853e-03, -5.18286880e-03, ...,\n",
       "                              5.95836528e-03, -1.34730525e-03,  1.09179895e-02],\n",
       "                            [-9.15608485e-04, -1.23055717e-02,  7.22088502e-04, ...,\n",
       "                             -1.62935397e-03, -5.70357312e-03, -1.89785974e-03],\n",
       "                            [-1.15701258e-02,  2.92525976e-03,  4.06045094e-03, ...,\n",
       "                              8.69459007e-03,  2.53190007e-03,  1.20555386e-02],\n",
       "                            [-1.12639107e-02,  8.02711025e-03,  4.85516945e-03, ...,\n",
       "                             -1.56294387e-02,  8.67075101e-03, -8.21343565e-04]]],\n",
       "                          dtype=float32)>\n",
       "                    (v): <tf.Variable 'transformer/layer_._0/rel_attn/v:0' shape=(256, 4, 64) dtype=float32, numpy=\n",
       "                    array([[[ 1.12743890e-02,  3.65180569e-03, -5.87891228e-03, ...,\n",
       "                              1.95728447e-02, -9.14072525e-03,  2.66301190e-03],\n",
       "                            [-5.92285651e-04,  1.36637827e-02,  2.69047800e-03, ...,\n",
       "                              1.87259559e-02,  8.00513849e-03, -2.76323291e-03],\n",
       "                            [ 3.50950468e-05, -3.86647414e-03,  6.77337917e-03, ...,\n",
       "                             -1.64734963e-02,  7.88507611e-03,  1.01680811e-02],\n",
       "                            [ 9.88434418e-04, -1.37545364e-02,  6.17011823e-03, ...,\n",
       "                             -3.02622910e-03, -1.25944084e-02,  3.56242619e-03]],\n",
       "                    \n",
       "                           [[-2.76690954e-03, -2.47568777e-03, -1.52663542e-02, ...,\n",
       "                              9.78256948e-03,  9.64937173e-03, -1.60688553e-02],\n",
       "                            [-3.71749001e-03, -5.53742470e-03, -1.90079177e-03, ...,\n",
       "                             -7.31567713e-03,  2.21050344e-03,  5.04353829e-03],\n",
       "                            [-1.63361803e-03,  1.68491714e-02, -4.33664012e-04, ...,\n",
       "                              1.00881672e-02,  1.19031472e-02, -4.47005127e-03],\n",
       "                            [-1.39727509e-02,  5.20076463e-03, -1.29566211e-02, ...,\n",
       "                              2.29520607e-03,  7.41988933e-03,  1.17399194e-03]],\n",
       "                    \n",
       "                           [[-2.18485453e-04, -7.50596449e-03, -1.37114655e-02, ...,\n",
       "                              1.86109927e-03, -9.76685528e-03, -4.40726010e-03],\n",
       "                            [-1.18376864e-02, -8.65160115e-03,  1.57195609e-03, ...,\n",
       "                             -1.07983779e-02, -8.06699414e-03, -1.36325425e-02],\n",
       "                            [ 3.21431609e-04,  7.41414167e-03,  1.61953121e-02, ...,\n",
       "                              3.29789729e-03,  3.02438566e-04,  4.65682661e-03],\n",
       "                            [ 6.52881921e-04,  1.05142444e-02, -2.36699684e-03, ...,\n",
       "                              1.45267099e-02, -8.11475050e-03, -1.18995532e-02]],\n",
       "                    \n",
       "                           ...,\n",
       "                    \n",
       "                           [[ 7.28693185e-03,  2.24118610e-03,  1.54912856e-03, ...,\n",
       "                             -7.54931895e-03,  1.13649210e-02,  1.31437201e-02],\n",
       "                            [-1.06691401e-02,  9.96140204e-03,  1.77899841e-02, ...,\n",
       "                              6.68462086e-03,  2.07549124e-03,  9.62058362e-03],\n",
       "                            [-1.17533868e-02,  6.96446933e-03,  3.68783646e-03, ...,\n",
       "                             -6.88382983e-03,  2.05170340e-03, -1.26798684e-03],\n",
       "                            [ 2.31853942e-03, -4.09393711e-03, -5.89999487e-04, ...,\n",
       "                             -1.30616492e-02, -1.10532483e-02,  3.76035972e-03]],\n",
       "                    \n",
       "                           [[ 6.27388421e-04,  1.94375706e-03, -7.56320590e-03, ...,\n",
       "                             -1.10668747e-03,  5.41144935e-03,  1.31051624e-02],\n",
       "                            [-1.14906929e-03,  6.54641725e-03, -1.68072600e-02, ...,\n",
       "                             -1.60588492e-02,  1.62326323e-04, -1.44504011e-03],\n",
       "                            [-4.68373211e-04, -1.93986259e-02, -7.64923962e-03, ...,\n",
       "                             -8.81672278e-03,  1.33583257e-02,  5.86138899e-03],\n",
       "                            [-1.19730569e-02,  6.62063947e-03, -3.53644695e-03, ...,\n",
       "                             -2.61843856e-03,  1.48882354e-02,  1.83876585e-02]],\n",
       "                    \n",
       "                           [[-4.64126701e-03, -1.41984094e-02, -1.43692922e-02, ...,\n",
       "                             -6.87857391e-03, -4.20213817e-03, -7.09458138e-04],\n",
       "                            [ 1.69501570e-03, -2.06355401e-03, -8.45665205e-03, ...,\n",
       "                              4.40626917e-03,  2.40007954e-04,  9.75598954e-03],\n",
       "                            [ 7.67474761e-03,  6.88532880e-03,  1.03522688e-02, ...,\n",
       "                             -3.42185376e-03, -1.18698105e-02, -7.31590157e-03],\n",
       "                            [-3.89883481e-03, -6.51360489e-03, -7.82806799e-03, ...,\n",
       "                             -1.17105614e-04, -4.70915530e-03, -1.53480284e-02]]],\n",
       "                          dtype=float32)>\n",
       "                    (o): <tf.Variable 'transformer/layer_._0/rel_attn/o:0' shape=(256, 4, 64) dtype=float32, numpy=\n",
       "                    array([[[-1.59462374e-02,  7.09896628e-03,  1.64417946e-03, ...,\n",
       "                              1.86597649e-02,  1.01802563e-02, -3.52442474e-03],\n",
       "                            [-2.34604022e-03,  1.39904208e-02,  6.23674178e-03, ...,\n",
       "                              2.46312399e-03, -5.10569103e-03, -1.46986656e-02],\n",
       "                            [ 1.70169417e-02,  6.76512346e-03,  1.26669090e-02, ...,\n",
       "                             -3.91981611e-03, -9.40794963e-03, -2.80247442e-03],\n",
       "                            [-1.50161786e-02,  1.28925256e-02, -1.24810133e-02, ...,\n",
       "                             -1.71831083e-02, -4.89449501e-03,  1.54844718e-02]],\n",
       "                    \n",
       "                           [[ 1.12314057e-03,  1.17070857e-03,  1.08005637e-02, ...,\n",
       "                              4.87507088e-03,  1.48537681e-02, -3.42207844e-04],\n",
       "                            [ 1.53690604e-02,  4.19465406e-03, -9.33476631e-03, ...,\n",
       "                              1.93952478e-03,  5.55025367e-03, -2.57873209e-03],\n",
       "                            [ 1.72732938e-02,  5.88704227e-03, -2.09000218e-03, ...,\n",
       "                              1.32829361e-02, -8.63811374e-03,  1.40973879e-02],\n",
       "                            [ 6.63836719e-03,  3.93198384e-03, -1.15679083e-02, ...,\n",
       "                             -1.24292951e-02, -3.88377754e-04,  1.76112793e-04]],\n",
       "                    \n",
       "                           [[-2.23665941e-03, -2.92175333e-04, -1.64830429e-03, ...,\n",
       "                              5.66885341e-03,  1.24743404e-02, -3.87190841e-03],\n",
       "                            [ 4.52636887e-04,  7.81212980e-03,  7.91000947e-03, ...,\n",
       "                             -1.08373305e-02, -9.01836902e-05,  6.94174133e-03],\n",
       "                            [-5.99821983e-03,  6.44895621e-03, -2.78561283e-03, ...,\n",
       "                             -1.37826020e-03,  1.10293766e-02,  8.53400677e-03],\n",
       "                            [ 6.16563903e-03, -1.50682242e-03,  1.26390308e-02, ...,\n",
       "                              5.30137448e-03,  1.40405595e-02, -1.09868801e-04]],\n",
       "                    \n",
       "                           ...,\n",
       "                    \n",
       "                           [[-6.01246534e-03, -1.17233451e-02, -5.27738081e-03, ...,\n",
       "                              1.08166421e-02, -7.63550168e-03,  6.74394099e-03],\n",
       "                            [ 6.21269504e-03, -2.64103385e-03, -1.52780795e-02, ...,\n",
       "                              2.81508267e-03, -7.19883293e-03, -6.15265570e-04],\n",
       "                            [-9.98945255e-03,  5.23763569e-03, -6.88339397e-03, ...,\n",
       "                              4.50991187e-03, -8.01084097e-03,  1.07206022e-02],\n",
       "                            [-5.22836484e-03, -1.80600048e-03,  7.31493952e-03, ...,\n",
       "                             -1.93624140e-03, -2.14240071e-03,  5.91469090e-03]],\n",
       "                    \n",
       "                           [[-1.12830000e-02,  8.81625980e-04, -1.01369014e-03, ...,\n",
       "                              1.04911793e-02, -6.69683795e-03, -2.12319172e-03],\n",
       "                            [-1.52743515e-02, -8.61280877e-03, -6.13969460e-04, ...,\n",
       "                              4.21738205e-03, -2.68975878e-03, -6.46529440e-03],\n",
       "                            [-7.41599966e-03,  7.29813939e-03, -8.30380525e-03, ...,\n",
       "                             -3.22207762e-03,  9.16641206e-03,  5.01654949e-03],\n",
       "                            [-1.38656842e-02, -1.45429559e-02, -5.68084093e-03, ...,\n",
       "                             -1.07372608e-02,  3.56571283e-03, -7.25981267e-03]],\n",
       "                    \n",
       "                           [[ 1.38473855e-02,  3.92660935e-04, -6.29336759e-03, ...,\n",
       "                             -1.91646791e-03,  8.09725747e-03,  9.37537476e-03],\n",
       "                            [-2.73346412e-03, -7.08647724e-03,  1.12332385e-02, ...,\n",
       "                             -1.43801756e-02, -7.22669950e-03,  7.08566047e-03],\n",
       "                            [ 1.12498170e-02, -1.30873797e-02, -2.15870808e-04, ...,\n",
       "                              5.54785598e-03, -1.19009353e-02,  7.77508970e-03],\n",
       "                            [-3.51362396e-03,  2.90276692e-03, -1.79425981e-02, ...,\n",
       "                              5.87725732e-03, -5.44556137e-03,  5.68768196e-03]]],\n",
       "                          dtype=float32)>\n",
       "                    (r): <tf.Variable 'transformer/layer_._0/rel_attn/r:0' shape=(256, 4, 64) dtype=float32, numpy=\n",
       "                    array([[[-0.00245649, -0.00472909,  0.00967604, ...,  0.00203733,\n",
       "                              0.00120894,  0.00568306],\n",
       "                            [-0.01331903,  0.01370894, -0.0098804 , ..., -0.01441352,\n",
       "                             -0.01202606,  0.01695489],\n",
       "                            [-0.00720078,  0.00048308, -0.01278234, ..., -0.01450641,\n",
       "                             -0.00093147, -0.01703029],\n",
       "                            [ 0.00636851,  0.00247038, -0.00314291, ..., -0.00461696,\n",
       "                              0.00265114,  0.00211391]],\n",
       "                    \n",
       "                           [[ 0.01008102,  0.00012481,  0.00684177, ..., -0.01311987,\n",
       "                              0.00672555,  0.0063989 ],\n",
       "                            [-0.00856095, -0.00113542, -0.00373594, ...,  0.00809254,\n",
       "                              0.00315868,  0.00107248],\n",
       "                            [ 0.00879459, -0.00882898,  0.00704892, ..., -0.0083779 ,\n",
       "                              0.00386497,  0.00160432],\n",
       "                            [-0.00455693,  0.00167196,  0.00785993, ...,  0.01073674,\n",
       "                              0.00832883,  0.01679543]],\n",
       "                    \n",
       "                           [[ 0.00751762, -0.00707852, -0.00085149, ..., -0.01867422,\n",
       "                             -0.0018981 , -0.00793842],\n",
       "                            [ 0.00340655, -0.00470047,  0.00883148, ...,  0.00254198,\n",
       "                              0.01313012, -0.01359838],\n",
       "                            [-0.01258144,  0.00590022,  0.00397595, ...,  0.00097627,\n",
       "                             -0.00220032, -0.00833726],\n",
       "                            [-0.0170747 ,  0.00648328,  0.0081662 , ...,  0.00128365,\n",
       "                              0.00326696, -0.00306636]],\n",
       "                    \n",
       "                           ...,\n",
       "                    \n",
       "                           [[-0.01334906, -0.01151874,  0.00079771, ...,  0.00826202,\n",
       "                             -0.01233951,  0.00590487],\n",
       "                            [ 0.00708014,  0.00508957,  0.00086172, ...,  0.00290268,\n",
       "                              0.01971706, -0.00479322],\n",
       "                            [ 0.00999542,  0.0156053 , -0.00211232, ..., -0.01472798,\n",
       "                              0.01872318,  0.00559984],\n",
       "                            [ 0.00857269,  0.00170904,  0.00576853, ...,  0.00918589,\n",
       "                             -0.00104371, -0.01227198]],\n",
       "                    \n",
       "                           [[ 0.00848153,  0.00263733, -0.00122638, ..., -0.00511883,\n",
       "                              0.00377996, -0.00656914],\n",
       "                            [-0.00039688,  0.00119215, -0.00754254, ..., -0.00365619,\n",
       "                              0.00018783,  0.00340034],\n",
       "                            [-0.01393688, -0.0054265 , -0.01363974, ..., -0.01363104,\n",
       "                             -0.0122125 , -0.00055366],\n",
       "                            [-0.00579688,  0.00330674, -0.00608863, ...,  0.00267224,\n",
       "                             -0.00747264,  0.01326886]],\n",
       "                    \n",
       "                           [[ 0.00102496, -0.01525298, -0.01149352, ...,  0.00044062,\n",
       "                              0.00421006, -0.00862175],\n",
       "                            [ 0.00757385, -0.00107995,  0.01130774, ..., -0.00054011,\n",
       "                              0.00508178, -0.00150619],\n",
       "                            [-0.00432921, -0.00673526, -0.0080264 , ...,  0.00621746,\n",
       "                              0.0051528 , -0.00311248],\n",
       "                            [-0.01774589,  0.01193669,  0.01023979, ...,  0.0136222 ,\n",
       "                             -0.00079578,  0.01513526]]], dtype=float32)>\n",
       "                    (r_r_bias): <tf.Variable 'transformer/layer_._0/rel_attn/r_r_bias:0' shape=(4, 64) dtype=float32, numpy=\n",
       "                    array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "                          dtype=float32)>\n",
       "                    (r_s_bias): <tf.Variable 'transformer/layer_._0/rel_attn/r_s_bias:0' shape=(4, 64) dtype=float32, numpy=\n",
       "                    array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "                          dtype=float32)>\n",
       "                    (r_w_bias): <tf.Variable 'transformer/layer_._0/rel_attn/r_w_bias:0' shape=(4, 64) dtype=float32, numpy=\n",
       "                    array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "                          dtype=float32)>\n",
       "                    (seg_embed): <tf.Variable 'transformer/layer_._0/rel_attn/seg_embed:0' shape=(2, 4, 64) dtype=float32, numpy=\n",
       "                    array([[[ 1.42587470e-02, -2.74015986e-03,  7.10329320e-03,\n",
       "                              9.27471835e-03,  5.97660476e-03,  5.72591042e-03,\n",
       "                              9.54735000e-03, -1.29871340e-02, -2.82773678e-03,\n",
       "                              1.44916782e-02, -4.70206840e-03,  6.38663114e-05,\n",
       "                              2.65252474e-03, -1.72060216e-03,  1.16729615e-02,\n",
       "                             -6.61933888e-03, -1.23342285e-02, -1.18986489e-02,\n",
       "                             -9.71719809e-03, -9.08953324e-03,  6.74338604e-04,\n",
       "                              9.00769327e-03, -1.52205378e-02, -6.23747893e-03,\n",
       "                              2.36045709e-03,  6.12826226e-03, -1.31326029e-03,\n",
       "                             -8.65172688e-03,  1.85088068e-02,  9.09934938e-03,\n",
       "                             -5.59888361e-03,  1.70234516e-02, -5.44952089e-03,\n",
       "                              1.90182794e-02,  1.45178037e-02,  4.54859715e-03,\n",
       "                              1.10985932e-03, -1.32407593e-02, -1.24414135e-02,\n",
       "                              8.59394763e-03, -7.48101994e-03, -4.42806492e-03,\n",
       "                              8.39218684e-03, -1.44365048e-02, -9.49770864e-03,\n",
       "                             -8.33270792e-03,  1.43358344e-02,  6.51078357e-04,\n",
       "                             -1.12595875e-03,  1.17379613e-03,  3.05967871e-03,\n",
       "                              4.20218479e-04,  3.30571341e-03, -3.75596015e-03,\n",
       "                              1.81648247e-02, -4.96183336e-03, -7.58308638e-03,\n",
       "                             -4.24149213e-03, -1.38480403e-02, -2.91570264e-04,\n",
       "                              3.80135141e-04, -1.12979729e-02,  1.63515983e-03,\n",
       "                              4.57958638e-04],\n",
       "                            [ 8.08123965e-03, -5.07104676e-03,  8.76853708e-03,\n",
       "                             -8.96159653e-03,  2.08500982e-03, -1.34714087e-02,\n",
       "                              6.85097184e-03, -2.89594801e-03,  7.23809144e-03,\n",
       "                              9.63562354e-03,  8.17904435e-03, -7.56176142e-03,\n",
       "                              6.22353051e-03, -6.06515165e-03,  1.37987584e-02,\n",
       "                             -1.14315422e-02, -3.49174789e-03,  7.72285601e-03,\n",
       "                              4.85471357e-03, -8.26512743e-03, -3.18485848e-03,\n",
       "                             -4.61831782e-03, -4.92264656e-03,  3.28882202e-03,\n",
       "                             -1.04626399e-02,  5.05984481e-03,  8.55141226e-03,\n",
       "                              6.10913010e-03, -3.00316769e-03, -1.78599579e-03,\n",
       "                             -5.91062848e-03,  3.37075721e-03,  1.04414281e-02,\n",
       "                              9.18126944e-03,  1.00647481e-02, -5.25194744e-04,\n",
       "                              1.73084047e-02, -1.05738547e-02,  6.52860152e-04,\n",
       "                              7.05980742e-03,  5.28999604e-03, -6.02741726e-03,\n",
       "                              3.81121831e-03, -2.84575881e-03,  1.15978308e-02,\n",
       "                             -1.31140687e-02, -7.82921445e-03, -3.00229900e-03,\n",
       "                             -8.33934639e-03,  4.38740896e-03, -7.74423126e-03,\n",
       "                             -4.87963807e-05, -1.16916262e-02, -1.42217707e-02,\n",
       "                              1.18226148e-02,  3.15128174e-03,  2.27348483e-03,\n",
       "                              2.80476897e-03, -6.27319934e-03, -3.96141841e-04,\n",
       "                             -1.08961249e-02, -1.51551107e-03,  2.36701802e-03,\n",
       "                             -1.00547373e-02],\n",
       "                            [-7.97784794e-03, -4.20246477e-04,  6.02311594e-03,\n",
       "                             -1.26946373e-02, -1.57751441e-02,  2.66095391e-03,\n",
       "                             -4.90776682e-03,  2.00060895e-03,  4.67469823e-03,\n",
       "                             -1.81481279e-02, -1.78106111e-02,  5.22828952e-04,\n",
       "                             -1.08855795e-02, -1.59953106e-02,  5.84422564e-03,\n",
       "                             -2.37566209e-03, -3.30726174e-03, -1.00089447e-03,\n",
       "                             -1.39910467e-02, -1.42001398e-02,  4.89653833e-03,\n",
       "                             -3.12344218e-03,  1.12657817e-02, -1.18234875e-02,\n",
       "                              2.57264823e-04, -1.19162882e-02, -8.04681797e-03,\n",
       "                             -5.79957571e-03,  1.40625173e-02,  8.61837342e-03,\n",
       "                             -4.49810224e-03,  3.02821025e-03,  6.66939246e-04,\n",
       "                              4.06629452e-03,  8.92135780e-03, -1.40877366e-02,\n",
       "                              3.60675086e-03, -2.03026878e-03,  4.54160245e-03,\n",
       "                              4.49390337e-03, -1.35192741e-02, -5.78049105e-03,\n",
       "                             -1.29773803e-02, -8.17125570e-03, -6.24270318e-03,\n",
       "                              7.58770341e-03,  3.65122512e-04, -7.01518613e-04,\n",
       "                              1.04506118e-02,  9.80909541e-03, -1.60788503e-02,\n",
       "                              1.10086203e-02,  1.43651422e-02,  1.00612892e-02,\n",
       "                             -1.01422158e-03,  3.04736826e-03,  1.77847538e-02,\n",
       "                             -1.32738650e-02,  1.36381611e-02, -1.48119656e-02,\n",
       "                             -1.06872246e-02,  2.31497083e-03,  8.37550592e-03,\n",
       "                              1.07482942e-02],\n",
       "                            [ 5.48687344e-03, -1.11145731e-02, -7.55836209e-03,\n",
       "                              1.55840779e-03,  6.79576164e-03, -1.93809140e-02,\n",
       "                              1.31612169e-02,  4.28688666e-03, -9.42352135e-03,\n",
       "                              8.65054689e-03,  2.72663264e-03, -5.37895830e-04,\n",
       "                              1.36647932e-02,  5.05219970e-04, -8.15693941e-03,\n",
       "                              1.13737583e-03, -1.25403404e-02, -1.38814701e-03,\n",
       "                              4.03548870e-03, -2.25537689e-03, -1.49235679e-02,\n",
       "                              4.79497574e-03,  7.73144467e-03, -1.54392477e-02,\n",
       "                              4.37705638e-03, -9.16368607e-03, -5.65965101e-03,\n",
       "                              1.92182902e-02,  7.74621847e-04,  1.45315938e-02,\n",
       "                             -1.41777918e-02,  5.73117519e-03,  2.50363513e-03,\n",
       "                              1.27815939e-02, -2.02765083e-03,  1.39806420e-04,\n",
       "                              9.70724877e-03,  1.03199920e-02,  1.39119457e-02,\n",
       "                             -1.72002744e-02,  3.68860946e-03, -3.90579971e-03,\n",
       "                              1.03387702e-02, -2.09546601e-03, -5.36197703e-03,\n",
       "                             -1.42045710e-02,  3.01083596e-03, -4.05518431e-03,\n",
       "                             -1.75515674e-02, -1.09159788e-02, -2.46921857e-03,\n",
       "                             -1.67165545e-03, -5.50166611e-03, -1.24355322e-02,\n",
       "                             -6.10407442e-03, -5.18002035e-03, -4.09316085e-03,\n",
       "                              1.72666535e-02, -9.73965600e-03,  8.28720105e-04,\n",
       "                             -1.77848455e-03,  1.15585024e-03,  1.23362821e-02,\n",
       "                             -1.97164398e-02]],\n",
       "                    \n",
       "                           [[ 1.58792280e-03, -1.40500779e-04,  2.73770629e-03,\n",
       "                             -1.78961493e-02,  4.44445002e-04, -7.84220756e-04,\n",
       "                             -5.59022604e-03,  8.90891533e-03,  2.77641020e-03,\n",
       "                              4.40776208e-03,  1.16188964e-02,  1.48738790e-02,\n",
       "                              1.44262537e-02,  2.69927713e-03, -7.00695906e-03,\n",
       "                             -4.18283325e-03,  3.71703948e-03, -8.78044590e-03,\n",
       "                              7.87651725e-03, -1.09734917e-02,  9.48923826e-03,\n",
       "                              1.57347706e-04,  9.45737585e-04,  1.48073828e-03,\n",
       "                             -1.60093829e-02,  6.89841853e-03,  8.08024965e-03,\n",
       "                             -1.24745036e-03, -3.79228010e-03,  1.41390776e-02,\n",
       "                             -3.88862565e-03,  1.30466493e-02,  4.03757021e-03,\n",
       "                             -3.06265848e-03,  1.93958113e-03, -4.37727105e-03,\n",
       "                             -9.38936253e-04,  5.54784248e-03,  1.21358894e-02,\n",
       "                             -1.16184065e-02, -1.56989321e-02, -6.91804104e-03,\n",
       "                             -1.27998265e-02, -3.90574243e-03, -1.48739070e-02,\n",
       "                             -7.78159965e-03, -4.55262512e-03, -9.82968137e-03,\n",
       "                              7.09615787e-03,  3.86492372e-03,  1.15131734e-04,\n",
       "                              1.52518470e-02,  2.92972056e-03, -1.30869553e-03,\n",
       "                              1.34209506e-02,  2.10403348e-03, -1.35818985e-03,\n",
       "                             -6.20409427e-03,  8.38143751e-06,  5.69451484e-04,\n",
       "                             -1.34014674e-02,  1.06194718e-02,  8.46672896e-03,\n",
       "                              3.56785278e-03],\n",
       "                            [-4.13210317e-03,  4.76811035e-03, -1.08925242e-03,\n",
       "                             -6.88217860e-03,  1.08810375e-02, -1.46757215e-02,\n",
       "                              9.53682978e-03,  1.53930597e-02,  1.02877449e-02,\n",
       "                              4.83370014e-03,  6.44173799e-03, -2.45631160e-03,\n",
       "                             -4.55019716e-03, -3.62752285e-03,  1.46732177e-03,\n",
       "                             -4.85561648e-03,  2.05396791e-03, -2.92000524e-03,\n",
       "                             -6.47087581e-03,  1.01347668e-02,  1.20817460e-02,\n",
       "                             -9.84675810e-03,  9.68009792e-03, -2.19843583e-03,\n",
       "                              3.84355988e-03, -4.27736994e-03,  8.74230918e-03,\n",
       "                              3.85986059e-03,  3.23211867e-03, -3.49229807e-03,\n",
       "                              3.04007693e-03, -1.28140943e-02,  1.59927178e-02,\n",
       "                             -7.50699220e-03,  4.95680049e-03, -1.47844739e-02,\n",
       "                             -5.04665542e-03,  1.45636890e-02,  8.78349587e-04,\n",
       "                             -7.11972453e-03,  9.88741592e-03, -2.45423405e-03,\n",
       "                             -2.00785417e-03,  4.84679453e-03,  1.06763458e-02,\n",
       "                              1.32056326e-02, -4.51974757e-03, -2.46611959e-03,\n",
       "                              1.00014713e-02, -9.03224479e-03,  4.09219507e-03,\n",
       "                              1.51429195e-02,  2.79768114e-03, -9.11126100e-03,\n",
       "                              1.58370715e-02, -4.41209506e-03,  8.69447179e-03,\n",
       "                             -4.39147511e-03,  7.44922971e-03, -1.93069002e-03,\n",
       "                              5.23098744e-03,  1.13368342e-02, -7.69325951e-03,\n",
       "                             -9.48811136e-03],\n",
       "                            [-1.38776060e-02,  1.28427753e-02,  3.56468518e-04,\n",
       "                             -7.75118452e-03, -4.56711184e-03,  6.53375592e-03,\n",
       "                             -7.51324696e-03, -1.95308297e-03,  4.56093159e-03,\n",
       "                              1.43536171e-02, -5.37678506e-03, -9.87627823e-03,\n",
       "                             -5.89765282e-03,  1.20970868e-02, -6.41718798e-04,\n",
       "                              4.64476971e-03,  4.30891901e-04,  2.03157356e-03,\n",
       "                             -1.28302360e-02, -1.11516817e-02, -2.91024189e-04,\n",
       "                              7.76773971e-03, -2.17013221e-04, -9.25213681e-04,\n",
       "                             -1.97410048e-03, -1.85431484e-02,  1.66798774e-02,\n",
       "                              3.18046822e-03, -1.00223040e-02,  1.31144328e-02,\n",
       "                              2.56141648e-03, -9.65663604e-03,  9.45151784e-03,\n",
       "                             -9.44997743e-03,  6.07229210e-03, -3.96577292e-04,\n",
       "                              4.82137920e-03,  7.12293480e-03,  8.68095085e-03,\n",
       "                             -1.47069956e-03,  6.80841086e-03,  1.06196236e-02,\n",
       "                             -1.07022366e-02,  4.37384890e-03, -1.61801511e-03,\n",
       "                              3.43605410e-03, -2.59177550e-03, -5.28896879e-03,\n",
       "                             -2.44895555e-03,  8.08581710e-03,  3.13257406e-05,\n",
       "                              1.90353952e-03, -8.41003656e-03,  6.39081327e-03,\n",
       "                              1.42933903e-02, -4.69536241e-03, -1.95344468e-03,\n",
       "                             -9.59238410e-03,  6.98012812e-03, -2.52199080e-03,\n",
       "                             -1.05092460e-02, -1.68008339e-02, -2.13806913e-03,\n",
       "                             -1.33298547e-03],\n",
       "                            [-1.14749325e-02,  1.12452330e-02,  1.57609005e-02,\n",
       "                             -9.02664848e-03, -7.30341906e-03,  8.34801793e-03,\n",
       "                              1.23722954e-02, -9.98139265e-04, -9.83115472e-03,\n",
       "                              1.50757823e-02,  9.94382426e-04,  1.16711026e-02,\n",
       "                             -1.65666509e-02, -1.11232279e-02,  2.24751281e-03,\n",
       "                             -1.51347099e-02,  1.46635957e-02, -1.91890553e-03,\n",
       "                             -1.08363284e-02,  2.09808029e-04, -1.17160855e-02,\n",
       "                             -8.78077559e-03, -8.74008238e-03,  1.42871658e-03,\n",
       "                              5.50163491e-03,  1.25905555e-02, -6.77871192e-03,\n",
       "                              1.48392878e-02, -3.27569596e-03, -1.29354428e-02,\n",
       "                             -4.44869744e-03,  4.93808417e-03,  2.87512061e-03,\n",
       "                             -5.41520212e-03,  5.72095066e-03, -2.52439035e-03,\n",
       "                              4.44017351e-03,  1.09997094e-02, -4.78388695e-03,\n",
       "                             -9.62561404e-04,  5.80096291e-03, -5.46655757e-03,\n",
       "                              7.65613699e-03, -6.19519968e-04, -3.87195556e-04,\n",
       "                             -9.13051423e-03,  1.04371842e-03,  4.34464449e-03,\n",
       "                             -1.93959323e-03,  7.06385588e-03, -1.11287814e-02,\n",
       "                              3.60587183e-05,  1.41959041e-02, -9.96854389e-04,\n",
       "                              4.27347150e-05,  1.03980871e-02, -3.07887240e-04,\n",
       "                             -1.53686153e-02,  1.70211948e-03,  9.75977071e-03,\n",
       "                              5.34731150e-03,  4.34576301e-03, -2.32411432e-03,\n",
       "                              3.58263217e-03]]], dtype=float32)>\n",
       "                    (_feature_shapes): Dict(\n",
       "                      (f_47_list_seq): TensorShape([1024, None])\n",
       "                      (f_68_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_last): TensorShape([1024, 1])\n",
       "                    )\n",
       "                    (_feature_dtypes): Dict(\n",
       "                      (f_47_list_seq): tf.int32\n",
       "                      (f_68_list_seq): tf.int32\n",
       "                      (item_id_list_seq): tf.int32\n",
       "                      (item_id_last): tf.int32\n",
       "                    )\n",
       "                  )\n",
       "                  (ff): TFXLNetFeedForward(\n",
       "                    (layer_norm): LayerNormalization(\n",
       "                      (axis): List(\n",
       "                        (0): 2\n",
       "                      )\n",
       "                      (gamma): <tf.Variable 'transformer/layer_._0/ff/layer_norm/gamma:0' shape=(256,) dtype=float32, numpy=\n",
       "                      array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1.], dtype=float32)>\n",
       "                      (beta): <tf.Variable 'transformer/layer_._0/ff/layer_norm/beta:0' shape=(256,) dtype=float32, numpy=\n",
       "                      array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0.], dtype=float32)>\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (layer_1): Dense(\n",
       "                      1024, activation=linear, use_bias=True\n",
       "                      (kernel): <tf.Variable 'transformer/layer_._0/ff/layer_1/kernel:0' shape=(256, 1024) dtype=float32, numpy=\n",
       "                      array([[ 0.00122021, -0.01309623, -0.00131465, ...,  0.01096298,\n",
       "                              -0.00373527, -0.01103484],\n",
       "                             [ 0.00052945,  0.00649835, -0.00691219, ..., -0.00324413,\n",
       "                               0.00074677, -0.01068254],\n",
       "                             [-0.00254404, -0.00082936, -0.00934914, ..., -0.00590009,\n",
       "                               0.00561833,  0.00665511],\n",
       "                             ...,\n",
       "                             [ 0.00735733, -0.0107628 ,  0.01237672, ..., -0.00340118,\n",
       "                               0.00917537,  0.00221891],\n",
       "                             [ 0.00099108,  0.0171504 , -0.00438142, ..., -0.00830791,\n",
       "                               0.01284312, -0.00783986],\n",
       "                             [-0.00323197, -0.00895801,  0.00666873, ..., -0.00548351,\n",
       "                               0.00032354,  0.01365087]], dtype=float32)>\n",
       "                      (bias): <tf.Variable 'transformer/layer_._0/ff/layer_1/bias:0' shape=(1024,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (layer_2): Dense(\n",
       "                      256, activation=linear, use_bias=True\n",
       "                      (kernel): <tf.Variable 'transformer/layer_._0/ff/layer_2/kernel:0' shape=(1024, 256) dtype=float32, numpy=\n",
       "                      array([[ 1.9046225e-02, -4.2805541e-03, -1.2044662e-02, ...,\n",
       "                              -1.8474824e-03,  1.6631940e-02, -9.7221285e-03],\n",
       "                             [ 1.0381548e-03, -3.0505168e-03, -1.9746067e-03, ...,\n",
       "                              -1.1931110e-02, -3.2283179e-03,  6.5794721e-04],\n",
       "                             [ 9.0870839e-03,  1.2008111e-02,  6.2230793e-03, ...,\n",
       "                              -1.1219397e-03,  1.4492314e-02, -1.2348772e-02],\n",
       "                             ...,\n",
       "                             [ 5.3155050e-03,  8.9985662e-04, -1.8180915e-03, ...,\n",
       "                               8.0853244e-03,  1.0584386e-02,  3.9044779e-03],\n",
       "                             [-6.7951321e-03,  1.1417157e-02,  7.9483297e-03, ...,\n",
       "                               1.8273877e-03,  1.5578029e-02,  9.7348784e-05],\n",
       "                             [-5.1361560e-03,  5.9644962e-03,  5.0693839e-03, ...,\n",
       "                               6.1548236e-03, -6.1377035e-03,  9.4038974e-03]], dtype=float32)>\n",
       "                      (bias): <tf.Variable 'transformer/layer_._0/ff/layer_2/bias:0' shape=(256,) dtype=float32, numpy=\n",
       "                      array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0.], dtype=float32)>\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (dropout): Dropout(\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (_feature_shapes): Dict(\n",
       "                      (f_47_list_seq): TensorShape([1024, None])\n",
       "                      (f_68_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_last): TensorShape([1024, 1])\n",
       "                    )\n",
       "                    (_feature_dtypes): Dict(\n",
       "                      (f_47_list_seq): tf.int32\n",
       "                      (f_68_list_seq): tf.int32\n",
       "                      (item_id_list_seq): tf.int32\n",
       "                      (item_id_last): tf.int32\n",
       "                    )\n",
       "                  )\n",
       "                  (dropout): Dropout(\n",
       "                    (_feature_shapes): Dict(\n",
       "                      (f_47_list_seq): TensorShape([1024, None])\n",
       "                      (f_68_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_last): TensorShape([1024, 1])\n",
       "                    )\n",
       "                    (_feature_dtypes): Dict(\n",
       "                      (f_47_list_seq): tf.int32\n",
       "                      (f_68_list_seq): tf.int32\n",
       "                      (item_id_list_seq): tf.int32\n",
       "                      (item_id_last): tf.int32\n",
       "                    )\n",
       "                  )\n",
       "                  (_feature_shapes): Dict(\n",
       "                    (f_47_list_seq): TensorShape([1024, None])\n",
       "                    (f_68_list_seq): TensorShape([1024, None])\n",
       "                    (item_id_list_seq): TensorShape([1024, None])\n",
       "                    (item_id_last): TensorShape([1024, 1])\n",
       "                  )\n",
       "                  (_feature_dtypes): Dict(\n",
       "                    (f_47_list_seq): tf.int32\n",
       "                    (f_68_list_seq): tf.int32\n",
       "                    (item_id_list_seq): tf.int32\n",
       "                    (item_id_last): tf.int32\n",
       "                  )\n",
       "                )\n",
       "                (1): TFXLNetLayer(\n",
       "                  (rel_attn): TFXLNetRelativeAttention(\n",
       "                    (layer_norm): LayerNormalization(\n",
       "                      (axis): List(\n",
       "                        (0): 2\n",
       "                      )\n",
       "                      (gamma): <tf.Variable 'transformer/layer_._1/rel_attn/layer_norm/gamma:0' shape=(256,) dtype=float32, numpy=\n",
       "                      array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1.], dtype=float32)>\n",
       "                      (beta): <tf.Variable 'transformer/layer_._1/rel_attn/layer_norm/beta:0' shape=(256,) dtype=float32, numpy=\n",
       "                      array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0.], dtype=float32)>\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (dropout): Dropout(\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (q): <tf.Variable 'transformer/layer_._1/rel_attn/q:0' shape=(256, 4, 64) dtype=float32, numpy=\n",
       "                    array([[[-3.39624850e-04, -1.28781414e-02,  1.33154923e-02, ...,\n",
       "                              1.21387001e-03, -6.99557131e-03, -5.70451375e-04],\n",
       "                            [-1.36007881e-02,  2.11063679e-03,  3.42921587e-03, ...,\n",
       "                             -3.70459398e-03, -3.94226285e-03,  1.37776248e-02],\n",
       "                            [ 1.02923149e-02, -1.28563493e-02,  7.45713990e-03, ...,\n",
       "                             -8.24920833e-04,  1.47402585e-02,  5.78313880e-03],\n",
       "                            [-3.88567074e-04, -1.25001082e-02,  5.95946505e-04, ...,\n",
       "                             -9.16351844e-03, -3.90949252e-04, -2.71281635e-04]],\n",
       "                    \n",
       "                           [[ 1.43211083e-02, -2.79461918e-03,  1.53058721e-03, ...,\n",
       "                              1.00539643e-02,  2.93701841e-03, -8.58693197e-03],\n",
       "                            [ 7.34081911e-03, -6.70789368e-03, -7.19675329e-03, ...,\n",
       "                              4.22976166e-03, -1.30614769e-02, -9.34202038e-03],\n",
       "                            [-1.45335384e-02,  5.51063335e-03, -3.68805882e-03, ...,\n",
       "                              1.15151443e-02,  7.59224035e-03, -1.76572092e-02],\n",
       "                            [-5.94963133e-03,  3.03026079e-03, -1.39374705e-02, ...,\n",
       "                             -9.08487663e-03, -2.85244733e-03, -9.89594776e-03]],\n",
       "                    \n",
       "                           [[ 3.27936234e-03,  1.35658830e-02, -1.03752380e-02, ...,\n",
       "                              9.92241502e-03, -1.38127860e-02, -6.47082785e-03],\n",
       "                            [-1.53806787e-02,  1.90806035e-02,  2.35680188e-03, ...,\n",
       "                             -1.61886346e-02, -6.46052277e-03, -6.62461971e-04],\n",
       "                            [-4.91399970e-03,  1.40662687e-02, -1.51016731e-02, ...,\n",
       "                              1.68874580e-02, -6.42739004e-03, -2.35232571e-03],\n",
       "                            [-3.37772025e-03, -7.26003759e-03, -1.52848382e-02, ...,\n",
       "                             -9.01892781e-03, -1.69964079e-02, -6.06126292e-03]],\n",
       "                    \n",
       "                           ...,\n",
       "                    \n",
       "                           [[ 1.17823966e-02,  6.83843857e-04, -4.63070668e-04, ...,\n",
       "                              1.61812305e-02,  6.82549225e-03,  7.25576701e-03],\n",
       "                            [ 8.87670461e-03,  1.15908775e-02,  3.91546819e-05, ...,\n",
       "                              2.87754461e-03, -1.01661112e-03,  5.58298873e-03],\n",
       "                            [ 1.00422166e-02,  8.44457932e-03,  6.89091906e-03, ...,\n",
       "                             -5.58940088e-03,  4.34795953e-03,  1.47155544e-03],\n",
       "                            [ 3.66431545e-03,  1.72057394e-02, -1.81378797e-03, ...,\n",
       "                             -3.76845640e-03, -3.42005724e-03, -7.93116353e-03]],\n",
       "                    \n",
       "                           [[-7.30168028e-03,  7.64483493e-03,  3.41430935e-03, ...,\n",
       "                             -1.28522217e-02,  3.52087663e-03, -1.02635110e-02],\n",
       "                            [ 5.25066489e-03,  6.76641380e-03,  2.98202876e-03, ...,\n",
       "                             -4.46871947e-03,  3.55644128e-03,  3.04684532e-03],\n",
       "                            [-1.60872061e-02,  7.02932070e-04,  9.13362671e-03, ...,\n",
       "                             -1.32489465e-02,  1.30526833e-02,  1.63722523e-02],\n",
       "                            [-1.08292205e-02,  7.55062327e-03,  5.84918773e-03, ...,\n",
       "                             -1.10203037e-02,  7.12311408e-03,  1.28332032e-02]],\n",
       "                    \n",
       "                           [[-6.18928520e-04, -1.14380522e-02, -1.48027865e-02, ...,\n",
       "                              9.87309217e-03,  6.12788554e-03, -5.58420457e-03],\n",
       "                            [-9.85667109e-03, -2.08209152e-04,  1.00642843e-02, ...,\n",
       "                              7.64709618e-03, -8.46095197e-03,  2.80563999e-03],\n",
       "                            [ 4.15816857e-03, -1.47195929e-03, -1.70231070e-02, ...,\n",
       "                              3.81558971e-03, -2.61934567e-03, -3.53924767e-03],\n",
       "                            [ 1.54306903e-03, -8.48435797e-03,  5.11316629e-03, ...,\n",
       "                              1.21480308e-03,  7.27321208e-03, -2.53065373e-03]]],\n",
       "                          dtype=float32)>\n",
       "                    (k): <tf.Variable 'transformer/layer_._1/rel_attn/k:0' shape=(256, 4, 64) dtype=float32, numpy=\n",
       "                    array([[[ 5.42450137e-03,  2.83959991e-04, -1.51200616e-03, ...,\n",
       "                              1.12599926e-03,  2.97799055e-03,  3.76485917e-03],\n",
       "                            [-4.31864941e-03, -7.85978802e-04,  1.93840768e-02, ...,\n",
       "                              1.06898900e-02,  3.61241028e-03,  1.84359066e-02],\n",
       "                            [ 5.51586086e-03,  1.02614556e-02,  2.45404523e-03, ...,\n",
       "                              1.62028987e-02, -1.25181116e-02, -7.18517276e-03],\n",
       "                            [-2.35458394e-03, -6.34417811e-04,  1.97484973e-03, ...,\n",
       "                             -2.64390884e-03,  3.21484939e-03,  1.77843068e-02]],\n",
       "                    \n",
       "                           [[ 5.29406546e-03, -4.28148638e-03,  1.50579796e-03, ...,\n",
       "                              1.81122473e-03,  7.05384417e-03, -1.52173908e-02],\n",
       "                            [-9.17960983e-03,  6.95911702e-03,  2.52705370e-03, ...,\n",
       "                              7.26173958e-03,  6.28032209e-03,  9.61477682e-03],\n",
       "                            [ 8.84466805e-03,  3.46087664e-03,  1.14751905e-02, ...,\n",
       "                              7.52994930e-03, -5.77013940e-03,  1.79726612e-02],\n",
       "                            [ 9.42425244e-03,  4.94692868e-05, -2.59726052e-03, ...,\n",
       "                              1.44906603e-02, -8.12600460e-03,  3.28478869e-03]],\n",
       "                    \n",
       "                           [[ 1.07674999e-02, -7.47285038e-03,  1.44729419e-02, ...,\n",
       "                              3.71507625e-03,  1.45271104e-02,  8.86796601e-03],\n",
       "                            [ 1.20876795e-02,  5.82983019e-03,  2.64066504e-03, ...,\n",
       "                             -1.68697145e-02,  3.55312019e-03, -8.47979262e-03],\n",
       "                            [ 1.88044421e-02, -6.91416021e-03, -1.16084440e-04, ...,\n",
       "                             -5.31761535e-03, -3.96327901e-04,  1.72034465e-03],\n",
       "                            [-1.93373375e-02,  4.55140974e-03,  1.04957046e-02, ...,\n",
       "                             -1.47887189e-02,  3.07553355e-03,  2.02496682e-04]],\n",
       "                    \n",
       "                           ...,\n",
       "                    \n",
       "                           [[-1.00333104e-02, -6.10837154e-03,  1.25476094e-02, ...,\n",
       "                              1.35877002e-02, -1.08423484e-02, -1.01526463e-02],\n",
       "                            [ 3.84213054e-03, -8.58113077e-03, -5.97968511e-03, ...,\n",
       "                             -9.08410549e-03, -9.65431985e-03, -1.06194532e-02],\n",
       "                            [ 1.40556032e-02, -4.92684124e-03, -1.07302768e-02, ...,\n",
       "                              2.40690797e-03,  4.49367845e-03, -1.65847316e-02],\n",
       "                            [-8.29094462e-03,  2.98063410e-03, -3.94774182e-03, ...,\n",
       "                              1.18639693e-02, -1.53874586e-04,  1.22983558e-02]],\n",
       "                    \n",
       "                           [[-4.33994783e-03, -1.39707504e-02,  4.05454403e-03, ...,\n",
       "                              1.20134349e-03,  4.68029082e-03,  4.61729505e-04],\n",
       "                            [ 1.32382580e-03,  6.83804136e-03, -3.73909948e-03, ...,\n",
       "                              2.80703907e-03, -3.90120479e-03,  5.70796011e-03],\n",
       "                            [ 1.08104395e-02, -1.60142640e-03,  1.58416163e-02, ...,\n",
       "                              1.38940141e-02,  4.67072194e-03, -1.29341031e-03],\n",
       "                            [-5.92910219e-03, -5.11635887e-03,  1.41355759e-02, ...,\n",
       "                              1.60698835e-02, -1.58308893e-02, -1.52231175e-02]],\n",
       "                    \n",
       "                           [[ 1.38249062e-02,  1.28363138e-02, -1.02579501e-03, ...,\n",
       "                              1.07007492e-02, -5.78083610e-03, -9.07300785e-03],\n",
       "                            [-8.83008074e-03, -4.14895220e-03, -3.43521382e-03, ...,\n",
       "                             -7.49352667e-03,  4.57015587e-03,  8.84641893e-03],\n",
       "                            [ 4.64058761e-03,  1.18657369e-02,  4.60573146e-03, ...,\n",
       "                             -2.88242428e-03, -2.26924405e-03, -5.11232251e-03],\n",
       "                            [-1.70492437e-02, -1.20344115e-02,  9.07966867e-03, ...,\n",
       "                              6.46697497e-03, -1.52522177e-02,  1.92636810e-03]]],\n",
       "                          dtype=float32)>\n",
       "                    (v): <tf.Variable 'transformer/layer_._1/rel_attn/v:0' shape=(256, 4, 64) dtype=float32, numpy=\n",
       "                    array([[[-6.94664335e-03,  3.44904023e-03, -1.05702858e-02, ...,\n",
       "                             -7.49731064e-03, -1.99360624e-02,  4.25055390e-03],\n",
       "                            [-1.67993214e-02, -3.95101216e-03, -2.36124732e-03, ...,\n",
       "                              1.25026070e-02,  3.60180042e-03,  1.26788870e-03],\n",
       "                            [-8.47792835e-04,  3.88997374e-03,  8.08258820e-03, ...,\n",
       "                              9.51867085e-03, -6.76184101e-03,  7.95172155e-03],\n",
       "                            [ 1.22575117e-02, -3.66264093e-03, -1.19783645e-02, ...,\n",
       "                              4.90018306e-03, -8.36496428e-03,  6.97060861e-03]],\n",
       "                    \n",
       "                           [[-8.31765239e-04, -1.58564784e-02, -2.84948084e-03, ...,\n",
       "                              3.96251492e-03,  9.96892340e-03, -1.67654771e-02],\n",
       "                            [-6.89227879e-03, -2.42467690e-03, -4.56067454e-03, ...,\n",
       "                              1.32132802e-04,  1.08100455e-02, -4.41864459e-03],\n",
       "                            [ 1.68221891e-02, -8.66271928e-03,  1.21586192e-02, ...,\n",
       "                             -3.88555275e-03,  1.83076726e-03,  1.37695670e-02],\n",
       "                            [ 5.21335006e-03,  4.24470985e-03,  2.93483748e-03, ...,\n",
       "                              9.44123510e-03,  1.07745349e-03, -7.36182323e-03]],\n",
       "                    \n",
       "                           [[ 2.74899625e-03,  1.13704763e-02,  2.56257760e-03, ...,\n",
       "                              6.47669518e-03,  1.32312439e-03, -3.63268773e-03],\n",
       "                            [-1.32964680e-03,  1.03199873e-02,  6.57364214e-03, ...,\n",
       "                              7.86196999e-03,  1.22902459e-02, -5.44173503e-03],\n",
       "                            [-2.15900666e-03, -1.86120104e-02,  1.48075158e-02, ...,\n",
       "                             -1.10076405e-02,  9.24740802e-04,  3.40092840e-04],\n",
       "                            [-1.25119854e-02,  1.19710304e-02,  7.59043905e-04, ...,\n",
       "                             -6.31694077e-03, -1.16576524e-02, -1.67639628e-02]],\n",
       "                    \n",
       "                           ...,\n",
       "                    \n",
       "                           [[-5.43129304e-03,  1.64948367e-02, -1.59463659e-02, ...,\n",
       "                              8.94702316e-05, -4.12290730e-03, -1.79776654e-03],\n",
       "                            [-1.91393755e-02,  1.45884287e-02,  5.30142523e-03, ...,\n",
       "                              8.00634082e-03, -5.13915252e-03, -2.87496927e-03],\n",
       "                            [ 1.13168033e-02, -6.91536348e-04, -5.24582202e-03, ...,\n",
       "                             -1.12005649e-02,  4.02790261e-03, -1.28285345e-02],\n",
       "                            [ 1.74363796e-02, -8.99822917e-03, -4.96895891e-03, ...,\n",
       "                             -8.89771804e-03, -6.59090700e-03, -5.81722427e-03]],\n",
       "                    \n",
       "                           [[-5.36436739e-04, -1.32774636e-02, -8.85420106e-03, ...,\n",
       "                             -1.71570461e-02,  5.45177702e-03,  3.05829453e-03],\n",
       "                            [-3.41444882e-03, -2.69355252e-03,  9.44185071e-03, ...,\n",
       "                              9.20481700e-03, -9.81202535e-03, -1.31013356e-02],\n",
       "                            [-1.43374233e-02, -1.91073474e-02,  7.23311072e-03, ...,\n",
       "                              9.73651186e-03, -9.59787052e-04,  1.42945107e-02],\n",
       "                            [-9.73679777e-03,  1.22692119e-02, -2.60175136e-03, ...,\n",
       "                              4.63515706e-03,  1.44672608e-02, -4.29231813e-03]],\n",
       "                    \n",
       "                           [[-9.10033192e-03,  3.64597472e-05, -1.52518228e-03, ...,\n",
       "                              9.85149876e-04,  1.11297928e-02, -1.27133087e-03],\n",
       "                            [ 2.08077137e-03, -3.61471460e-03,  4.50030062e-03, ...,\n",
       "                              5.56009216e-03,  1.13496967e-02, -1.87526140e-02],\n",
       "                            [-1.14198206e-02, -5.54469461e-03,  1.32042952e-02, ...,\n",
       "                             -7.12127145e-03,  1.25645951e-03, -1.76391599e-03],\n",
       "                            [ 7.31079839e-03, -6.53722370e-03, -1.73589829e-02, ...,\n",
       "                              2.59454851e-03,  1.67809557e-02, -1.22093530e-02]]],\n",
       "                          dtype=float32)>\n",
       "                    (o): <tf.Variable 'transformer/layer_._1/rel_attn/o:0' shape=(256, 4, 64) dtype=float32, numpy=\n",
       "                    array([[[ 9.90694482e-03,  9.19915549e-03,  4.55052499e-03, ...,\n",
       "                              1.75309777e-02, -8.09796527e-03,  4.70422348e-03],\n",
       "                            [ 1.13294297e-03, -8.81636143e-03, -8.02666694e-03, ...,\n",
       "                              9.31646768e-03,  1.54856769e-02, -8.44919414e-04],\n",
       "                            [ 4.97903815e-03, -5.34376502e-03, -7.37239560e-03, ...,\n",
       "                              5.60349505e-03,  1.62471496e-02, -2.90810852e-03],\n",
       "                            [ 1.43251149e-02,  3.23351851e-04,  8.96667782e-03, ...,\n",
       "                             -1.27031757e-02,  9.74851311e-04, -7.05617014e-03]],\n",
       "                    \n",
       "                           [[-1.83443688e-02,  4.88560321e-03, -1.26303844e-02, ...,\n",
       "                              6.14666939e-03, -1.21397469e-02,  2.51450483e-03],\n",
       "                            [ 7.57902069e-03,  1.76970605e-02, -1.41308568e-02, ...,\n",
       "                              1.21238660e-02,  1.15910089e-02,  6.63374458e-03],\n",
       "                            [-2.33345549e-03,  3.73115507e-03,  2.47339252e-04, ...,\n",
       "                              1.76690500e-02, -1.28021988e-03,  7.12560955e-03],\n",
       "                            [ 6.47917972e-04, -8.26801732e-03, -6.07109396e-03, ...,\n",
       "                              2.37577638e-04,  9.36206896e-03, -1.71620548e-02]],\n",
       "                    \n",
       "                           [[ 1.48252929e-02, -1.90919954e-02,  7.45534245e-03, ...,\n",
       "                              2.69764918e-03,  7.42993224e-03, -2.11907830e-03],\n",
       "                            [ 5.82710421e-03, -3.08827450e-03, -9.03808884e-03, ...,\n",
       "                             -1.06928200e-02,  3.44028085e-05,  1.81831904e-02],\n",
       "                            [ 7.44932890e-03,  1.19071649e-02, -1.17750941e-02, ...,\n",
       "                             -6.39759051e-03,  7.22725876e-04,  4.18556808e-03],\n",
       "                            [ 2.41537765e-03, -9.54782497e-03,  4.91821161e-03, ...,\n",
       "                             -3.42205609e-03,  4.88344580e-03,  4.92302980e-03]],\n",
       "                    \n",
       "                           ...,\n",
       "                    \n",
       "                           [[-5.96309360e-03, -1.62793472e-02, -1.49970166e-02, ...,\n",
       "                              3.79211991e-03,  8.30093306e-03,  1.77070033e-02],\n",
       "                            [-6.73453929e-03, -1.23746889e-02,  4.15165396e-03, ...,\n",
       "                              3.40084033e-03, -7.09060580e-03, -2.32222001e-03],\n",
       "                            [ 7.32192677e-03,  7.78322807e-03, -1.03766508e-02, ...,\n",
       "                              3.84692755e-03, -7.53495935e-03,  7.76162650e-03],\n",
       "                            [-5.22747450e-03,  5.98451076e-03, -1.86669566e-02, ...,\n",
       "                             -1.53187141e-02, -9.36225615e-03, -7.64218671e-03]],\n",
       "                    \n",
       "                           [[-1.52511382e-02,  4.41543013e-03, -8.18717014e-03, ...,\n",
       "                              1.56793417e-03, -2.51444033e-03,  7.09004514e-03],\n",
       "                            [ 1.56955188e-03, -5.28340926e-03,  2.57015647e-03, ...,\n",
       "                             -9.25794337e-03, -1.03968177e-02, -1.50227756e-03],\n",
       "                            [ 8.30347126e-04, -3.61889321e-03,  1.31165981e-02, ...,\n",
       "                              6.14446076e-03, -1.27106591e-03,  1.87005103e-02],\n",
       "                            [ 4.48661204e-03,  1.39052104e-02, -1.30520482e-02, ...,\n",
       "                             -4.79030045e-04, -1.11368103e-02, -1.46205835e-02]],\n",
       "                    \n",
       "                           [[-1.31460540e-02, -8.35874677e-03,  1.12157268e-02, ...,\n",
       "                              3.26133147e-03,  2.28171307e-03, -4.84846393e-03],\n",
       "                            [ 1.23516202e-03,  1.52659072e-02,  2.50169984e-03, ...,\n",
       "                             -4.73832572e-03,  9.31018125e-03, -1.13696151e-03],\n",
       "                            [-7.79936183e-03, -1.83688607e-02, -8.35975446e-03, ...,\n",
       "                             -7.03890854e-03, -1.10799167e-02, -2.37057777e-03],\n",
       "                            [ 2.20012362e-03, -2.85512907e-03,  1.78356189e-02, ...,\n",
       "                             -1.34995338e-02,  1.40254898e-02,  1.32735707e-02]]],\n",
       "                          dtype=float32)>\n",
       "                    (r): <tf.Variable 'transformer/layer_._1/rel_attn/r:0' shape=(256, 4, 64) dtype=float32, numpy=\n",
       "                    array([[[ 1.38909498e-03, -4.07922501e-03,  1.77812707e-02, ...,\n",
       "                             -3.49875283e-03,  4.53151437e-03, -1.65672519e-03],\n",
       "                            [ 9.57530341e-04,  2.16713408e-03, -4.39874741e-04, ...,\n",
       "                             -3.93526210e-03,  8.02193582e-03, -7.34446431e-03],\n",
       "                            [-1.02488315e-02, -7.20777130e-03, -1.59179121e-02, ...,\n",
       "                             -5.01494110e-03, -4.41415049e-03, -1.09208152e-02],\n",
       "                            [ 1.39121583e-03,  1.30918669e-03, -1.31989236e-03, ...,\n",
       "                              1.37307299e-02,  3.86714074e-03,  4.59832791e-03]],\n",
       "                    \n",
       "                           [[-9.65769589e-03,  1.74263213e-02,  3.89475899e-04, ...,\n",
       "                              2.98239669e-04, -2.56095454e-03,  1.25107616e-02],\n",
       "                            [-1.54808816e-03, -3.14068538e-03, -1.17548574e-02, ...,\n",
       "                              6.42717071e-03,  4.77788597e-03, -9.74107441e-03],\n",
       "                            [-9.19722579e-03, -5.36863785e-03,  6.25372864e-03, ...,\n",
       "                             -2.42419963e-04, -8.26372765e-03, -8.26864410e-03],\n",
       "                            [ 1.92860456e-03,  1.42827374e-03,  1.58268996e-02, ...,\n",
       "                             -9.49219335e-03, -4.40912042e-03,  2.62372941e-03]],\n",
       "                    \n",
       "                           [[-3.87747749e-03, -4.97773197e-03,  2.84075900e-03, ...,\n",
       "                              2.12681433e-03,  4.38758498e-03,  6.28014561e-03],\n",
       "                            [ 3.45937582e-03,  9.31069907e-03,  5.38155017e-03, ...,\n",
       "                              2.28996086e-03, -7.21405214e-03,  1.50495563e-02],\n",
       "                            [ 1.17504736e-02,  1.02927047e-03, -8.08535144e-03, ...,\n",
       "                              1.19494209e-02,  2.32760771e-03, -1.24995934e-03],\n",
       "                            [-1.10202131e-03,  5.23318350e-03,  6.47151330e-03, ...,\n",
       "                             -8.78635049e-03, -6.69592619e-03,  5.98837528e-03]],\n",
       "                    \n",
       "                           ...,\n",
       "                    \n",
       "                           [[-5.53456135e-03, -1.16550112e-02, -3.76451924e-03, ...,\n",
       "                              5.51126758e-03,  5.87565126e-03, -3.80779640e-03],\n",
       "                            [ 4.54956107e-03, -1.18347807e-02,  1.25550209e-02, ...,\n",
       "                             -1.75156386e-03, -4.08160547e-03, -3.67428362e-03],\n",
       "                            [-7.11359177e-03, -2.30407016e-03,  4.38143313e-03, ...,\n",
       "                             -1.18714501e-03,  3.66472034e-03, -5.93953114e-03],\n",
       "                            [ 3.83911980e-03,  1.57524534e-02, -4.83928295e-03, ...,\n",
       "                             -2.32920609e-03,  1.27369212e-02, -4.00709966e-03]],\n",
       "                    \n",
       "                           [[ 1.92171778e-03, -1.63355097e-02, -9.46655869e-03, ...,\n",
       "                             -1.65977876e-03, -3.98036791e-03,  9.85105243e-03],\n",
       "                            [-5.62448846e-03, -4.76045487e-03,  1.03804320e-02, ...,\n",
       "                             -3.81897972e-03, -1.88540723e-02, -1.46102710e-02],\n",
       "                            [ 4.19514021e-03,  8.21079966e-03,  1.55444629e-02, ...,\n",
       "                             -9.48785673e-05,  1.13205034e-02, -1.46535216e-02],\n",
       "                            [-9.92762204e-03,  2.57040584e-03,  3.57550010e-03, ...,\n",
       "                             -5.05627226e-03,  3.71973054e-03, -5.40841697e-03]],\n",
       "                    \n",
       "                           [[-1.30647598e-02,  1.25453458e-03,  7.19101867e-03, ...,\n",
       "                             -7.14690471e-03,  7.86982384e-03,  1.22145591e-02],\n",
       "                            [ 5.39236004e-03,  1.12324608e-02, -2.72116798e-04, ...,\n",
       "                             -1.03386221e-02, -1.43998917e-02,  1.52361030e-02],\n",
       "                            [-1.63695693e-03,  9.97741241e-03, -1.18541354e-02, ...,\n",
       "                              1.40824718e-02, -9.65302531e-03,  1.35626746e-02],\n",
       "                            [ 1.51319173e-03,  1.79371536e-02,  2.31638923e-03, ...,\n",
       "                             -1.25825657e-02, -2.07697554e-03,  5.76901203e-03]]],\n",
       "                          dtype=float32)>\n",
       "                    (r_r_bias): <tf.Variable 'transformer/layer_._1/rel_attn/r_r_bias:0' shape=(4, 64) dtype=float32, numpy=\n",
       "                    array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "                          dtype=float32)>\n",
       "                    (r_s_bias): <tf.Variable 'transformer/layer_._1/rel_attn/r_s_bias:0' shape=(4, 64) dtype=float32, numpy=\n",
       "                    array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "                          dtype=float32)>\n",
       "                    (r_w_bias): <tf.Variable 'transformer/layer_._1/rel_attn/r_w_bias:0' shape=(4, 64) dtype=float32, numpy=\n",
       "                    array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "                          dtype=float32)>\n",
       "                    (seg_embed): <tf.Variable 'transformer/layer_._1/rel_attn/seg_embed:0' shape=(2, 4, 64) dtype=float32, numpy=\n",
       "                    array([[[ 4.62806085e-03, -9.68601182e-03,  2.83580855e-03,\n",
       "                             -7.34532950e-03,  1.64039843e-02, -8.97031859e-04,\n",
       "                             -1.20242732e-02, -1.06353220e-02, -1.13050872e-02,\n",
       "                             -3.37017555e-04, -3.03919730e-03,  5.97301731e-03,\n",
       "                             -3.77476355e-03, -4.90297144e-03,  3.46007454e-03,\n",
       "                              1.07391095e-02, -6.30923547e-03, -1.30513720e-02,\n",
       "                             -1.38345715e-02,  1.23904394e-02, -5.33278612e-03,\n",
       "                              9.11072479e-04, -1.10141234e-02, -6.64919382e-04,\n",
       "                             -3.32516094e-04,  5.10975067e-03,  1.79513241e-04,\n",
       "                             -3.37016163e-03, -3.47353169e-04, -7.22669624e-03,\n",
       "                              1.03765698e-02, -5.07138576e-03, -1.66082680e-02,\n",
       "                              1.49569660e-02, -1.05179604e-02,  2.22114148e-03,\n",
       "                             -4.05722344e-03, -2.59252801e-03, -6.51346147e-03,\n",
       "                             -1.04511166e-02,  1.78222340e-02, -1.69898849e-03,\n",
       "                              6.67109760e-03,  8.66715144e-03, -4.79449332e-03,\n",
       "                              1.32664922e-03,  1.03728834e-03,  2.96358555e-03,\n",
       "                             -1.62560269e-02,  1.23702753e-02, -1.13800522e-02,\n",
       "                              4.23272233e-03,  4.98572411e-03,  1.07318349e-02,\n",
       "                             -8.33671819e-03,  4.18150751e-03, -4.35618742e-04,\n",
       "                              4.11730289e-04, -1.67401903e-03, -1.55019937e-02,\n",
       "                              1.86319556e-02, -3.86817637e-03, -9.23351676e-04,\n",
       "                             -6.83068670e-03],\n",
       "                            [ 7.66954478e-03,  7.27390172e-04,  7.92513788e-03,\n",
       "                              1.90598890e-02,  3.43158375e-03,  5.90585591e-03,\n",
       "                              5.03327837e-03, -2.26458156e-04, -2.02925690e-03,\n",
       "                              1.64617375e-02, -2.88724992e-03,  3.53944232e-03,\n",
       "                              5.58919692e-03, -1.88126527e-02,  3.03044962e-03,\n",
       "                              4.05925419e-03,  5.57564199e-03,  1.06965108e-02,\n",
       "                              9.90900397e-03,  1.15597807e-02, -4.70934063e-03,\n",
       "                             -1.94736221e-03,  7.20515056e-03, -1.26820272e-02,\n",
       "                              3.77873005e-03, -7.35957967e-03, -4.34886524e-03,\n",
       "                             -8.45411047e-03, -3.86702409e-03, -6.99648517e-05,\n",
       "                             -7.32462213e-04,  1.82808109e-03, -9.35317017e-03,\n",
       "                              1.81204099e-02,  1.06012011e-02, -9.23306041e-04,\n",
       "                              6.42012293e-03, -7.08352542e-03, -3.01060709e-03,\n",
       "                              1.03582593e-03,  8.15631635e-03, -3.57753062e-03,\n",
       "                             -9.51097719e-03,  1.02902828e-02,  1.26501627e-03,\n",
       "                             -9.08475835e-03, -7.79269449e-03, -8.19963217e-03,\n",
       "                              8.97380244e-03, -1.94293796e-03,  1.59252118e-02,\n",
       "                              1.05025833e-02,  1.26114897e-02,  4.19376232e-03,\n",
       "                              5.01311570e-03,  4.25587362e-03, -5.17983409e-03,\n",
       "                             -4.42952104e-03, -2.12477660e-03, -1.66993309e-02,\n",
       "                              5.71248448e-03,  5.40813897e-03,  5.51598612e-03,\n",
       "                             -3.25599662e-03],\n",
       "                            [-5.15101384e-03, -7.65948510e-03, -1.47792576e-02,\n",
       "                              6.38737762e-03,  1.14069255e-02, -3.31059535e-04,\n",
       "                              1.04504153e-02,  4.57512029e-03,  4.36607329e-03,\n",
       "                             -1.22880312e-02,  1.05094332e-02,  1.65189672e-02,\n",
       "                              1.23719149e-03, -6.97013410e-03,  1.86871458e-02,\n",
       "                             -7.25263404e-03,  1.48775754e-02,  7.69502437e-03,\n",
       "                             -2.79788929e-03, -3.35197616e-03,  1.53383585e-02,\n",
       "                              1.08128479e-04, -5.13585801e-05,  1.30181462e-02,\n",
       "                              1.28629003e-02,  7.59838731e-04,  9.20480490e-03,\n",
       "                              6.47100015e-03,  1.89641528e-02, -1.39599585e-03,\n",
       "                             -5.83728356e-03, -1.35829514e-02,  2.01219955e-04,\n",
       "                              1.39356626e-03, -2.92997062e-03, -9.15154908e-03,\n",
       "                              2.46023340e-03, -6.03461964e-03,  9.88091109e-04,\n",
       "                             -1.33710438e-02,  5.45785017e-03, -7.59084430e-03,\n",
       "                              4.16523362e-05,  6.44067349e-03, -6.16368745e-03,\n",
       "                             -7.91170076e-03,  1.92724429e-02,  6.90009445e-03,\n",
       "                              9.11016716e-04,  2.58873450e-03, -2.38341768e-03,\n",
       "                             -5.64301852e-03,  2.96562095e-04,  6.34547090e-04,\n",
       "                              4.40010848e-03, -7.24535016e-03,  3.98956193e-03,\n",
       "                             -2.95753591e-03, -3.53838969e-03,  6.77911611e-03,\n",
       "                             -2.67618325e-05,  6.13058684e-03, -1.01258606e-02,\n",
       "                              1.57647475e-03],\n",
       "                            [ 6.68889564e-03, -1.00204546e-03, -7.93018145e-04,\n",
       "                              6.17510313e-03, -8.33149534e-03, -1.96419135e-02,\n",
       "                             -1.17378868e-02,  1.28880199e-02,  4.14629607e-03,\n",
       "                              7.72718852e-03,  2.81094597e-03, -1.14334878e-02,\n",
       "                             -9.20706615e-03, -7.44277518e-03,  1.03499228e-02,\n",
       "                             -3.56208975e-03, -6.05122140e-03,  1.71729515e-03,\n",
       "                              5.33508649e-03, -9.80289932e-03, -1.21526374e-02,\n",
       "                              4.46523074e-03, -9.28350375e-04,  7.78551819e-03,\n",
       "                              1.23217059e-02, -6.66087959e-03, -5.17851207e-03,\n",
       "                              1.04647726e-02, -1.36832893e-02,  6.08231360e-03,\n",
       "                              9.93630034e-04,  7.46618316e-05,  3.45343398e-03,\n",
       "                             -8.10268428e-03, -9.35801677e-03, -3.06688569e-04,\n",
       "                             -1.46676821e-03, -1.97837260e-02,  1.34633752e-02,\n",
       "                              1.07328165e-02, -1.84139721e-02, -2.52956362e-03,\n",
       "                             -9.69896652e-03,  1.01372832e-02, -9.03707929e-03,\n",
       "                              1.82846468e-02,  4.20214143e-03,  4.46546683e-03,\n",
       "                              5.02903527e-03,  1.82728376e-02, -1.26874342e-03,\n",
       "                              2.06510143e-04,  7.97221437e-03, -1.26280999e-02,\n",
       "                             -1.47656566e-02, -1.25406226e-02, -1.07153342e-03,\n",
       "                             -4.88341320e-03,  7.13376736e-04,  1.59307923e-02,\n",
       "                              8.48660991e-03, -1.27488868e-02,  1.20827388e-02,\n",
       "                              1.05361559e-03]],\n",
       "                    \n",
       "                           [[ 1.44653837e-03,  7.74950534e-03,  1.13659538e-02,\n",
       "                              8.97879619e-03,  7.35111116e-03, -4.28893417e-03,\n",
       "                              2.81572086e-03, -1.10729393e-02, -5.62726799e-03,\n",
       "                              6.48005540e-03, -6.06330344e-03, -4.01809625e-03,\n",
       "                              1.53374381e-03, -9.11375042e-03,  7.96037633e-03,\n",
       "                              1.04899174e-02, -8.19361769e-03,  1.00210644e-02,\n",
       "                             -2.28176778e-03, -3.58833256e-03, -8.56953324e-04,\n",
       "                             -6.11461524e-04, -4.47415368e-04, -2.81810854e-03,\n",
       "                             -9.66344029e-03, -7.77932408e-04, -4.27551661e-03,\n",
       "                              9.39243939e-03, -1.96323986e-03,  6.75217714e-04,\n",
       "                             -5.28461998e-03, -2.93309544e-03, -1.14603685e-02,\n",
       "                              2.30616913e-03,  8.08039680e-03, -8.03550065e-04,\n",
       "                              1.55277988e-02, -1.68151117e-03, -1.13520166e-02,\n",
       "                             -1.50939804e-02,  1.00865848e-02,  2.53487797e-03,\n",
       "                              8.91864020e-03,  1.47153949e-02,  1.48319779e-02,\n",
       "                             -8.71909875e-03, -7.03025283e-03, -4.91185347e-03,\n",
       "                             -1.25447763e-02, -1.36873825e-03, -3.03882780e-03,\n",
       "                             -8.42883252e-03,  1.16040884e-03, -2.83577759e-03,\n",
       "                             -2.91829638e-04,  8.67089164e-03,  5.42938709e-03,\n",
       "                              5.50531130e-03,  3.13557056e-03,  1.16460295e-02,\n",
       "                             -1.72610246e-02, -1.54815791e-02,  1.91902742e-03,\n",
       "                              3.38946842e-03],\n",
       "                            [-1.46021433e-02, -2.33833655e-03,  2.34379535e-04,\n",
       "                              8.76559876e-03, -1.10434403e-03,  1.20605836e-02,\n",
       "                              5.49796410e-03,  1.38561390e-02, -6.49204897e-03,\n",
       "                              1.04517825e-02,  6.57541491e-03, -1.87523867e-04,\n",
       "                             -1.50415236e-02, -7.56404456e-03, -9.21093114e-03,\n",
       "                              2.64009391e-03, -1.81113947e-02,  1.70482614e-03,\n",
       "                              1.16779073e-03, -1.75440498e-03,  1.15971370e-02,\n",
       "                              1.65357664e-02, -8.61747563e-03, -1.35254757e-02,\n",
       "                             -6.99841604e-03, -3.16726207e-03,  1.21027725e-02,\n",
       "                             -1.30942641e-02, -1.51879282e-03,  9.11410339e-03,\n",
       "                              1.52264452e-02, -9.31373052e-03, -7.07557378e-03,\n",
       "                              1.13689043e-02,  6.63426379e-03,  3.61875375e-03,\n",
       "                             -1.06415851e-02, -8.33610538e-03,  1.39492378e-02,\n",
       "                             -3.21222190e-03, -5.14123309e-03, -1.34788007e-02,\n",
       "                              2.34441497e-04, -5.01721259e-03,  7.69646605e-03,\n",
       "                              1.67842451e-02,  1.69800166e-02,  3.85885383e-03,\n",
       "                             -8.79363716e-03, -1.39109953e-03, -5.24302572e-03,\n",
       "                              1.34934504e-02, -5.12435380e-03, -9.25462972e-03,\n",
       "                              1.34786213e-04, -1.09460708e-02,  3.16976663e-03,\n",
       "                              1.08000319e-02,  1.53681217e-03, -2.89364927e-03,\n",
       "                             -4.78657801e-03,  9.63284634e-03, -1.75662769e-03,\n",
       "                              1.66718662e-02],\n",
       "                            [ 1.15303192e-02, -1.53868580e-02, -1.66532258e-03,\n",
       "                             -6.36314927e-03,  1.45798158e-02, -5.64284436e-03,\n",
       "                             -3.23312893e-03,  1.74998445e-03,  1.03812898e-02,\n",
       "                             -5.33326389e-03, -5.66179072e-03, -1.52485464e-02,\n",
       "                             -1.30478041e-02, -1.82447210e-02,  2.83855759e-03,\n",
       "                              3.69692501e-03,  8.91913380e-03,  5.63706039e-04,\n",
       "                              1.60507038e-02,  1.00444425e-02,  5.33998711e-03,\n",
       "                             -8.86595342e-03,  8.50966386e-03,  9.75468382e-03,\n",
       "                              1.45015074e-02, -4.84026317e-03, -9.13827121e-03,\n",
       "                             -1.81006193e-02,  1.89733424e-03, -9.24547843e-04,\n",
       "                              1.93737708e-02, -1.06993143e-03,  7.29478779e-04,\n",
       "                              7.19771441e-03, -1.89105701e-02, -7.87647348e-03,\n",
       "                              3.09256883e-03, -4.16254997e-03,  9.41525586e-03,\n",
       "                             -1.94757164e-03,  1.01412972e-02, -1.64337195e-02,\n",
       "                              1.65399967e-03,  4.97433823e-03, -2.83314497e-03,\n",
       "                             -9.85563174e-03,  4.33375547e-03,  1.80403248e-03,\n",
       "                              1.79854431e-03,  1.23870885e-03,  1.23785362e-02,\n",
       "                             -1.12287281e-03, -6.03402313e-03,  1.04246605e-02,\n",
       "                             -3.33713205e-03, -1.42968306e-02, -1.16916085e-02,\n",
       "                              7.55676487e-03,  1.67978536e-02,  5.82034001e-03,\n",
       "                              2.08380749e-03,  1.85494423e-02,  1.26709454e-02,\n",
       "                              4.64432407e-03],\n",
       "                            [ 6.04328187e-03, -1.37207722e-02, -7.25615490e-03,\n",
       "                             -1.54793693e-03,  1.16102835e-02,  8.94045457e-04,\n",
       "                              2.26904172e-03, -6.26019761e-03, -7.76232220e-03,\n",
       "                             -1.04247546e-02, -1.26407994e-03,  9.05703008e-03,\n",
       "                              1.70618556e-02, -4.79489844e-03, -1.47173628e-02,\n",
       "                              7.64862960e-03,  3.18965781e-03,  2.25735432e-03,\n",
       "                             -2.76501430e-03,  1.88446883e-02,  1.79974288e-02,\n",
       "                              1.57776047e-02,  9.02223494e-03, -4.19281097e-03,\n",
       "                             -1.12790391e-02, -3.13230325e-03, -6.63973484e-03,\n",
       "                             -1.54406705e-03, -5.65184047e-03, -1.35493679e-02,\n",
       "                             -1.69028640e-02,  9.36755631e-03, -8.83506332e-03,\n",
       "                              7.90842064e-03,  8.24017171e-03, -1.85317397e-02,\n",
       "                             -1.02735395e-02,  1.01437476e-02,  3.88908782e-04,\n",
       "                             -6.51335297e-03,  1.41964518e-02,  1.90597028e-02,\n",
       "                              1.42490147e-02,  1.47047350e-02, -1.16532901e-03,\n",
       "                              6.09136932e-03, -1.29187619e-02, -1.05401790e-02,\n",
       "                             -4.62323485e-04, -5.03375835e-04, -9.09007154e-03,\n",
       "                             -7.90987629e-03, -2.99699209e-03, -4.15432081e-03,\n",
       "                             -1.33519284e-02, -7.86054507e-03,  4.17823950e-03,\n",
       "                              2.73026666e-03, -6.79025950e-04,  4.09244793e-03,\n",
       "                             -1.98086612e-02, -6.68471493e-03, -3.23093962e-03,\n",
       "                              1.18542705e-02]]], dtype=float32)>\n",
       "                    (_feature_shapes): Dict(\n",
       "                      (f_47_list_seq): TensorShape([1024, None])\n",
       "                      (f_68_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_last): TensorShape([1024, 1])\n",
       "                    )\n",
       "                    (_feature_dtypes): Dict(\n",
       "                      (f_47_list_seq): tf.int32\n",
       "                      (f_68_list_seq): tf.int32\n",
       "                      (item_id_list_seq): tf.int32\n",
       "                      (item_id_last): tf.int32\n",
       "                    )\n",
       "                  )\n",
       "                  (ff): TFXLNetFeedForward(\n",
       "                    (layer_norm): LayerNormalization(\n",
       "                      (axis): List(\n",
       "                        (0): 2\n",
       "                      )\n",
       "                      (gamma): <tf.Variable 'transformer/layer_._1/ff/layer_norm/gamma:0' shape=(256,) dtype=float32, numpy=\n",
       "                      array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                             1.], dtype=float32)>\n",
       "                      (beta): <tf.Variable 'transformer/layer_._1/ff/layer_norm/beta:0' shape=(256,) dtype=float32, numpy=\n",
       "                      array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0.], dtype=float32)>\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (layer_1): Dense(\n",
       "                      1024, activation=linear, use_bias=True\n",
       "                      (kernel): <tf.Variable 'transformer/layer_._1/ff/layer_1/kernel:0' shape=(256, 1024) dtype=float32, numpy=\n",
       "                      array([[ 0.00043725,  0.01716939, -0.00380494, ..., -0.01025831,\n",
       "                              -0.00692977,  0.0049489 ],\n",
       "                             [ 0.00086954, -0.01571346,  0.01405775, ...,  0.0049195 ,\n",
       "                              -0.00579125,  0.00367529],\n",
       "                             [-0.00836099, -0.00745235,  0.00523631, ...,  0.00107618,\n",
       "                              -0.00488085,  0.00371667],\n",
       "                             ...,\n",
       "                             [ 0.00113813, -0.0110951 ,  0.00916544, ...,  0.01049807,\n",
       "                              -0.01458427, -0.0095422 ],\n",
       "                             [ 0.00973646,  0.01052612,  0.00700123, ..., -0.01171636,\n",
       "                               0.00672918,  0.0118688 ],\n",
       "                             [ 0.00771985, -0.0054574 ,  0.00448734, ...,  0.00210588,\n",
       "                               0.00614319,  0.00165516]], dtype=float32)>\n",
       "                      (bias): <tf.Variable 'transformer/layer_._1/ff/layer_1/bias:0' shape=(1024,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (layer_2): Dense(\n",
       "                      256, activation=linear, use_bias=True\n",
       "                      (kernel): <tf.Variable 'transformer/layer_._1/ff/layer_2/kernel:0' shape=(1024, 256) dtype=float32, numpy=\n",
       "                      array([[-6.5369537e-04, -1.6172876e-03,  1.8638721e-02, ...,\n",
       "                              -6.8676462e-03, -1.4801252e-02, -1.1056796e-02],\n",
       "                             [-6.5794545e-03,  9.5226178e-03, -2.8762193e-03, ...,\n",
       "                               1.1626997e-02, -1.8733885e-03, -8.9665101e-04],\n",
       "                             [-9.8145641e-03,  1.1694854e-03, -5.4633245e-03, ...,\n",
       "                               1.0389142e-02, -1.1003226e-02,  1.0049177e-02],\n",
       "                             ...,\n",
       "                             [ 9.4902208e-03,  1.1001037e-02, -4.9600331e-03, ...,\n",
       "                               5.1104934e-03,  9.8847086e-03,  1.5714535e-02],\n",
       "                             [ 3.5875647e-03,  1.6647322e-02, -5.6149787e-03, ...,\n",
       "                               7.0091719e-03, -3.5582852e-05, -1.8791014e-02],\n",
       "                             [ 7.6086895e-04,  7.0511596e-03, -1.4011254e-02, ...,\n",
       "                               2.8382754e-03, -2.6837820e-03, -1.6288845e-02]], dtype=float32)>\n",
       "                      (bias): <tf.Variable 'transformer/layer_._1/ff/layer_2/bias:0' shape=(256,) dtype=float32, numpy=\n",
       "                      array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                             0.], dtype=float32)>\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (dropout): Dropout(\n",
       "                      (_feature_shapes): Dict(\n",
       "                        (f_47_list_seq): TensorShape([1024, None])\n",
       "                        (f_68_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_list_seq): TensorShape([1024, None])\n",
       "                        (item_id_last): TensorShape([1024, 1])\n",
       "                      )\n",
       "                      (_feature_dtypes): Dict(\n",
       "                        (f_47_list_seq): tf.int32\n",
       "                        (f_68_list_seq): tf.int32\n",
       "                        (item_id_list_seq): tf.int32\n",
       "                        (item_id_last): tf.int32\n",
       "                      )\n",
       "                    )\n",
       "                    (_feature_shapes): Dict(\n",
       "                      (f_47_list_seq): TensorShape([1024, None])\n",
       "                      (f_68_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_last): TensorShape([1024, 1])\n",
       "                    )\n",
       "                    (_feature_dtypes): Dict(\n",
       "                      (f_47_list_seq): tf.int32\n",
       "                      (f_68_list_seq): tf.int32\n",
       "                      (item_id_list_seq): tf.int32\n",
       "                      (item_id_last): tf.int32\n",
       "                    )\n",
       "                  )\n",
       "                  (dropout): Dropout(\n",
       "                    (_feature_shapes): Dict(\n",
       "                      (f_47_list_seq): TensorShape([1024, None])\n",
       "                      (f_68_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_list_seq): TensorShape([1024, None])\n",
       "                      (item_id_last): TensorShape([1024, 1])\n",
       "                    )\n",
       "                    (_feature_dtypes): Dict(\n",
       "                      (f_47_list_seq): tf.int32\n",
       "                      (f_68_list_seq): tf.int32\n",
       "                      (item_id_list_seq): tf.int32\n",
       "                      (item_id_last): tf.int32\n",
       "                    )\n",
       "                  )\n",
       "                  (_feature_shapes): Dict(\n",
       "                    (f_47_list_seq): TensorShape([1024, None])\n",
       "                    (f_68_list_seq): TensorShape([1024, None])\n",
       "                    (item_id_list_seq): TensorShape([1024, None])\n",
       "                    (item_id_last): TensorShape([1024, 1])\n",
       "                  )\n",
       "                  (_feature_dtypes): Dict(\n",
       "                    (f_47_list_seq): tf.int32\n",
       "                    (f_68_list_seq): tf.int32\n",
       "                    (item_id_list_seq): tf.int32\n",
       "                    (item_id_last): tf.int32\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(\n",
       "                (_feature_shapes): Dict(\n",
       "                  (f_47_list_seq): TensorShape([1024, None])\n",
       "                  (f_68_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_last): TensorShape([1024, 1])\n",
       "                )\n",
       "                (_feature_dtypes): Dict(\n",
       "                  (f_47_list_seq): tf.int32\n",
       "                  (f_68_list_seq): tf.int32\n",
       "                  (item_id_list_seq): tf.int32\n",
       "                  (item_id_last): tf.int32\n",
       "                )\n",
       "              )\n",
       "              (_kwargs): Dict(\n",
       "                (name): 'transformer'\n",
       "              )\n",
       "              (mask_emb): <tf.Variable 'model/mask_emb:0' shape=(1, 1, 256) dtype=float32, numpy=\n",
       "              array([[[-0.00368789,  0.00444443,  0.00211494,  0.00052619,\n",
       "                       -0.01565937, -0.01063097, -0.00101558, -0.00673222,\n",
       "                        0.01911413,  0.01429316, -0.00597647,  0.00942443,\n",
       "                        0.0090453 ,  0.00390143,  0.01415944, -0.01015258,\n",
       "                       -0.00230917, -0.00587802, -0.00847566, -0.01467544,\n",
       "                       -0.01563293, -0.0068689 ,  0.00142864,  0.00573018,\n",
       "                       -0.0019309 , -0.01714618, -0.00903177, -0.01002289,\n",
       "                        0.00989663,  0.00126065, -0.00360341,  0.00297336,\n",
       "                       -0.00334377,  0.00908262,  0.01653155, -0.00125164,\n",
       "                        0.010246  ,  0.00385871, -0.00212631, -0.00563519,\n",
       "                       -0.0193055 ,  0.01287888,  0.00807449, -0.01613857,\n",
       "                       -0.00188007, -0.01603613,  0.00563888,  0.00456584,\n",
       "                        0.00095871,  0.00431993, -0.01088995,  0.0116177 ,\n",
       "                       -0.00293967, -0.00441829, -0.01565825,  0.01794585,\n",
       "                       -0.00926415,  0.00593648,  0.00102861,  0.0055107 ,\n",
       "                       -0.00124564, -0.00026   ,  0.00059412,  0.00632476,\n",
       "                        0.01236673,  0.01462718,  0.01118617,  0.01025998,\n",
       "                       -0.00753015, -0.01314759,  0.00707592,  0.00023099,\n",
       "                       -0.00939742,  0.00431938, -0.00200593, -0.005034  ,\n",
       "                       -0.0181234 ,  0.00356684,  0.00521097, -0.00908969,\n",
       "                       -0.00928476, -0.00449117,  0.01496515, -0.00120328,\n",
       "                       -0.00765012,  0.00562503, -0.00109068,  0.01507501,\n",
       "                        0.0017166 , -0.00959032, -0.00553446,  0.01832069,\n",
       "                        0.00316651, -0.0013068 ,  0.00966615, -0.00779426,\n",
       "                       -0.00152102,  0.01381326, -0.00596952, -0.01157475,\n",
       "                        0.00653097,  0.0133415 , -0.00162687, -0.01768287,\n",
       "                        0.00336552,  0.01184487, -0.00815475,  0.00409813,\n",
       "                        0.00253362, -0.00198899, -0.01935337,  0.00130856,\n",
       "                       -0.00089251, -0.00291687,  0.01610414, -0.01544971,\n",
       "                        0.0005924 , -0.00303508,  0.01476053, -0.00511748,\n",
       "                        0.00712092,  0.01187666,  0.00315231,  0.0086514 ,\n",
       "                       -0.01138869, -0.01252414,  0.01400947, -0.00219629,\n",
       "                        0.00351593,  0.00479242,  0.00674055,  0.01400801,\n",
       "                        0.00039298,  0.00190988, -0.00134995,  0.00537574,\n",
       "                        0.00885713, -0.00548384, -0.01745418, -0.00461525,\n",
       "                        0.00708167, -0.00512621,  0.0093583 , -0.00782992,\n",
       "                        0.0073394 , -0.00600291, -0.00129769,  0.01337706,\n",
       "                        0.01122419,  0.00017401, -0.00387513,  0.00229089,\n",
       "                       -0.01584349, -0.0026956 ,  0.00074849, -0.00305802,\n",
       "                        0.00302223, -0.00832318,  0.00568213,  0.00792485,\n",
       "                       -0.00759037, -0.01551365, -0.00251418,  0.01577918,\n",
       "                       -0.0165049 ,  0.00482887,  0.00926293,  0.00562355,\n",
       "                       -0.00593899,  0.00449037, -0.01599763, -0.01066699,\n",
       "                        0.00053705, -0.00103044,  0.00676742, -0.00712728,\n",
       "                       -0.01611998,  0.00434968, -0.00483386,  0.00474722,\n",
       "                        0.00911529,  0.00928614,  0.00522785,  0.0194924 ,\n",
       "                        0.0107628 , -0.00022254,  0.00399482,  0.00215491,\n",
       "                        0.01581914, -0.01017098,  0.005125  , -0.01735075,\n",
       "                        0.01215216,  0.00437759,  0.0068762 ,  0.01352768,\n",
       "                       -0.01238116, -0.00955847, -0.00077273,  0.01567098,\n",
       "                       -0.00535144,  0.0047448 ,  0.00410056,  0.00734857,\n",
       "                       -0.01347085, -0.00462248,  0.00366341, -0.00937564,\n",
       "                        0.01380444,  0.00626946,  0.00249485,  0.00265482,\n",
       "                        0.00158451,  0.00104216, -0.01834268, -0.00293599,\n",
       "                       -0.01138022,  0.00706604, -0.0004241 , -0.00636189,\n",
       "                       -0.00303335, -0.00322865,  0.01015065,  0.01640783,\n",
       "                        0.00079055,  0.0123895 , -0.00201562, -0.00296379,\n",
       "                       -0.00945247,  0.00311439,  0.00784434,  0.0043023 ,\n",
       "                       -0.01348799,  0.0045474 ,  0.00464799,  0.00815969,\n",
       "                        0.01436242,  0.00041552,  0.00024658, -0.00870907,\n",
       "                        0.00446955, -0.0033238 ,  0.00797869,  0.00702652,\n",
       "                        0.01010207,  0.01753412, -0.01115743,  0.00990481,\n",
       "                        0.01158433,  0.00266878,  0.00536768, -0.00834788,\n",
       "                       -0.01560667, -0.00619452,  0.00620877, -0.00184965]]],\n",
       "                    dtype=float32)>\n",
       "              (_feature_shapes): Dict(\n",
       "                (f_47_list_seq): TensorShape([1024, None])\n",
       "                (f_68_list_seq): TensorShape([1024, None])\n",
       "                (item_id_list_seq): TensorShape([1024, None])\n",
       "                (item_id_last): TensorShape([1024, 1])\n",
       "              )\n",
       "              (_feature_dtypes): Dict(\n",
       "                (f_47_list_seq): tf.int32\n",
       "                (f_68_list_seq): tf.int32\n",
       "                (item_id_list_seq): tf.int32\n",
       "                (item_id_last): tf.int32\n",
       "              )\n",
       "            )\n",
       "            (transformer_pre): PrepareTransformerInputs(\n",
       "              (_feature_shapes): Dict(\n",
       "                (f_47_list_seq): TensorShape([1024, None])\n",
       "                (f_68_list_seq): TensorShape([1024, None])\n",
       "                (item_id_list_seq): TensorShape([1024, None])\n",
       "                (item_id_last): TensorShape([1024, 1])\n",
       "              )\n",
       "              (_feature_dtypes): Dict(\n",
       "                (f_47_list_seq): tf.int32\n",
       "                (f_68_list_seq): tf.int32\n",
       "                (item_id_list_seq): tf.int32\n",
       "                (item_id_last): tf.int32\n",
       "              )\n",
       "            )\n",
       "            (transformer_post): LastHiddenState(\n",
       "              (_feature_shapes): Dict(\n",
       "                (f_47_list_seq): TensorShape([1024, None])\n",
       "                (f_68_list_seq): TensorShape([1024, None])\n",
       "                (item_id_list_seq): TensorShape([1024, None])\n",
       "                (item_id_last): TensorShape([1024, 1])\n",
       "              )\n",
       "              (_feature_dtypes): Dict(\n",
       "                (f_47_list_seq): tf.int32\n",
       "                (f_68_list_seq): tf.int32\n",
       "                (item_id_list_seq): tf.int32\n",
       "                (item_id_last): tf.int32\n",
       "              )\n",
       "            )\n",
       "            (post): SequenceMean(\n",
       "              (_feature_shapes): Dict(\n",
       "                (f_47_list_seq): TensorShape([1024, None])\n",
       "                (f_68_list_seq): TensorShape([1024, None])\n",
       "                (item_id_list_seq): TensorShape([1024, None])\n",
       "                (item_id_last): TensorShape([1024, 1])\n",
       "              )\n",
       "              (_feature_dtypes): Dict(\n",
       "                (f_47_list_seq): tf.int32\n",
       "                (f_68_list_seq): tf.int32\n",
       "                (item_id_list_seq): tf.int32\n",
       "                (item_id_last): tf.int32\n",
       "              )\n",
       "            )\n",
       "            (_feature_shapes): Dict(\n",
       "              (f_47_list_seq): TensorShape([1024, None])\n",
       "              (f_68_list_seq): TensorShape([1024, None])\n",
       "              (item_id_list_seq): TensorShape([1024, None])\n",
       "              (item_id_last): TensorShape([1024, 1])\n",
       "            )\n",
       "            (_feature_dtypes): Dict(\n",
       "              (f_47_list_seq): tf.int32\n",
       "              (f_68_list_seq): tf.int32\n",
       "              (item_id_list_seq): tf.int32\n",
       "              (item_id_last): tf.int32\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (_feature_shapes): Dict(\n",
       "          (f_47_list_seq): TensorShape([1024, None])\n",
       "          (f_68_list_seq): TensorShape([1024, None])\n",
       "          (item_id_list_seq): TensorShape([1024, None])\n",
       "          (item_id_last): TensorShape([1024, 1])\n",
       "        )\n",
       "        (_feature_dtypes): Dict(\n",
       "          (f_47_list_seq): tf.int32\n",
       "          (f_68_list_seq): tf.int32\n",
       "          (item_id_list_seq): tf.int32\n",
       "          (item_id_last): tf.int32\n",
       "        )\n",
       "      )\n",
       "      (cat_inputs): ParallelBlock(\n",
       "        (_aggregation): ConcatFeatures(\n",
       "          (_feature_shapes): Dict(\n",
       "            (f_47_list_seq): TensorShape([1024, None])\n",
       "            (f_68_list_seq): TensorShape([1024, None])\n",
       "            (item_id_list_seq): TensorShape([1024, None])\n",
       "            (item_id_last): TensorShape([1024, 1])\n",
       "          )\n",
       "          (_feature_dtypes): Dict(\n",
       "            (f_47_list_seq): tf.int32\n",
       "            (f_68_list_seq): tf.int32\n",
       "            (item_id_list_seq): tf.int32\n",
       "            (item_id_last): tf.int32\n",
       "          )\n",
       "        )\n",
       "        (parallel_layers): Dict(\n",
       "          (categorical): ParallelBlock(\n",
       "            (parallel_layers): Dict(\n",
       "              (item_id_purchase_id): EmbeddingTable(\n",
       "                (features): Dict(\n",
       "                  (item_id_last): ColumnSchema(name='item_id_last', tags={<Tags.ID: 'id'>, <Tags.ITEM: 'item'>, <Tags.ITEM_ID: 'item_id'>, <Tags.CATEGORICAL: 'categorical'>}, properties={'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 1.0, 'cat_path': './/categories/unique.item_id_purchase_id.parquet', 'embedding_sizes': {'cardinality': 23272.0, 'dimension': 446.0}, 'domain': {'min': 0, 'max': 23271, 'name': 'item_id_purchase_id'}}, dtype=dtype('int32'), is_list=False, is_ragged=False)\n",
       "                )\n",
       "                (table): Embedding(\n",
       "                  (embeddings): <tf.Variable 'parallel_block_1/embeddings:0' shape=(23272, 256) dtype=float32, numpy=\n",
       "                  array([[ 3.6688630e-02, -1.5694238e-02,  2.6231039e-02, ...,\n",
       "                          -7.4518695e-03, -2.5550593e-02,  4.2430650e-02],\n",
       "                         [-1.1260748e-02,  2.7411427e-02, -1.4256310e-02, ...,\n",
       "                           4.6440367e-02, -2.0525312e-02, -3.9007377e-02],\n",
       "                         [-1.4978837e-02,  1.5060198e-02,  2.0313766e-02, ...,\n",
       "                           2.1291886e-02, -1.6071964e-02,  3.2992911e-02],\n",
       "                         ...,\n",
       "                         [-4.8669744e-02,  3.8978908e-02,  3.4969810e-02, ...,\n",
       "                           7.5179115e-03, -3.3062696e-03, -3.1209374e-02],\n",
       "                         [-2.8146021e-03, -9.7707398e-03,  4.2427331e-05, ...,\n",
       "                           3.5752807e-02, -7.8923590e-03,  3.6476362e-02],\n",
       "                         [ 2.5456075e-02, -7.2887540e-03,  3.8526919e-02, ...,\n",
       "                           4.2750265e-02,  1.1477694e-03, -4.4036772e-02]], dtype=float32)>\n",
       "                  (_feature_shapes): Dict(\n",
       "                    (f_47_list_seq): TensorShape([1024, None])\n",
       "                    (f_68_list_seq): TensorShape([1024, None])\n",
       "                    (item_id_list_seq): TensorShape([1024, None])\n",
       "                    (item_id_last): TensorShape([1024, 1])\n",
       "                  )\n",
       "                  (_feature_dtypes): Dict(\n",
       "                    (f_47_list_seq): tf.int32\n",
       "                    (f_68_list_seq): tf.int32\n",
       "                    (item_id_list_seq): tf.int32\n",
       "                    (item_id_last): tf.int32\n",
       "                  )\n",
       "                )\n",
       "                (_feature_shapes): Dict(\n",
       "                  (f_47_list_seq): TensorShape([1024, None])\n",
       "                  (f_68_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_list_seq): TensorShape([1024, None])\n",
       "                  (item_id_last): TensorShape([1024, 1])\n",
       "                )\n",
       "                (_feature_dtypes): Dict(\n",
       "                  (f_47_list_seq): tf.int32\n",
       "                  (f_68_list_seq): tf.int32\n",
       "                  (item_id_list_seq): tf.int32\n",
       "                  (item_id_last): tf.int32\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (_feature_shapes): Dict(\n",
       "              (f_47_list_seq): TensorShape([1024, None])\n",
       "              (f_68_list_seq): TensorShape([1024, None])\n",
       "              (item_id_list_seq): TensorShape([1024, None])\n",
       "              (item_id_last): TensorShape([1024, 1])\n",
       "            )\n",
       "            (_feature_dtypes): Dict(\n",
       "              (f_47_list_seq): tf.int32\n",
       "              (f_68_list_seq): tf.int32\n",
       "              (item_id_list_seq): tf.int32\n",
       "              (item_id_last): tf.int32\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (_feature_shapes): Dict(\n",
       "          (f_47_list_seq): TensorShape([1024, None])\n",
       "          (f_68_list_seq): TensorShape([1024, None])\n",
       "          (item_id_list_seq): TensorShape([1024, None])\n",
       "          (item_id_last): TensorShape([1024, 1])\n",
       "        )\n",
       "        (_feature_dtypes): Dict(\n",
       "          (f_47_list_seq): tf.int32\n",
       "          (f_68_list_seq): tf.int32\n",
       "          (item_id_list_seq): tf.int32\n",
       "          (item_id_last): tf.int32\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (_feature_shapes): Dict(\n",
       "      (f_47_list_seq): TensorShape([1024, None])\n",
       "      (f_68_list_seq): TensorShape([1024, None])\n",
       "      (item_id_list_seq): TensorShape([1024, None])\n",
       "      (item_id_last): TensorShape([1024, 1])\n",
       "    )\n",
       "    (_feature_dtypes): Dict(\n",
       "      (f_47_list_seq): tf.int32\n",
       "      (f_68_list_seq): tf.int32\n",
       "      (item_id_list_seq): tf.int32\n",
       "      (item_id_last): tf.int32\n",
       "    )\n",
       "  ), MLPBlock(\n",
       "    (layers): List(\n",
       "      (0): _Dense(\n",
       "        (dense): Dense(\n",
       "          32, activation=relu, use_bias=True\n",
       "          (kernel): <tf.Variable 'sequential_block_4/sequential_block_3/private__dense_4/dense_4/kernel:0' shape=(288, 32) dtype=float32, numpy=\n",
       "          array([[ 0.13668561, -0.00331074,  0.05509979, ..., -0.1048767 ,\n",
       "                  -0.06435522,  0.1269579 ],\n",
       "                 [-0.02135976,  0.11579561,  0.09275207, ..., -0.08785652,\n",
       "                   0.03408624, -0.13315791],\n",
       "                 [ 0.0110897 ,  0.12874144, -0.07588747, ...,  0.09341864,\n",
       "                  -0.12620835, -0.0447849 ],\n",
       "                 ...,\n",
       "                 [-0.03476546,  0.08247851, -0.08052737, ...,  0.11434636,\n",
       "                  -0.09653316,  0.07015914],\n",
       "                 [ 0.05088949, -0.09901311,  0.1276901 , ..., -0.12818041,\n",
       "                   0.05082168,  0.13061997],\n",
       "                 [ 0.02416109, -0.09868706,  0.06914937, ...,  0.0439043 ,\n",
       "                   0.00019096, -0.11566934]], dtype=float32)>\n",
       "          (bias): <tf.Variable 'sequential_block_4/sequential_block_3/private__dense_4/dense_4/bias:0' shape=(32,) dtype=float32, numpy=\n",
       "          array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                dtype=float32)>\n",
       "          (_feature_shapes): Dict(\n",
       "            (f_47_list_seq): TensorShape([1024, None])\n",
       "            (f_68_list_seq): TensorShape([1024, None])\n",
       "            (item_id_list_seq): TensorShape([1024, None])\n",
       "            (item_id_last): TensorShape([1024, 1])\n",
       "          )\n",
       "          (_feature_dtypes): Dict(\n",
       "            (f_47_list_seq): tf.int32\n",
       "            (f_68_list_seq): tf.int32\n",
       "            (item_id_list_seq): tf.int32\n",
       "            (item_id_last): tf.int32\n",
       "          )\n",
       "        )\n",
       "        (_feature_shapes): Dict(\n",
       "          (f_47_list_seq): TensorShape([1024, None])\n",
       "          (f_68_list_seq): TensorShape([1024, None])\n",
       "          (item_id_list_seq): TensorShape([1024, None])\n",
       "          (item_id_last): TensorShape([1024, 1])\n",
       "        )\n",
       "        (_feature_dtypes): Dict(\n",
       "          (f_47_list_seq): tf.int32\n",
       "          (f_68_list_seq): tf.int32\n",
       "          (item_id_list_seq): tf.int32\n",
       "          (item_id_last): tf.int32\n",
       "        )\n",
       "      )\n",
       "      (1): Dropout(\n",
       "        (_feature_shapes): Dict(\n",
       "          (f_47_list_seq): TensorShape([1024, None])\n",
       "          (f_68_list_seq): TensorShape([1024, None])\n",
       "          (item_id_list_seq): TensorShape([1024, None])\n",
       "          (item_id_last): TensorShape([1024, 1])\n",
       "        )\n",
       "        (_feature_dtypes): Dict(\n",
       "          (f_47_list_seq): tf.int32\n",
       "          (f_68_list_seq): tf.int32\n",
       "          (item_id_list_seq): tf.int32\n",
       "          (item_id_last): tf.int32\n",
       "        )\n",
       "      )\n",
       "      (2): _Dense(\n",
       "        (dense): Dense(\n",
       "          256, activation=linear, use_bias=True\n",
       "          (kernel): <tf.Variable 'sequential_block_4/sequential_block_3/private__dense_5/dense_5/kernel:0' shape=(32, 256) dtype=float32, numpy=\n",
       "          array([[ 0.05131534, -0.05413289, -0.08483787, ...,  0.13316393,\n",
       "                   0.0393841 ,  0.12440467],\n",
       "                 [-0.03066889,  0.01768437,  0.02352177, ..., -0.11354895,\n",
       "                   0.12201235, -0.13605091],\n",
       "                 [-0.13135311,  0.05547984,  0.0597591 , ..., -0.0390357 ,\n",
       "                  -0.01086327,  0.03687899],\n",
       "                 ...,\n",
       "                 [-0.03804816, -0.12745136, -0.01011722, ..., -0.07244129,\n",
       "                  -0.02058902, -0.02280251],\n",
       "                 [-0.06398636,  0.09930643,  0.07152282, ..., -0.07264003,\n",
       "                   0.1368024 ,  0.10540514],\n",
       "                 [ 0.11821625,  0.09571522, -0.05523643, ...,  0.10063526,\n",
       "                  -0.13430037, -0.06427357]], dtype=float32)>\n",
       "          (bias): <tf.Variable 'sequential_block_4/sequential_block_3/private__dense_5/dense_5/bias:0' shape=(256,) dtype=float32, numpy=\n",
       "          array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                 0.], dtype=float32)>\n",
       "          (_feature_shapes): Dict(\n",
       "            (f_47_list_seq): TensorShape([1024, None])\n",
       "            (f_68_list_seq): TensorShape([1024, None])\n",
       "            (item_id_list_seq): TensorShape([1024, None])\n",
       "            (item_id_last): TensorShape([1024, 1])\n",
       "          )\n",
       "          (_feature_dtypes): Dict(\n",
       "            (f_47_list_seq): tf.int32\n",
       "            (f_68_list_seq): tf.int32\n",
       "            (item_id_list_seq): tf.int32\n",
       "            (item_id_last): tf.int32\n",
       "          )\n",
       "        )\n",
       "        (_feature_shapes): Dict(\n",
       "          (f_47_list_seq): TensorShape([1024, None])\n",
       "          (f_68_list_seq): TensorShape([1024, None])\n",
       "          (item_id_list_seq): TensorShape([1024, None])\n",
       "          (item_id_last): TensorShape([1024, 1])\n",
       "        )\n",
       "        (_feature_dtypes): Dict(\n",
       "          (f_47_list_seq): tf.int32\n",
       "          (f_68_list_seq): tf.int32\n",
       "          (item_id_list_seq): tf.int32\n",
       "          (item_id_last): tf.int32\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (_feature_shapes): Dict(\n",
       "      (f_47_list_seq): TensorShape([1024, None])\n",
       "      (f_68_list_seq): TensorShape([1024, None])\n",
       "      (item_id_list_seq): TensorShape([1024, None])\n",
       "      (item_id_last): TensorShape([1024, 1])\n",
       "    )\n",
       "    (_feature_dtypes): Dict(\n",
       "      (f_47_list_seq): tf.int32\n",
       "      (f_68_list_seq): tf.int32\n",
       "      (item_id_list_seq): tf.int32\n",
       "      (item_id_last): tf.int32\n",
       "    )\n",
       "  ), CategoricalOutput(\n",
       "    (to_call): EmbeddingTablePrediction(\n",
       "      (table): EmbeddingTable(\n",
       "        (features): Dict(\n",
       "          (item_id_list_seq): ColumnSchema(name='item_id_list_seq', tags={<Tags.ITEM: 'item'>, <Tags.ITEM_ID: 'item_id'>, <Tags.ID: 'id'>, <Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.LIST: 'list'>}, properties={'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 1.0, 'cat_path': './/categories/unique.item_id_purchase_id.parquet', 'embedding_sizes': {'cardinality': 23272.0, 'dimension': 446.0}, 'domain': {'min': 0, 'max': 23271, 'name': 'item_id_purchase_id'}}, dtype=dtype('int32'), is_list=True, is_ragged=True)\n",
       "        )\n",
       "        (table): Embedding(\n",
       "          (embeddings): <tf.Variable 'parallel_block/embeddings:0' shape=(23272, 256) dtype=float32, numpy=\n",
       "          array([[ 0.04811797, -0.00899345, -0.04444077, ..., -0.00330721,\n",
       "                   0.00299748,  0.03726442],\n",
       "                 [-0.01568881,  0.04338299, -0.04912223, ..., -0.03949897,\n",
       "                   0.02617436, -0.0135996 ],\n",
       "                 [-0.00171857,  0.0334307 , -0.00416709, ..., -0.03254211,\n",
       "                  -0.00979654, -0.01041394],\n",
       "                 ...,\n",
       "                 [ 0.04733298, -0.01479564, -0.04049425, ..., -0.04113694,\n",
       "                  -0.03093792,  0.00712723],\n",
       "                 [-0.04123167,  0.00964994,  0.01854951, ..., -0.01597718,\n",
       "                  -0.04847977,  0.00324798],\n",
       "                 [-0.04678471,  0.02663937,  0.0143769 , ...,  0.01008096,\n",
       "                  -0.00546101,  0.03424305]], dtype=float32)>\n",
       "          (_feature_shapes): Dict(\n",
       "            (f_47_list_seq): TensorShape([1024, None])\n",
       "            (f_68_list_seq): TensorShape([1024, None])\n",
       "            (item_id_list_seq): TensorShape([1024, None])\n",
       "            (item_id_last): TensorShape([1024, 1])\n",
       "          )\n",
       "          (_feature_dtypes): Dict(\n",
       "            (f_47_list_seq): tf.int32\n",
       "            (f_68_list_seq): tf.int32\n",
       "            (item_id_list_seq): tf.int32\n",
       "            (item_id_last): tf.int32\n",
       "          )\n",
       "        )\n",
       "        (_feature_shapes): Dict(\n",
       "          (f_47_list_seq): TensorShape([1024, None])\n",
       "          (f_68_list_seq): TensorShape([1024, None])\n",
       "          (item_id_list_seq): TensorShape([1024, None])\n",
       "          (item_id_last): TensorShape([1024, 1])\n",
       "        )\n",
       "        (_feature_dtypes): Dict(\n",
       "          (f_47_list_seq): tf.int32\n",
       "          (f_68_list_seq): tf.int32\n",
       "          (item_id_list_seq): tf.int32\n",
       "          (item_id_last): tf.int32\n",
       "        )\n",
       "      )\n",
       "      (_feature_shapes): Dict(\n",
       "        (f_47_list_seq): TensorShape([1024, None])\n",
       "        (f_68_list_seq): TensorShape([1024, None])\n",
       "        (item_id_list_seq): TensorShape([1024, None])\n",
       "        (item_id_last): TensorShape([1024, 1])\n",
       "      )\n",
       "      (_feature_dtypes): Dict(\n",
       "        (f_47_list_seq): tf.int32\n",
       "        (f_68_list_seq): tf.int32\n",
       "        (item_id_list_seq): tf.int32\n",
       "        (item_id_last): tf.int32\n",
       "      )\n",
       "      (output_layer_bias): <tf.Variable 'model/output_layer_bias:0' shape=(23272,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>\n",
       "      (bias): <tf.Variable 'model/output_layer_bias:0' shape=(23272,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>\n",
       "    )\n",
       "    (logits_scaler): LogitsTemperatureScaler(\n",
       "      (_feature_shapes): Dict(\n",
       "        (f_47_list_seq): TensorShape([1024, None])\n",
       "        (f_68_list_seq): TensorShape([1024, None])\n",
       "        (item_id_list_seq): TensorShape([1024, None])\n",
       "        (item_id_last): TensorShape([1024, 1])\n",
       "      )\n",
       "      (_feature_dtypes): Dict(\n",
       "        (f_47_list_seq): tf.int32\n",
       "        (f_68_list_seq): tf.int32\n",
       "        (item_id_list_seq): tf.int32\n",
       "        (item_id_last): tf.int32\n",
       "      )\n",
       "    )\n",
       "    (_feature_shapes): Dict(\n",
       "      (f_47_list_seq): TensorShape([1024, None])\n",
       "      (f_68_list_seq): TensorShape([1024, None])\n",
       "      (item_id_list_seq): TensorShape([1024, None])\n",
       "      (item_id_last): TensorShape([1024, 1])\n",
       "    )\n",
       "    (_feature_dtypes): Dict(\n",
       "      (f_47_list_seq): tf.int32\n",
       "      (f_68_list_seq): tf.int32\n",
       "      (item_id_list_seq): tf.int32\n",
       "      (item_id_last): tf.int32\n",
       "    )\n",
       "  )))\n",
       "  (context): ModelContext(\n",
       "    (_feature_shapes): Dict(\n",
       "      (f_47_list_seq): TensorShape([1024, None])\n",
       "      (f_68_list_seq): TensorShape([1024, None])\n",
       "      (item_id_list_seq): TensorShape([1024, None])\n",
       "      (item_id_last): TensorShape([1024, 1])\n",
       "    )\n",
       "    (_feature_dtypes): Dict(\n",
       "      (f_47_list_seq): tf.int32\n",
       "      (f_68_list_seq): tf.int32\n",
       "      (item_id_list_seq): tf.int32\n",
       "      (item_id_last): tf.int32\n",
       "    )\n",
       "  )\n",
       "  (process_list): ProcessList()\n",
       "  (_should_compute_train_metrics_for_batch): <tf.Variable 'should_compute_train_metrics_for_batch:0' shape=() dtype=bool, numpy=False>\n",
       "  (output_names): List(\n",
       "    (0): 'purchase_id_first/categorical_output'\n",
       "  )\n",
       "  (optimizer): Adam()\n",
       "  (loss): Dict(\n",
       "    (purchase_id_first/categorical_output): CategoricalCrossentropy()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transformer.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad2bf6e-5bfe-4831-bc07-0b1f460b313f",
   "metadata": {},
   "source": [
    "We evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847c456-c401-4284-94fe-eb888b2f0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_transformer = model_transformer.evaluate(val_loader, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49643076-dc75-41f0-bae8-678b2fe68fbd",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1c6ea-4576-4bd4-86e7-f4972ebce380",
   "metadata": {},
   "source": [
    "In this example, we focused on concepts which are relevant for a broad range of recommender system use cases- session-based recommendation task. If you compare the MRR to the ACM RecSys'22 competition, you will notice, that the MRR can be much higher. Following are additional techniques that can be applied to improve the MRR:\n",
    "- Data Augmentations - in the RecSys'22 challenge, we used a lot of different techniques to increase the training dataset. The techniques are specific to the dataset and we did not include it in the example:\n",
    "- Additional item features - we focused on only a few item features\n",
    "- Stacking - we stacked 17 models with a two-step approach\n",
    "- Ensemble - we ensembled 3 different stacked models\n",
    "- Hyperparameter Search - we ran multiple HPO jobs to find the best hyperparameters\n",
    "\n",
    "In addition, the MRR on the June month (test data) was in general higher than in May (validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab403bb43341787581f43b51cdd291d61392c89ddb0f92179de653921d4e05db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
