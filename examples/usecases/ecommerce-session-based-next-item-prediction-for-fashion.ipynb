{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62dc451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a58d3",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Session-Based Next Item Prediction for Fashion E-Commerce\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container. \n",
    "\n",
    "## Overview\n",
    "\n",
    "NVIDIA-Merlin team participated in [Recsys2022 challenge](http://www.recsyschallenge.com/2022/index.html) and secured 3rd position. This notebook contains the various techniques used in the solution.\n",
    "\n",
    "### Learning Objective\n",
    "\n",
    "In this notebook, we will apply important concepts that improve recommender systems. We leveraged them for our RecSys2022 participations:\n",
    "- MultiClass next item prediction head with Merlin Models\n",
    "- Sequential input features representing user sessions\n",
    "- Label Smoothing \n",
    "- Temperature Scaling\n",
    "- Weight Tying\n",
    "\n",
    "### Brief Description of the Concepts\n",
    "\n",
    "##### Label smoothing\n",
    "When the probabilities predicted by a Classification model are higher than its accuracy we say the model is overconfident. It can be prevented by using Label smoothing. This technique basically, transforms One-hot encoded labels into smoothed labels. \n",
    "$$  \\begin{array}{l}\n",
    "y_{l} \\ =\\ ( 1\\ -\\ \\alpha \\ ) \\ *\\ y_{o} \\ +\\ ( \\alpha \\ /\\ L)\\\\\n",
    "\\alpha :\\ Label\\ smoothing\\\\\n",
    "L:\\ Total\\ number\\ of\\ label\\ classes\\\\\n",
    "y_{o} :\\ One-hot\\ encoded\\ label\\ vector\n",
    "\\end{array}\n",
    "$$\n",
    "When α is 0, we have the original one-hot encoded labels, and as α increases, we move towards smoothed labels. Read [this](https://arxiv.org/abs/1906.02629) paper to learn more about it.\n",
    "\n",
    "\n",
    "##### Temperature Scaling\n",
    "Similar to Label Smoothing, Temperature Scaling is done to reduce the overconfidence of a model. In this, we divide the logits (inputs to the softmax function) by a scalar parameter (T) . For more information on Temperature Scaling read [this](https://arxiv.org/pdf/1706.04599.pdf) paper.\n",
    "$$ softmax\\ =\\ \\frac{e\\ ^{( z_{i} \\ /\\ \\ T)}}{\\sum _{j} \\ e^{( z_{j} \\ /\\ T)} \\ } $$\n",
    "\n",
    "\n",
    "##### Weight Tying\n",
    "In this technique, we share the Embedding layer's weights which is used to convert the input to embeddings, as the softmax weights,  to convert hidden layer output to softmax layer output. This drastically reduces the number of parameters and allows the model to train better. For more information read [this](https://arxiv.org/pdf/1608.05859v3.pdf) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e105ffec",
   "metadata": {},
   "source": [
    "## Downloading and preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f77b7a",
   "metadata": {},
   "source": [
    "We will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dac596a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 14:07:38.392579: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-19 14:07:40.734255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16255 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:0a:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cupy\n",
    "import cudf\n",
    "import dask_cudf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import nvtabular as nvt\n",
    "from merlin.dag import ColumnSelector\n",
    "from merlin.io import Dataset\n",
    "from merlin.schema import Schema, Tags\n",
    "from nvtabular.ops import (\n",
    "    AddMetadata,\n",
    ")\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from merlin.io import Dataset\n",
    "from tensorflow.keras import regularizers\n",
    "from merlin.models.tf.dataset import BatchedDataset\n",
    "from merlin.models.tf.utils.tf_utils import extract_topk\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.models.tf import InputBlock\n",
    "from merlin.models.tf.models.base import Model\n",
    "from merlin.models.tf.core.aggregation import SequenceAggregation, SequenceAggregator\n",
    "from merlin.models.tf.core.transformations import (\n",
    "    ItemsPredictionWeightTying,\n",
    "    L2Norm,\n",
    "    LogitsTemperatureScaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf858cf2",
   "metadata": {},
   "source": [
    "###  Dressipi\n",
    "[Dressipi](http://www.recsyschallenge.com/2022/dataset.html) hosted the [Recsys2022 challenge](http://www.recsyschallenge.com/2022/index.html) and provided an anonymized dataset. It contains 1.1 M online retail sessions that resulted in a purchase. It provides details about items that were viewed in a session, the item purchased at the end of the session and numerous features of those items. The item features are categorical IDs and are not interpretable.\n",
    "\n",
    "The task of this competition was, given a sequence of items predict which item will be purchased at the end of a session.\n",
    "\n",
    "<img src=\"http://www.recsyschallenge.com/2022/images/session_purchase_data.jpeg\" alt=\"dressipi_dataset\" style=\"width: 400px; float: center;\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e8bb5",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We provide a function `get_dressipi2022` which preprocess the dataset. Currently, we can't download this dataset automatically so this needs to be downloaded manually. To use this function, prepare the data by following these 3 steps:\n",
    "1. Sign up and download the data from [dressipi-recsys2022.com](https://www.dressipi-recsys2022.com/).\n",
    "2. Unzip the raw data to a directory.\n",
    "3. Define `DATA_FOLDER` to the directory\n",
    "\n",
    "In case you want to use this dataset to run our examples, you can also opt for synthetic data. Synthetic data can be generated by running::\n",
    "\n",
    "```python\n",
    "    from merlin.datasets.synthetic import generate_data\n",
    "    train, valid = generate_data(\"dressipi2022-preprocessed\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2df4579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.datasets.ecommerce import get_dressipi2022\n",
    "\n",
    "DATA_FOLDER = '../../../../../dressipi_recsys2022'\n",
    "train, valid = get_dressipi2022(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596826c7",
   "metadata": {},
   "source": [
    "The dataset contains:\n",
    "- `session_id`, id of a session, in which a user viewed and purchased an item. \n",
    "- `item_id` which was viewed at a given `timestamp` in a session\n",
    "- `purchase_id` which is the id of item bought at the end of the session. \n",
    "In addition to `timestamp`, we have `day` and `date` features for representing the chronological order in which items were viewed.\n",
    "\n",
    "The items in the Dresspi dataset had a many features out of which we took 22 most important features, namely \n",
    "`f_3 ,f_4 ,f_5 ,f_7 ,f_17 ,f_24 ,f_30 ,f_45 ,f_46 ,f_47 ,f_50 ,f_53 ,f_55 ,f_56 ,f_58 ,f_61 ,f_63 ,f_65 ,f_68 ,f_69 ,f_72 ,f_73`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbb6f16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['session_id', 'item_id', 'date', 'f_3', 'f_5', 'f_7', 'f_17', 'f_24',\n",
       "       'f_45', 'f_47', 'f_50', 'f_55', 'f_56', 'f_58', 'f_61', 'f_63', 'f_65',\n",
       "       'f_68', 'f_69', 'f_72', 'f_73', 'timestamp', 'day', 'purchase_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.to_ddf().head().columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e5f42",
   "metadata": {},
   "source": [
    "## Feature Engineering with NVTabular\n",
    "\n",
    "We use NVTabular for Feature Engineering. If you want to learn more about NVTabular, we recommend the [examples in the NVTabular GitHub Repository](https://github.com/NVIDIA-Merlin/NVTabular/tree/main/examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50949a5",
   "metadata": {},
   "source": [
    "### Categorify\n",
    "\n",
    "We want to use embedding layers for our categorical features. First, we need to Categorify them, that they are continuous integers. \n",
    "\n",
    "The features `item_id` and `purchase_id` belongs to the same category. If `item_id` is 8432 and `purchase_id` is 8432, they are the same item. When we want to apply Categorify, we want to keep the connection. We can achieve this by encoding them jointly providing them as a list in the list `[['item_id', 'purchase_id']]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768169f",
   "metadata": {},
   "source": [
    "We will use only 5 of the categorical item features in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68cbfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 151 µs, total: 151 µs\n",
      "Wall time: 172 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "item_features_names = ['f_' + str(col) for col in [47, 68]]\n",
    "cat_features = ['session_id', ['item_id', 'purchase_id']] + item_features_names >> nvt.ops.Categorify()\n",
    "\n",
    "features = ['timestamp','date'] + cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74264e1a",
   "metadata": {},
   "source": [
    "### GroupBy the data by sessions.\n",
    "\n",
    "Currently, every row is a viewed item in the dataset. Our goal is to predict the item purchased after the last view in a session. Therefore, we groupby the dataset by `session_id` to have one row for each prediction.\n",
    "\n",
    "Each row will have a sequence of encoded items ids with which a user interacted. The last item of a session has special importance as it is closer to the user's intention. We will keep the viewed item as a separate feature.\n",
    "\n",
    "The NVTabular Op `GroupBy` enables the transformation. \n",
    "\n",
    "First, we define how the different colums should be aggregates:\n",
    "- Keep the first occurance of `date`\n",
    "- Keep the last item and concatenate all items to a list (results are 2 features)\n",
    "- Keep the first occurance of `purchase_id` (purchase_id should be the same for all rows of one session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dfdc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_aggregate = {}\n",
    "to_aggregate['date'] = [\"first\"]\n",
    "to_aggregate['item_id'] = [\"last\", \"list\"]\n",
    "to_aggregate['purchase_id'] = [\"first\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a1fe03",
   "metadata": {},
   "source": [
    "In addition, we concatenate each item features to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00aded7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in item_features_names: \n",
    "    to_aggregate[name] = ['list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b623d6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': ['first'],\n",
       " 'item_id': ['last', 'list'],\n",
       " 'purchase_id': ['first'],\n",
       " 'f_47': ['list'],\n",
       " 'f_68': ['list']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f08313",
   "metadata": {},
   "source": [
    "We want to sort the dataframe by `date` and groupby the columns by `session_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f03fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_features = features >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    sort_cols=[\"date\"],\n",
    "    aggs= to_aggregate,\n",
    "    name_sep=\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55463f3",
   "metadata": {},
   "source": [
    "Merlin Models can infer the neural network architecture from the dataset schema. We will Tag the columns accordingly to the dataset type. If you want to learn more, we recommend our [Dataset Schema Example](https://github.com/NVIDIA-Merlin/models/blob/main/examples/02-Merlin-Models-and-NVTabular-integration.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ad05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_last = (\n",
    "    groupby_features['item_id_last'] >> \n",
    "    AddMetadata(tags=[Tags.ITEM, Tags.ITEM_ID])\n",
    ")\n",
    "item_list = (\n",
    "    groupby_features['item_id_list'] >> \n",
    "    AddMetadata(\n",
    "        tags=[Tags.ITEM, Tags.ITEM_ID, Tags.LIST, Tags.SEQUENCE]\n",
    "    )\n",
    ")\n",
    "feature_list = (\n",
    "    groupby_features[[name+'_list' for name in item_features_names]] >> \n",
    "    AddMetadata(\n",
    "        tags=[Tags.SEQUENCE, Tags.ITEM, Tags.LIST]\n",
    "    )\n",
    ")\n",
    "target_feature = (\n",
    "    groupby_features['purchase_id_first'] >> \n",
    "    AddMetadata(tags=[Tags.TARGET])\n",
    ")\n",
    "other_features = groupby_features['session_id', 'date_first']\n",
    "\n",
    "groupby_features = item_last + item_list + feature_list + other_features + target_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99ea8c",
   "metadata": {},
   "source": [
    "### Truncate and Padding for a Maximum Sequence Length\n",
    "\n",
    "We want to truncate and pad the sequential features. We define the columns, which are sequential features and the non-sequential ones. We truncate the sequence by keeping the last 3 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f77f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = [name+'_list' for name in item_features_names] + ['item_id_list']\n",
    "nonlist_features = ['session_id', 'date_first', 'item_id_last', 'purchase_id_first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2ad8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSIONS_MAX_LENGTH = 3\n",
    "truncated_features = groupby_features[list_features] >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH, pad=True) >> nvt.ops.Rename(postfix = '_seq')\n",
    "\n",
    "final_features = groupby_features[nonlist_features] + truncated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21eadf5",
   "metadata": {},
   "source": [
    "We initalize our NVTabular workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "550c301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow(final_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700c6f8",
   "metadata": {},
   "source": [
    "We call fit and transform similar to the scikit learn API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5821feeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = nvt.Dataset(cudf.concat([train.to_ddf().compute(), valid.to_ddf().compute()]))\n",
    "# fit data\n",
    "workflow.fit(dataset)\n",
    "\n",
    "# transform data\n",
    "workflow.transform(train).to_parquet(os.path.join(DATA_FOLDER, \"train/\"), output_files=10)\n",
    "workflow.transform(valid).to_parquet(os.path.join(DATA_FOLDER, \"train/\"), output_files=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae62f8",
   "metadata": {},
   "source": [
    "### Sort the Training Dataset by Time\n",
    "\n",
    "Now let's save the data for training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bbc8597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 ms, sys: 3.48 ms, total: 14 ms\n",
      "Wall time: 12.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_ds = Dataset(train_2.to_ddf().sort_values('date_first'), schema=train_2.schema)\n",
    "valid_ds = Dataset(valid_2.to_ddf().sort_values('date_first'), schema=valid_2.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc1cfc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.to_parquet(os.path.join(DATA_FOLDER, \"train/\"), output_files=10)\n",
    "valid_ds.to_parquet(os.path.join(DATA_FOLDER, \"valid/\"), output_files=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4eac01",
   "metadata": {},
   "source": [
    "## Training - MLP\n",
    "\n",
    "A Sequential- Multi-Layer Perceptron model with average of the sequence as final representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54bcf38",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce9b290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.2 #3e-1\n",
    "CLIPNORM = True\n",
    "DROPOUT= 0.2 \n",
    "LABEL_SMOOTHING = 0.2\n",
    "TEMPERATURE_SCALING = 2\n",
    "OPTIMIZER_NAME = 'adam'\n",
    "LOSS='CategoricalCrossentropy'\n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75010de1",
   "metadata": {},
   "source": [
    "#### Load the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcde20d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = Dataset(os.path.join(DATA_FOLDER, 'train/*.parquet'), shuffle=False,)\n",
    "valid = Dataset(os.path.join(DATA_FOLDER, 'valid/*.parquet'), shuffle=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26ca38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import merlin.models.tf.dataset as tf_dataloader\n",
    "\n",
    "train_dl = tf_dataloader.BatchedDataset(\n",
    "    train,\n",
    "    batch_size = 512,\n",
    "    shuffle=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca6283",
   "metadata": {},
   "source": [
    "#### Schema \n",
    "Let’s visualize the schema. From this we will select features for training the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67014089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>session_id</td>\n",
       "      <td>(Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.session_id.parquet</td>\n",
       "      <td>1000001.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000001.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date_first</td>\n",
       "      <td>()</td>\n",
       "      <td>float64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>item_id_last</td>\n",
       "      <td>(Tags.ITEM, Tags.ITEM_ID, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.item_id_purchase_id.parquet</td>\n",
       "      <td>23619.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23619.0</td>\n",
       "      <td>item_id_purchase_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>purchase_id_first</td>\n",
       "      <td>(Tags.TARGET, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.item_id_purchase_id.parquet</td>\n",
       "      <td>23619.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23619.0</td>\n",
       "      <td>item_id_purchase_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f_47_list_seq</td>\n",
       "      <td>(Tags.ITEM, Tags.SEQUENCE, Tags.CATEGORICAL, T...</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.f_47.parquet</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>f_47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f_68_list_seq</td>\n",
       "      <td>(Tags.ITEM, Tags.SEQUENCE, Tags.CATEGORICAL, T...</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.f_68.parquet</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>f_68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>item_id_list_seq</td>\n",
       "      <td>(Tags.ITEM, Tags.SEQUENCE, Tags.ITEM_ID, Tags....</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.item_id_purchase_id.parquet</td>\n",
       "      <td>23619.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23619.0</td>\n",
       "      <td>item_id_purchase_id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'session_id', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.session_id.parquet', 'embedding_sizes': {'cardinality': 1000001.0, 'dimension': 512.0}, 'domain': {'min': 0, 'max': 1000001}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'date_first', 'tags': set(), 'properties': {}, 'dtype': dtype('float64'), 'is_list': False, 'is_ragged': False}, {'name': 'item_id_last', 'tags': {<Tags.ITEM: 'item'>, <Tags.ITEM_ID: 'item_id'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.item_id_purchase_id.parquet', 'embedding_sizes': {'cardinality': 23619.0, 'dimension': 450.0}, 'domain': {'min': 0, 'max': 23619, 'name': 'item_id_purchase_id'}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'purchase_id_first', 'tags': {<Tags.TARGET: 'target'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.item_id_purchase_id.parquet', 'embedding_sizes': {'cardinality': 23619.0, 'dimension': 450.0}, 'domain': {'min': 0, 'max': 23619, 'name': 'item_id_purchase_id'}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'f_47_list_seq', 'tags': {<Tags.ITEM: 'item'>, <Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.LIST: 'list'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.f_47.parquet', 'embedding_sizes': {'cardinality': 18.0, 'dimension': 16.0}, 'domain': {'min': 0, 'max': 18, 'name': 'f_47'}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': False}, {'name': 'f_68_list_seq', 'tags': {<Tags.ITEM: 'item'>, <Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.LIST: 'list'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.f_68.parquet', 'embedding_sizes': {'cardinality': 50.0, 'dimension': 16.0}, 'domain': {'min': 0, 'max': 50, 'name': 'f_68'}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': False}, {'name': 'item_id_list_seq', 'tags': {<Tags.ITEM: 'item'>, <Tags.SEQUENCE: 'sequence'>, <Tags.ITEM_ID: 'item_id'>, <Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.item_id_purchase_id.parquet', 'embedding_sizes': {'cardinality': 23619.0, 'dimension': 450.0}, 'domain': {'min': 0, 'max': 23619, 'name': 'item_id_purchase_id'}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': False}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27cbf7f",
   "metadata": {},
   "source": [
    "In this model, we will only use two features `item_id_last` and `item_id_list_seq` for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "362ea972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>item_id_list_seq</td>\n",
       "      <td>(Tags.ITEM, Tags.SEQUENCE, Tags.ITEM_ID, Tags....</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.item_id_purchase_id.parquet</td>\n",
       "      <td>23619.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23619</td>\n",
       "      <td>item_id_purchase_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>item_id_last</td>\n",
       "      <td>(Tags.ITEM, Tags.ITEM_ID, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.item_id_purchase_id.parquet</td>\n",
       "      <td>23619.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23619</td>\n",
       "      <td>item_id_purchase_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>purchase_id_first</td>\n",
       "      <td>(Tags.TARGET, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.item_id_purchase_id.parquet</td>\n",
       "      <td>23619.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23619</td>\n",
       "      <td>item_id_purchase_id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'item_id_list_seq', 'tags': {<Tags.ITEM: 'item'>, <Tags.SEQUENCE: 'sequence'>, <Tags.ITEM_ID: 'item_id'>, <Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.item_id_purchase_id.parquet', 'embedding_sizes': {'cardinality': 23619.0, 'dimension': 450.0}, 'domain': {'min': 0, 'max': 23619, 'name': 'item_id_purchase_id'}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': False}, {'name': 'item_id_last', 'tags': {<Tags.ITEM: 'item'>, <Tags.ITEM_ID: 'item_id'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.item_id_purchase_id.parquet', 'embedding_sizes': {'cardinality': 23619.0, 'dimension': 450.0}, 'domain': {'min': 0, 'max': 23619, 'name': 'item_id_purchase_id'}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'purchase_id_first', 'tags': {<Tags.TARGET: 'target'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.item_id_purchase_id.parquet', 'embedding_sizes': {'cardinality': 23619.0, 'dimension': 450.0}, 'domain': {'min': 0, 'max': 23619, 'name': 'item_id_purchase_id'}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_model = train.schema.select_by_name(['item_id_list_seq', 'item_id_last', 'purchase_id_first'])\n",
    "schema_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c779f3",
   "metadata": {},
   "source": [
    "Let's visualise the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3feee3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>date_first</th>\n",
       "      <th>item_id_last</th>\n",
       "      <th>purchase_id_first</th>\n",
       "      <th>f_47_list_seq</th>\n",
       "      <th>f_68_list_seq</th>\n",
       "      <th>item_id_list_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144306</td>\n",
       "      <td>2020-01-01 00:00:01.359</td>\n",
       "      <td>8558</td>\n",
       "      <td>14343</td>\n",
       "      <td>[2, 5, 13]</td>\n",
       "      <td>[1, 8, 7]</td>\n",
       "      <td>[670, 4633, 8558]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102504</td>\n",
       "      <td>2020-01-01 00:00:21.440</td>\n",
       "      <td>12231</td>\n",
       "      <td>18295</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[8, 45, 6]</td>\n",
       "      <td>[15367, 8469, 12231]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>993759</td>\n",
       "      <td>2020-01-01 00:00:48.505</td>\n",
       "      <td>5726</td>\n",
       "      <td>14805</td>\n",
       "      <td>[13, 0, 0]</td>\n",
       "      <td>[9, 0, 0]</td>\n",
       "      <td>[5726, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9972</td>\n",
       "      <td>2020-01-01 00:06:37.801</td>\n",
       "      <td>10329</td>\n",
       "      <td>12877</td>\n",
       "      <td>[7, 8, 7]</td>\n",
       "      <td>[14, 3, 3]</td>\n",
       "      <td>[19797, 16836, 10329]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>357643</td>\n",
       "      <td>2020-01-01 00:08:19.297</td>\n",
       "      <td>15432</td>\n",
       "      <td>13374</td>\n",
       "      <td>[1, 5, 1]</td>\n",
       "      <td>[5, 1, 8]</td>\n",
       "      <td>[938, 15840, 15432]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id              date_first  item_id_last  purchase_id_first  \\\n",
       "0      144306 2020-01-01 00:00:01.359          8558              14343   \n",
       "1      102504 2020-01-01 00:00:21.440         12231              18295   \n",
       "2      993759 2020-01-01 00:00:48.505          5726              14805   \n",
       "3        9972 2020-01-01 00:06:37.801         10329              12877   \n",
       "4      357643 2020-01-01 00:08:19.297         15432              13374   \n",
       "\n",
       "  f_47_list_seq f_68_list_seq       item_id_list_seq  \n",
       "0    [2, 5, 13]     [1, 8, 7]      [670, 4633, 8558]  \n",
       "1     [1, 1, 1]    [8, 45, 6]   [15367, 8469, 12231]  \n",
       "2    [13, 0, 0]     [9, 0, 0]           [5726, 0, 0]  \n",
       "3     [7, 8, 7]    [14, 3, 3]  [19797, 16836, 10329]  \n",
       "4     [1, 5, 1]     [5, 1, 8]    [938, 15840, 15432]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949903cf",
   "metadata": {},
   "source": [
    "#### Build the Model\n",
    "Now we will create an InputBlock which takes sequential features, concatenate them and return the sequence of interaction embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f62b2c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_options = mm.EmbeddingOptions(\n",
    "        embedding_dims = None,\n",
    "        embedding_dim_default=256,\n",
    "        infer_embedding_sizes=False,\n",
    ")\n",
    "\n",
    "input_block = InputBlock(\n",
    "    schema_model.select_by_name(['item_id_list_seq', 'item_id_last']), \n",
    "    aggregation='concat',\n",
    "    embedding_options = emb_options\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938fcce",
   "metadata": {},
   "source": [
    "The Multi-Classiffication Prediction head will have\n",
    "- Layer Normalization\n",
    "- Weight Tying\n",
    "- Labels as One-hot encoded vectors, used for label smoothing \n",
    "- Temperature Scaling to reduce the overconfidence of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17bc4a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_call = L2Norm().connect(\n",
    "    ItemsPredictionWeightTying(schema_model), \n",
    "    mm.LabelToOneHot(),\n",
    "    LogitsTemperatureScaler(temperature=TEMPERATURE_SCALING)\n",
    ")\n",
    "\n",
    "prediction_task = mm.MultiClassClassificationTask(\n",
    "    target_name=\"purchase_id_first\",\n",
    "    pre=prediction_call,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87683ce7",
   "metadata": {},
   "source": [
    "Now, we will build a model with a MLPBlock, to get the sequence of hidden representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aabb868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp = mm.Model.from_block(\n",
    "    mm.MLPBlock([128,256], no_activation_last_layer=True, dropout=0.2),\n",
    "    schema_model, \n",
    "    input_block=input_block,\n",
    "    prediction_tasks=prediction_task\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10588a6",
   "metadata": {},
   "source": [
    "#### Define the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b91c7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "model_mlp.compile(\n",
    "    optimizer=optimizer,\n",
    "    run_eagerly=True,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=LABEL_SMOOTHING),\n",
    "    metrics=mm.TopKMetricsAggregator.default_metrics(top_ks=[100])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a9d3c",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023e2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1799/1799 [==============================] - 192s 103ms/step - loss: 7.8168 - recall_at_100: 0.3760 - mrr_at_100: 0.0790 - ndcg_at_100: 0.1350 - map_at_100: 0.0790 - precision_at_100: 0.0038 - regularization_loss: 0.0000e+00 - val_loss: 7.8541 - val_recall_at_100: 0.4859 - val_mrr_at_100: 0.1150 - val_ndcg_at_100: 0.1866 - val_map_at_100: 0.1150 - val_precision_at_100: 0.0049 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      " 167/1799 [=>............................] - ETA: 2:39 - loss: 8.2953 - recall_at_100: 0.2852 - mrr_at_100: 0.0681 - ndcg_at_100: 0.1095 - map_at_100: 0.0681 - precision_at_100: 0.0029 - regularization_loss: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model_mlp.fit(\n",
    "    train_dl,\n",
    "    validation_data=valid,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    schema=schema_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e948bf",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7e319b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 9s 64ms/step - loss: 8.0177 - recall_at_100: 0.5105 - mrr_at_100: 0.1510 - ndcg_at_100: 0.2210 - map_at_100: 0.1510 - precision_at_100: 0.0051 - regularization_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 8.017659187316895,\n",
       " 'recall_at_100': 0.4807376563549042,\n",
       " 'mrr_at_100': 0.1448851227760315,\n",
       " 'ndcg_at_100': 0.21048013865947723,\n",
       " 'map_at_100': 0.1448851227760315,\n",
       " 'precision_at_100': 0.004807377699762583,\n",
       " 'regularization_loss': 0.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.evaluate(valid_ds, batch_size=1024, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87f381ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(pred, df_agg, batch_size=1024, n_topk=100):\n",
    "    print('Mask Predictions')\n",
    "    print('Generate Top100 Recommendations')\n",
    "    out_pred = []\n",
    "    out_score = []\n",
    "    for i in range(0, pred.shape[0]//batch_size+1):\n",
    "        batch_start = (i)*batch_size\n",
    "        batch_end = min((i+1)*batch_size, pred.shape[0])\n",
    "        pred_tmp = pred[batch_start:batch_end]\n",
    "        cp_pred = cupy.asarray(pred_tmp)\n",
    "        pred_idx = cupy.argsort(-cp_pred)\n",
    "        pred_idx = cupy.asnumpy(pred_idx)\n",
    "        for j in range(pred_idx.shape[0]):\n",
    "            topk = []\n",
    "            score = []\n",
    "            for k in range(n_topk):\n",
    "                idx = pred_idx[j][k]\n",
    "                topk.append(idx)\n",
    "                score.append(pred_tmp[j][idx])\n",
    "            out_pred.append(topk)\n",
    "            out_score.append(score)\n",
    "    \n",
    "    print('Transform Top100 Recommendations')\n",
    "    metadata = df_agg[['session_id', 'purchase_id_first']].to_pandas().values.tolist()\n",
    "    out = []\n",
    "    for i, ex in enumerate(metadata):\n",
    "        session_id = ex[0]\n",
    "        purchase = ex[1]\n",
    "        for k in range(n_topk):\n",
    "            out.append([session_id, purchase, out_pred[i][k], out_score[i][k]])\n",
    "\n",
    "    df_rec = cudf.DataFrame(out)\n",
    "    df_rec.columns = ['session_id', 'purchased', 'rec', 'score']\n",
    "    return(df_rec)\n",
    "\n",
    "def evaluate(df, add_folds=False):\n",
    "    print('Model evaluation')\n",
    "    df = df.drop_duplicates(['session_id', 'rec'])\n",
    "    df = df.sort_values(['session_id', 'score'], ascending=False)\n",
    "    df['dummy'] = 1\n",
    "    df['rank'] = df[['session_id', 'dummy']].groupby('session_id').cumsum()\n",
    "    df = df[df['rank']<=100]\n",
    "    df.drop('dummy', inplace=True, axis=1)\n",
    "    df['mrr'] = 1/df['rank']\n",
    "    df.loc[df['purchased']!=df['rec'], 'mrr'] = 0\n",
    "    out = {}\n",
    "    mrr = df[df['purchased']==df['rec']]['mrr'].sum()/df['session_id'].drop_duplicates().shape[0]\n",
    "    out['total'] = mrr\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "953dab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 4s 42ms/step\n",
      "Mask Predictions\n",
      "Generate Top100 Recommendations\n",
      "Transform Top100 Recommendations\n",
      "Model evaluation\n",
      "MRR:  0.14488511239789104\n",
      "CPU times: user 51.9 s, sys: 12.1 s, total: 1min 3s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = model_mlp.predict(valid, batch_size=1024, verbose=1)\n",
    "ddf = valid.to_ddf()\n",
    "ddf = ddf[['session_id', 'purchase_id_first']].compute()\n",
    "df_rec = generate_recommendations(predictions, ddf)\n",
    "val_mrr = evaluate(df_rec)['total']\n",
    "print('MRR: ',val_mrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06099b00",
   "metadata": {},
   "source": [
    "## Training Bi-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839653fd",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b44af163",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1024 #512\n",
    "LEARNING_RATE = 3e-1\n",
    "CLIPNORM = True\n",
    "DROPOUT= 0.2 #0.01\n",
    "LABEL_SMOOTHING = 0.2\n",
    "TEMPERATURE_SCALING = 2\n",
    "OPTIMIZER_NAME = 'adam'\n",
    "LOSS='CategoricalCrossentropy'\n",
    "\n",
    "BI_LSTM_HIDDEN_DIM = 64\n",
    "tf.keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b68fb",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Let's create a `BiLSTM Block`, that will have a `inputs` dictionary to send the sequence of interaction embeddings `input_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e78b0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(mm.Block):\n",
    "    def __init__(self, hidden_dim= 64, **kwargs):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        lstm = tf.keras.layers.LSTM(hidden_dim, return_sequences=False, dropout=0.05,\n",
    "                                   kernel_regularizer=regularizers.l2(1e-4))\n",
    "        self.lstm = tf.keras.layers.Bidirectional(lstm)\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, inputs, training=False, **kwargs) -> tf.Tensor:  \n",
    "        interactions = inputs['input_sequence']\n",
    "        sequence_representation = self.lstm(interactions)\n",
    "        return sequence_representation\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        input_shape = input_shape['input_sequence']\n",
    "        return (input_shape[0], input_shape[1], self.hidden_dim*2)\n",
    "    \n",
    "    \n",
    "bilstm = BiLSTM(hidden_dim=BI_LSTM_HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551125cf",
   "metadata": {},
   "source": [
    "For the Bi-LSTM model, let's use only `item_id_list_seq` for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "159b2cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>item_id_list_seq</td>\n",
       "      <td>(Tags.SEQUENCE, Tags.ITEM, Tags.ITEM_ID, Tags....</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.item_id_purchase_id.parquet</td>\n",
       "      <td>23619.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23619</td>\n",
       "      <td>item_id_purchase_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>purchase_id_first</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.TARGET)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.item_id_purchase_id.parquet</td>\n",
       "      <td>23619.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23619</td>\n",
       "      <td>item_id_purchase_id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'item_id_list_seq', 'tags': {<Tags.SEQUENCE: 'sequence'>, <Tags.ITEM: 'item'>, <Tags.ITEM_ID: 'item_id'>, <Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.item_id_purchase_id.parquet', 'embedding_sizes': {'cardinality': 23619.0, 'dimension': 450.0}, 'domain': {'min': 0, 'max': 23619, 'name': 'item_id_purchase_id'}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': False}, {'name': 'purchase_id_first', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.TARGET: 'target'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.item_id_purchase_id.parquet', 'embedding_sizes': {'cardinality': 23619.0, 'dimension': 450.0}, 'domain': {'min': 0, 'max': 23619, 'name': 'item_id_purchase_id'}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_model = train.schema.select_by_name(['item_id_list_seq', 'purchase_id_first'])\n",
    "schema_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a151720",
   "metadata": {},
   "source": [
    "#### Build the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d6937",
   "metadata": {},
   "source": [
    "Let's create a InputBlock which takes sequential features, concatenate them and return the sequence of interaction embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56b55008",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = InputBlock(\n",
    "        schema_model,\n",
    "        aggregation='concat',\n",
    "        seq=True,\n",
    "        max_seq_length=20,\n",
    "        embedding_options=mm.EmbeddingOptions(\n",
    "            embedding_dim_default=256,\n",
    "            infer_embedding_sizes=True,\n",
    "            infer_embedding_sizes_multiplier=2,\n",
    "            infer_embeddings_ensure_dim_multiple_of_8=True\n",
    "        ),\n",
    "        split_sparse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29891330",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_block = mm.ParallelBlock({'input_sequence': inputs}).connect(bilstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483babaa",
   "metadata": {},
   "source": [
    "A MLPBlock to get the sequence of hidden representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcad5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = mm.MLPBlock(\n",
    "                [64, 32],\n",
    "                activation='relu',\n",
    "                no_activation_last_layer=True,\n",
    "                dropout=DROPOUT,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc9eee",
   "metadata": {},
   "source": [
    "A Multi-Classiffication Prediction head which has\n",
    "- Layer Normalization\n",
    "- Weight Tying\n",
    "- Labels as One-hot encoded vectors, used for label smoothing \n",
    "- Temperature Scaling to reduce the overconfidence of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c014e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_call = L2Norm().connect(\n",
    "    ItemsPredictionWeightTying(schema_model), \n",
    "    mm.LabelToOneHot(), \n",
    "    LogitsTemperatureScaler(temperature=TEMPERATURE_SCALING)\n",
    ")\n",
    "\n",
    "prediction_task = mm.MultiClassClassificationTask(\n",
    "    target_name=\"purchase_id_first\",\n",
    "    pre=prediction_call,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9127f36",
   "metadata": {},
   "source": [
    "Now, we connect all the blocks togther to build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3614b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi_lstm = Model(dense_block, mlp_block, prediction_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31211efb",
   "metadata": {},
   "source": [
    "#### Define the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4edee4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    clipnorm=CLIPNORM\n",
    ")\n",
    "\n",
    "model_bi_lstm.compile(\n",
    "    optimizer=optimizer,\n",
    "    run_eagerly=True,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=LABEL_SMOOTHING),\n",
    "    metrics=mm.TopKMetricsAggregator.default_metrics(top_ks=[100])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e598f",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6126dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 06:23:00.631926: I tensorflow/stream_executor/cuda/cuda_dnn.cc:379] Loaded cuDNN version 8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "900/900 [==============================] - 131s 142ms/step - loss: 10.2243 - recall_at_100: 0.1718 - mrr_at_100: 0.0213 - ndcg_at_100: 0.0477 - map_at_100: 0.0213 - precision_at_100: 0.0017 - regularization_loss: 1.4159 - val_loss: 10.8529 - val_recall_at_100: 0.2108 - val_mrr_at_100: 0.0290 - val_ndcg_at_100: 0.0615 - val_map_at_100: 0.0290 - val_precision_at_100: 0.0021 - val_regularization_loss: 1.8728\n",
      "Epoch 2/10\n",
      "900/900 [==============================] - 121s 134ms/step - loss: 11.1808 - recall_at_100: 0.1776 - mrr_at_100: 0.0222 - ndcg_at_100: 0.0497 - map_at_100: 0.0222 - precision_at_100: 0.0018 - regularization_loss: 2.3790 - val_loss: 11.3215 - val_recall_at_100: 0.2182 - val_mrr_at_100: 0.0272 - val_ndcg_at_100: 0.0617 - val_map_at_100: 0.0272 - val_precision_at_100: 0.0022 - val_regularization_loss: 2.3580\n",
      "Epoch 3/10\n",
      "900/900 [==============================] - 124s 138ms/step - loss: 11.4790 - recall_at_100: 0.1805 - mrr_at_100: 0.0227 - ndcg_at_100: 0.0507 - map_at_100: 0.0227 - precision_at_100: 0.0018 - regularization_loss: 2.6898 - val_loss: 12.2739 - val_recall_at_100: 0.2118 - val_mrr_at_100: 0.0285 - val_ndcg_at_100: 0.0616 - val_map_at_100: 0.0285 - val_precision_at_100: 0.0021 - val_regularization_loss: 3.3104\n",
      "Epoch 4/10\n",
      "900/900 [==============================] - 124s 137ms/step - loss: 12.3759 - recall_at_100: 0.1831 - mrr_at_100: 0.0229 - ndcg_at_100: 0.0513 - map_at_100: 0.0229 - precision_at_100: 0.0018 - regularization_loss: 3.5997 - val_loss: 12.7626 - val_recall_at_100: 0.2152 - val_mrr_at_100: 0.0300 - val_ndcg_at_100: 0.0636 - val_map_at_100: 0.0300 - val_precision_at_100: 0.0022 - val_regularization_loss: 3.8414\n",
      "Epoch 6/10\n",
      "900/900 [==============================] - 124s 137ms/step - loss: 12.7663 - recall_at_100: 0.1844 - mrr_at_100: 0.0228 - ndcg_at_100: 0.0515 - map_at_100: 0.0228 - precision_at_100: 0.0018 - regularization_loss: 3.9898 - val_loss: 12.5975 - val_recall_at_100: 0.2217 - val_mrr_at_100: 0.0273 - val_ndcg_at_100: 0.0621 - val_map_at_100: 0.0273 - val_precision_at_100: 0.0022 - val_regularization_loss: 3.6727\n",
      "Epoch 7/10\n",
      "900/900 [==============================] - 123s 136ms/step - loss: 12.7859 - recall_at_100: 0.1861 - mrr_at_100: 0.0234 - ndcg_at_100: 0.0523 - map_at_100: 0.0234 - precision_at_100: 0.0019 - regularization_loss: 4.0216 - val_loss: 12.7387 - val_recall_at_100: 0.2065 - val_mrr_at_100: 0.0269 - val_ndcg_at_100: 0.0592 - val_map_at_100: 0.0269 - val_precision_at_100: 0.0021 - val_regularization_loss: 3.8049\n",
      "Epoch 8/10\n",
      "625/900 [===================>..........] - ETA: 35s - loss: 13.0817 - recall_at_100: 0.1733 - mrr_at_100: 0.0214 - ndcg_at_100: 0.0483 - map_at_100: 0.0214 - precision_at_100: 0.0017 - regularization_loss: 4.2670"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model_bi_lstm.fit(\n",
    "    train,\n",
    "    validation_data=valid,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    schema=schema_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e1645c",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8a83c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 10s 88ms/step - loss: 13.1749 - recall_at_100: 0.2540 - mrr_at_100: 0.0351 - ndcg_at_100: 0.0750 - map_at_100: 0.0351 - precision_at_100: 0.0025 - regularization_loss: 4.2864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 13.174932479858398,\n",
       " 'recall_at_100': 0.22673992812633514,\n",
       " 'mrr_at_100': 0.029583772644400597,\n",
       " 'ndcg_at_100': 0.06532561033964157,\n",
       " 'map_at_100': 0.029583772644400597,\n",
       " 'precision_at_100': 0.002267398638650775,\n",
       " 'regularization_loss': 4.286446571350098}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bi_lstm.evaluate(valid_ds, batch_size=1024, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86ef86e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 5s 57ms/step\n",
      "Mask Predictions\n",
      "Generate Top100 Recommendations\n",
      "Transform Top100 Recommendations\n",
      "Model evaluation\n",
      "MRR:  0.029583778549516854\n",
      "CPU times: user 50.5 s, sys: 12.6 s, total: 1min 3s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = model_bi_lstm.predict(valid, batch_size=1024, verbose=1)\n",
    "ddf = valid.to_ddf()\n",
    "ddf = ddf[['session_id', 'purchase_id_first']].compute()\n",
    "df_rec = generate_recommendations(predictions, ddf)\n",
    "val_mrr = evaluate(df_rec)['total']\n",
    "print('MRR: ',val_mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa1061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf63b42ee7281f212c620f1d25b2d8e8d34e0a013403ff3a06acffed6a925ac0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
