{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e493825",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b423f1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_models_accelerate-training-of-large-embedding-tables-by-lazyadam/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# 1. Use multiple optimizers to accelerate training with LazyAdam\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container. \n",
    "\n",
    "Merlin Models provide various model APIs for training, as shown in notebook [Iterating over Deep Learning Models using Merlin Models](https://nvidia-merlin.github.io/models/main/examples/03-Exploring-different-models.html). We can create a model, such as [Two Tower](https://nvidia-merlin.github.io/models/main/models_overview.html?highlight=two%20tower#two-tower), [DLRM](https://nvidia-merlin.github.io/models/main/examples/03-Exploring-different-models.html#dlrm-model) and so on, by simply one line: `model=mm.DLRMModel(schema)`. Some models contain large embedding tables, and training could be slow on such large sparse embeddings. However, this process could be accelerated by using a special optimizer, LazyAdam.\n",
    "\n",
    "In this example, we utilize LazyAdam for large embedding tables and original Adam for other trainable weights to accelerate the whole training process.\n",
    "\n",
    "\n",
    "**Learning objectives**\n",
    "- Training a model with multiple optimizers\n",
    "- Utilizing LazyAdam for training on large embedding tables\n",
    "\n",
    "For this notebook, we use a single 32GB Tesla V100 GPU and we report the training results and how much we speed up the model training time by using LazyAdam optimizer for large embedding tables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "381c615c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-10-20 19:37:18.414599: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-20 19:37:21.731176: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-10-20 19:37:21.731361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8080 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:0a:00.0, compute capability: 7.0\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.datasets.synthetic import generate_data\n",
    "from merlin.schema import Schema, Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d1408",
   "metadata": {},
   "source": [
    "## 1.1. Generate Synthetic Dataset\n",
    "To generate the synthetic dataset for our example, we can use `generate_data()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b36681a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "NUM_ROWS = int(os.environ.get(\"NUM_ROWS\", '1000000'))\n",
    "\n",
    "train, valid = generate_data(\"e-commerce-large\", int(NUM_ROWS), set_sizes=(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c62e9b",
   "metadata": {},
   "source": [
    "Create a schema object and remove the target columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3c1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = train.schema.without(['click', 'conversion'])\n",
    "train.schema = schema\n",
    "valid.schema = schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa90b44-2926-443b-a4bf-923fe9b4d647",
   "metadata": {},
   "source": [
    "## 1.2. Build a Two-Tower model and train with a single optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772531e0-7cb9-4e58-838e-9b15b36ee8bc",
   "metadata": {},
   "source": [
    "Now, let's create a Two-tower model and compile it only with `Adam` optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971f235-4175-4ea1-a36f-f3188732c398",
   "metadata": {},
   "source": [
    "Define item and query embeddings and feed them to the `InputBlockV2` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a908d37-c10d-46f3-8e42-b1155dfad5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embeddings = mm.Embeddings(schema.select_by_tag(Tags.ITEM), infer_embedding_sizes=True)\n",
    "query_embeddings = mm.Embeddings(schema.select_by_tag(Tags.USER), infer_embedding_sizes=True)\n",
    "model1 = mm.TwoTowerModel(schema, \n",
    "                         item_tower=mm.InputBlockV2(schema.select_by_tag(Tags.ITEM), categorical=item_embeddings).connect(mm.MLPBlock([64])), \n",
    "                         query_tower=mm.InputBlockV2(schema.select_by_tag(Tags.USER), categorical=query_embeddings).connect(mm.MLPBlock([64])),\n",
    "                         samplers=[mm.InBatchSampler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af396dc-d8b2-4ed2-af5b-374d0e272596",
   "metadata": {},
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0500ad25-29e0-40c8-85bc-6e3864107c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7feb6f06c1c0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The sampler InBatchSampler returned no samples for this batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7feb6f06c1c0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "782/782 [==============================] - 76s 71ms/step - loss: 6.9128 - recall_at_10: 0.0268 - mrr_at_10: 0.0216 - ndcg_at_10: 0.0229 - map_at_10: 0.0216 - precision_at_10: 0.0027 - regularization_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe73bce23a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(optimizer=\"adam\")\n",
    "model1.fit(train, batch_size=1024, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a197d69",
   "metadata": {},
   "source": [
    "## 1.3. Build a Two-Tower model and train with Multiple Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8872a-1394-418c-89c3-a785b60005f2",
   "metadata": {},
   "source": [
    "Now, let's create the same model but this time compile it with multiple-optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b31f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embeddings = mm.Embeddings(schema.select_by_tag(Tags.ITEM), infer_embedding_sizes=True)\n",
    "query_embeddings = mm.Embeddings(schema.select_by_tag(Tags.USER), infer_embedding_sizes=True)\n",
    "model2 = mm.TwoTowerModel(schema, \n",
    "                         item_tower=mm.InputBlockV2(schema.select_by_tag(Tags.ITEM), categorical=item_embeddings).connect(mm.MLPBlock([64])), \n",
    "                         query_tower=mm.InputBlockV2(schema.select_by_tag(Tags.USER), categorical=query_embeddings).connect(mm.MLPBlock([64])),\n",
    "                         samplers=[mm.InBatchSampler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc99674d",
   "metadata": {},
   "source": [
    "The model initializer would infer the embedding table size from the schema, where the first dimension (`input_dim`) of each embedding table is the same as the cardinalities (categories) of each feature, and the second dimension is specified by the user. By setting `infer_embedding_sizes=True`, the initializer would infer the size based on the cardinalities: \n",
    "$$output\\_dim=\\left \\lfloor cardinality^{0.25}\\times multiplier \\right \\rfloor$$\n",
    "The multiplier is set to 2.0 by default. To achieve the best performance with GPU operators, we adjust the embedding dimensions to multiples of 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed1f36-68f5-4c03-be97-f59957ea66ff",
   "metadata": {},
   "source": [
    "### 1.3.1. Apply Multiple Optimizers to the Model\n",
    "\n",
    "We usually set one optimizer to train a model, but for large embedding tables, at each batch, the weights to be updated could be really sparse, in other words, each time we only update the model based on a small batch of training data, so for a large embedding table (first dimension >>  batch size), at most batch_size rows would be updated. Thus in order to acceleate training on large embedding tables, we want to utilize the Lazy Adam for those large tables. \n",
    "\n",
    "Compared with Adam, Lazy Adam is optimized for sparse updates. It only update sparse variables indices for current batch. However it may result in slight difference in experiment results compared with Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c555f84d-040f-432e-bd26-772761aadea3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.3.1.1. Split Embedding Tables based on the First Dimension (`input_dim`)\n",
    "Since we want to apply LazyAdam to the large tables, we have to split all tables into two sets. The result of `split_embeddings_on_size` (i.e. `item_large_tables` and `item_small_tables`) are two lists of embedding tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69ab5d2a-6493-4bbf-9e31-94f5b545733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_large_tables, item_small_tables = mm.split_embeddings_on_size(item_embeddings, threshold=1000)\n",
    "query_large_tables, query_small_tables = mm.split_embeddings_on_size(query_embeddings, threshold=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6af11-37a0-4cae-8443-997ede87c523",
   "metadata": {},
   "source": [
    "We can print out the size of each embedding table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "705e2a95-37c9-4bc6-9092-cd2c1c1cff61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large embedding tables of query tower:\n",
      "user_categories first dimension:  6087 second dimension 24\n",
      "user_shops first dimension:  116742 second dimension 40\n",
      "user_brands first dimension:  58016 second dimension 32\n",
      "user_intentions first dimension:  33787 second dimension 32\n",
      "user_id first dimension:  294737 second dimension 48\n",
      "Small embedding tables of query tower:\n",
      "user_profile first dimension:  99 second dimension 8\n",
      "user_group first dimension:  15 second dimension 8\n",
      "user_gender first dimension:  4 second dimension 8\n",
      "user_age first dimension:  9 second dimension 8\n",
      "user_consumption_1 first dimension:  5 second dimension 8\n",
      "user_consumption_2 first dimension:  5 second dimension 8\n",
      "user_is_occupied first dimension:  4 second dimension 8\n",
      "user_geography first dimension:  6 second dimension 8\n"
     ]
    }
   ],
   "source": [
    "print(\"Large embedding tables of query tower:\")\n",
    "for t in query_large_tables:\n",
    "    print(t.name, \"first dimension: \", t.input_dim, \"second dimension\", t.dim)\n",
    "print(\"Small embedding tables of query tower:\")\n",
    "for t in  query_small_tables:\n",
    "    print(t.name, \"first dimension: \", t.input_dim, \"second dimension\", t.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb23734-e2ea-4cec-8cc9-5109be0d08b7",
   "metadata": {},
   "source": [
    "#### 1.3.1.2. Set MultiOptimizer\n",
    "\n",
    "The `MultiOptimizer` module enables multiple optimizers [(e.g. Adam, SGD, RMSProp, Adagrad)](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) to be used in different layers in parallel. Here we want to apply `LazyAdam` for large embedding tables, and `Adam` for the small embedding tables and all other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de3b8aac-44c1-4237-b31b-f025461ed2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = mm.MultiOptimizer(\n",
    "                default_optimizer=\"adam\",\n",
    "                optimizers_and_blocks=[mm.OptimizerBlocks(mm.LazyAdam(), item_large_tables + query_large_tables),\n",
    "                                       mm.OptimizerBlocks(\"adam\", item_small_tables + query_small_tables)]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e6d56-8a20-40b2-bc45-5dbf3ecce51d",
   "metadata": {},
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c08b359-ee01-4ed1-855e-61074f52e99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The sampler InBatchSampler returned no samples for this batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 22s 18ms/step - loss: 6.9128 - recall_at_10: 0.0267 - mrr_at_10: 0.0198 - ndcg_at_10: 0.0215 - map_at_10: 0.0198 - precision_at_10: 0.0027 - regularization_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe73a382250>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(optimizer=optimizer)\n",
    "model2.fit(train, batch_size=1024, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c1351-1585-44ea-885b-e250d9e5c161",
   "metadata": {},
   "source": [
    "Note all other trainable parameters not specified an optimizer would use the `default_optimizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9990df4-37bf-4c33-b7cc-17eeec2f3591",
   "metadata": {},
   "source": [
    "## 1.4. Compare model training times with multiple optimizers vs single optimizers\n",
    "\n",
    "We first created a Two-Tower model and trained only with `Adam` optimizer. The training result shows that for each step, it costs about `71 ms` and total training time for one epoch is `76s`. Afterwards, we trained the same model this time with multiple optimizers where we use `LazyAdam` for large embedding tables and `Adam` for small embeding tables. As shown in the experiment above, the training time with multi-optimizer is about `22s`, and it achieves almost `3.5X` speed up.\n",
    "\n",
    "That's it! In this example, you learned how to use multiple optimizers for training a Two-Tower model, where one of the optimizers is `LazyAdam` which is variant of the Adam optimizer that handles sparse updates more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c78a7de67f1468ee33d22a76790123f2989400fa0e73ac6b45f15b09432f615d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
