{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f49a48e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be5b7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlinmodelsrankingwithmultitasklearning/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "    \n",
    "\n",
    "# Multi-Task Learning for Ranking\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container. \n",
    "    \n",
    "In the industry, it is common to find scenarios where you need to score the likelihood of different user events regarding items, e.g., clicking, liking, sharing, commenting, following the author, etc. Instead of spending computational resources to train and deploy different models for each task, Multi-Task Learning (MTL) techniques have been popular to train a single model that is able to predict multiple targets. By using MTL, it is possible to improve the tasks accuracy, in particular for sparser targets.\n",
    "\n",
    "In this example, we demonstrate how to build and train ranking models with multiple targets. We introduce the building blocks Merlin Models provides for MTL support and also MTL-specific architectures designed for accuracy improvement of many different tasks: [**Multi-gate Mixture-of-Experts (MMoE)**](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007) and [**Progressive Layered Extraction (PLE)**](https://dl.acm.org/doi/10.1145/3383313.3412236).\n",
    "\n",
    "In this example notebook, we use synthetic data based on the schema of a dataset released in the [TenRec paper](https://arxiv.org/abs/2210.10629), which is suitable for multi-task learning for providing multiple targets (types of user-item events). \n",
    "\n",
    "### Learning objectives\n",
    "- Getting to know the buiilding blocks Merlin provides for MTL\n",
    "- Training different deep learning-based ranking models with multi-task learning using Merlin Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee3b8fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 12:24:29.026178: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-12 12:24:31.356461: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-12 12:24:33.423192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29249 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "2023-01-12 12:24:33.424271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30625 MB memory:  -> device: 1, name: Quadro GV100, pci bus id: 0000:2d:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "#os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "os.environ[\"TF_MEMORY_ALLOCATION\"] = \"0.9\"\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.schema.tags import Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b124794",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a0c62",
   "metadata": {},
   "source": [
    "Here we generate synthetic dataset based on the [schema](https://github.com/NVIDIA-Merlin/models/blob/main/merlin/datasets/entertainment/tenrec_video/schema.pbtxt) of the `tenrec-video` dataset. The original dataset was released by the [TenRec paper](https://arxiv.org/abs/2210.10629), and is suitable for multi-task learning for providing multiple targets (types of user-item events).  \n",
    "To make the synthetic data more realistic, our data generator takes into account the original cardinalities of categorical features and the dependency of user features to user id and item features to item id.\n"
    "For more information about how the schema API works you can check this [example](https://github.com/NVIDIA-Merlin/models/blob/main/merlin/datasets/entertainment/tenrec_video/schema.pbtxt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42727f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from merlin.datasets.synthetic import generate_data\n",
    "\n",
    "NUM_ROWS = os.environ.get(\"NUM_ROWS\", 100_000)\n",
    "\n",
    "train_ds, valid_ds = generate_data(\"tenrec-video\", int(NUM_ROWS), set_sizes=(0.8, 0.2))\n",
    "schema = train_ds.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea991536",
   "metadata": {},
   "source": [
    "By inspecting the columns tagging on the dataset schema, we can notice that there are number of user features (`user_id`, `gender`, `age`) and item features (`item_id`, `video_category`). There are also four binary classification targets (`click`, `follow`, `like`, and `share`) and one regression target (`watching_times`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28589d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_id</td>\n",
       "      <td>(Tags.ID, Tags.USER_ID, Tags.USER, Tags.CATEGO...</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>.//categories/unique.user_id.parquet</td>\n",
       "      <td>2633851.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>user_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>item_id</td>\n",
       "      <td>(Tags.ITEM_ID, Tags.ID, Tags.CATEGORICAL, Tags...</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>.//categories/unique.item_id.parquet</td>\n",
       "      <td>179280.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179280.0</td>\n",
       "      <td>item_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video_category</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.ITEM)</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>.//categories/unique.video_category.parquet</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>video_category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gender</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.USER)</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>.//categories/unique.gender.parquet</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>age</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.USER)</td>\n",
       "      <td>int32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>.//categories/unique.age.parquet</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>click</td>\n",
       "      <td>(Tags.BINARY_CLASSIFICATION, Tags.TARGET, Tags...</td>\n",
       "      <td>int8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>follow</td>\n",
       "      <td>(Tags.BINARY_CLASSIFICATION, Tags.TARGET, Tags...</td>\n",
       "      <td>int8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>like</td>\n",
       "      <td>(Tags.BINARY_CLASSIFICATION, Tags.TARGET, Tags...</td>\n",
       "      <td>int8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>share</td>\n",
       "      <td>(Tags.BINARY_CLASSIFICATION, Tags.TARGET, Tags...</td>\n",
       "      <td>int8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>watching_times</td>\n",
       "      <td>(Tags.REGRESSION, Tags.TARGET)</td>\n",
       "      <td>int16</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>watching_times</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'user_id', 'tags': {<Tags.ID: 'id'>, <Tags.USER_ID: 'user_id'>, <Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'start_index': 1.0, 'cat_path': './/categories/unique.user_id.parquet', 'embedding_sizes': {'cardinality': 2633851.0, 'dimension': 512.0}, 'max_size': 0.0, 'domain': {'min': 0, 'max': 100000, 'name': 'user_id'}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'item_id', 'tags': {<Tags.ITEM_ID: 'item_id'>, <Tags.ID: 'id'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, 'properties': {'cat_path': './/categories/unique.item_id.parquet', 'embedding_sizes': {'dimension': 512.0, 'cardinality': 179280.0}, 'max_size': 0.0, 'num_buckets': None, 'freq_threshold': 0.0, 'start_index': 1.0, 'domain': {'min': 0, 'max': 179280, 'name': 'item_id'}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'video_category', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, 'properties': {'embedding_sizes': {'dimension': 16.0, 'cardinality': 5.0}, 'cat_path': './/categories/unique.video_category.parquet', 'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 1.0, 'domain': {'min': 0, 'max': 5, 'name': 'video_category'}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'gender', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>}, 'properties': {'max_size': 0.0, 'num_buckets': None, 'freq_threshold': 0.0, 'cat_path': './/categories/unique.gender.parquet', 'start_index': 1.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 5.0}, 'domain': {'min': 0, 'max': 5, 'name': 'gender'}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'age', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>}, 'properties': {'cat_path': './/categories/unique.age.parquet', 'start_index': 1.0, 'num_buckets': None, 'embedding_sizes': {'cardinality': 10.0, 'dimension': 16.0}, 'freq_threshold': 0.0, 'max_size': 0.0, 'domain': {'min': 0, 'max': 10, 'name': 'age'}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'click', 'tags': {<Tags.BINARY_CLASSIFICATION: 'binary_classification'>, <Tags.TARGET: 'target'>, <Tags.BINARY: 'binary'>}, 'properties': {}, 'dtype': dtype('int8'), 'is_list': False, 'is_ragged': False}, {'name': 'follow', 'tags': {<Tags.BINARY_CLASSIFICATION: 'binary_classification'>, <Tags.TARGET: 'target'>, <Tags.BINARY: 'binary'>}, 'properties': {}, 'dtype': dtype('int8'), 'is_list': False, 'is_ragged': False}, {'name': 'like', 'tags': {<Tags.BINARY_CLASSIFICATION: 'binary_classification'>, <Tags.TARGET: 'target'>, <Tags.BINARY: 'binary'>}, 'properties': {}, 'dtype': dtype('int8'), 'is_list': False, 'is_ragged': False}, {'name': 'share', 'tags': {<Tags.BINARY_CLASSIFICATION: 'binary_classification'>, <Tags.TARGET: 'target'>, <Tags.BINARY: 'binary'>}, 'properties': {}, 'dtype': dtype('int8'), 'is_list': False, 'is_ragged': False}, {'name': 'watching_times', 'tags': {<Tags.REGRESSION: 'regression'>, <Tags.TARGET: 'target'>}, 'properties': {'domain': {'min': 0, 'max': 5, 'name': 'watching_times'}}, 'dtype': dtype('int16'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5caa9151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>item_id</th>\n",
       "      <th>video_category</th>\n",
       "      <th>click</th>\n",
       "      <th>follow</th>\n",
       "      <th>like</th>\n",
       "      <th>share</th>\n",
       "      <th>watching_times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  gender  age  item_id  video_category  click  follow  like  share  \\\n",
       "0       80       1    1       24               1      1       1     1      0   \n",
       "1        2       1    1        6               1      0       0     0      1   \n",
       "2       22       1    1       20               1      1       1     1      0   \n",
       "3       18       1    1       14               1      1       0     0      1   \n",
       "4       43       1    1        8               1      0       0     1      1   \n",
       "\n",
       "   watching_times  \n",
       "0               2  \n",
       "1               0  \n",
       "2               1  \n",
       "3               1  \n",
       "4               4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing first rows of the generated dataframe\n",
    "train_ds.to_ddf().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f155bd",
   "metadata": {},
   "source": [
    "## Building and training MTL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a71708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4 * 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc4e12",
   "metadata": {},
   "source": [
    "The simplest way to build a model with Merlin Models is using `InputBlockV2` and `OutputBlock` building blocks, that infer the input features and target columns from the schema.\n",
    "The `InputBlockV2` creates the embedding layers for categorical features and concatenates all features. The `OutputBlock` creates a head `ModelOutput` for each target depending on the task type (tagged in the column schema), e.g. `RegressionOutput()` for regression, `BinaryOuput()` for binary classification, `CategoricalOutput` for multi-class classification.  \n",
    "\n",
    "You can inspect below a multi-task learning model created for this dataset with just four lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81465998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (blocks): _TupleWrapper((ParallelBlock(\n",
       "    (_aggregation): ConcatFeatures()\n",
       "    (parallel_layers): Dict(\n",
       "      (categorical): ParallelBlock(\n",
       "        (parallel_layers): Dict(\n",
       "          (user_id): EmbeddingTable(\n",
       "            (features): Dict(\n",
       "              (user_id): ColumnSchema(name='user_id', tags={<Tags.ID: 'id'>, <Tags.USER_ID: 'user_id'>, <Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, properties={'freq_threshold': 0.0, 'num_buckets': None, 'start_index': 1.0, 'cat_path': './/categories/unique.user_id.parquet', 'embedding_sizes': {'cardinality': 2633851.0, 'dimension': 512.0}, 'max_size': 0.0, 'domain': {'min': 0, 'max': 100000, 'name': 'user_id'}}, dtype=dtype('int32'), is_list=False, is_ragged=False)\n",
       "            )\n",
       "            (table): Embedding()\n",
       "          )\n",
       "          (item_id): EmbeddingTable(\n",
       "            (features): Dict(\n",
       "              (item_id): ColumnSchema(name='item_id', tags={<Tags.ITEM_ID: 'item_id'>, <Tags.ID: 'id'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, properties={'cat_path': './/categories/unique.item_id.parquet', 'embedding_sizes': {'dimension': 512.0, 'cardinality': 179280.0}, 'max_size': 0.0, 'num_buckets': None, 'freq_threshold': 0.0, 'start_index': 1.0, 'domain': {'min': 0, 'max': 179280, 'name': 'item_id'}}, dtype=dtype('int32'), is_list=False, is_ragged=False)\n",
       "            )\n",
       "            (table): Embedding()\n",
       "          )\n",
       "          (video_category): EmbeddingTable(\n",
       "            (features): Dict(\n",
       "              (video_category): ColumnSchema(name='video_category', tags={<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, properties={'embedding_sizes': {'dimension': 16.0, 'cardinality': 5.0}, 'cat_path': './/categories/unique.video_category.parquet', 'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 1.0, 'domain': {'min': 0, 'max': 5, 'name': 'video_category'}}, dtype=dtype('int32'), is_list=False, is_ragged=False)\n",
       "            )\n",
       "            (table): Embedding()\n",
       "          )\n",
       "          (gender): EmbeddingTable(\n",
       "            (features): Dict(\n",
       "              (gender): ColumnSchema(name='gender', tags={<Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>}, properties={'max_size': 0.0, 'num_buckets': None, 'freq_threshold': 0.0, 'cat_path': './/categories/unique.gender.parquet', 'start_index': 1.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 5.0}, 'domain': {'min': 0, 'max': 5, 'name': 'gender'}}, dtype=dtype('int32'), is_list=False, is_ragged=False)\n",
       "            )\n",
       "            (table): Embedding()\n",
       "          )\n",
       "          (age): EmbeddingTable(\n",
       "            (features): Dict(\n",
       "              (age): ColumnSchema(name='age', tags={<Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>}, properties={'cat_path': './/categories/unique.age.parquet', 'start_index': 1.0, 'num_buckets': None, 'embedding_sizes': {'cardinality': 10.0, 'dimension': 16.0}, 'freq_threshold': 0.0, 'max_size': 0.0, 'domain': {'min': 0, 'max': 10, 'name': 'age'}}, dtype=dtype('int32'), is_list=False, is_ragged=False)\n",
       "            )\n",
       "            (table): Embedding()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  ), MLPBlock(\n",
       "    (layers): List(\n",
       "      (0): _Dense(\n",
       "        (dense): Dense(32, activation=relu, use_bias=True)\n",
       "      )\n",
       "      (1): _Dense(\n",
       "        (dense): Dense(16, activation=relu, use_bias=True)\n",
       "      )\n",
       "    )\n",
       "  ), ParallelBlock(\n",
       "    (parallel_layers): Dict(\n",
       "      (click/binary_output): BinaryOutput(\n",
       "        (to_call): Dense(1, activation=sigmoid, use_bias=True)\n",
       "      )\n",
       "      (follow/binary_output): BinaryOutput(\n",
       "        (to_call): Dense(1, activation=sigmoid, use_bias=True)\n",
       "      )\n",
       "      (like/binary_output): BinaryOutput(\n",
       "        (to_call): Dense(1, activation=sigmoid, use_bias=True)\n",
       "      )\n",
       "      (share/binary_output): BinaryOutput(\n",
       "        (to_call): Dense(1, activation=sigmoid, use_bias=True)\n",
       "      )\n",
       "      (watching_times/regression_output): RegressionOutput(\n",
       "        (to_call): Dense(1, activation=linear, use_bias=True)\n",
       "      )\n",
       "    )\n",
       "  )))\n",
       "  (context): ModelContext()\n",
       "  (process_list): ProcessList()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mm.Model(\n",
    "    mm.InputBlockV2(schema),\n",
    "    mm.MLPBlock([32,16]),\n",
    "    mm.OutputBlock(schema)\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a8c333",
   "metadata": {},
   "source": [
    "*Note*: If you want to build a model for just a subset of the target features, you can either remove the unwanted columns from schema: `schema.without([\"like\", \"follow\", \"share\"])`\n",
    "\n",
    "OR you can replace `mm.OutputBlock(schema)` by a `ParallelBlock` with only the desired targets:\n",
    "```python\n",
    "mm.ParallelBlock(\n",
    "  mm.BinaryOutput(\"click\"), mm.RegressionOutput(\"watching_times\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d685284",
   "metadata": {},
   "source": [
    "### Train and evaluation of MTL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea008b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 5s 24ms/step - loss: 8.3883 - click/binary_output_loss: 0.6934 - follow/binary_output_loss: 0.6933 - like/binary_output_loss: 0.6934 - share/binary_output_loss: 0.6933 - watching_times/regression_output_loss: 5.6149 - click/binary_output/precision: 0.4967 - click/binary_output/recall: 0.9650 - click/binary_output/binary_accuracy: 0.4968 - click/binary_output/auc: 0.4990 - follow/binary_output/precision: 0.4970 - follow/binary_output/recall: 0.1199 - follow/binary_output/binary_accuracy: 0.4987 - follow/binary_output/auc: 0.4988 - like/binary_output/precision: 0.4984 - like/binary_output/recall: 0.7945 - like/binary_output/binary_accuracy: 0.4984 - like/binary_output/auc: 0.5007 - share/binary_output/precision: 0.5021 - share/binary_output/recall: 0.7559 - share/binary_output/binary_accuracy: 0.5012 - share/binary_output/auc: 0.4972 - watching_times/regression_output/root_mean_squared_error: 2.3696 - regularization_loss: 0.0000e+00 - loss_batch: 8.3557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17585ff130>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False)\n",
    "model.fit(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80157c3f",
   "metadata": {},
   "source": [
    "By inspecting the metrics output from model evaluation, we can observe that there are specific default metrics to each target; for binary classification (`precision`, `recall`, `binary_accuracy`, `auc`) and for regression (`root_mean_squared_error`) tasks.  \n",
    "Each task has its own loss (e.g. `click/binary_output_loss`, `watching_times/regression_output_loss`) and the `loss` is the sum of all tasks losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ed0567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 16ms/step - loss: 7.8596 - click/binary_output_loss: 0.6932 - follow/binary_output_loss: 0.6934 - like/binary_output_loss: 0.6939 - share/binary_output_loss: 0.6936 - watching_times/regression_output_loss: 5.0855 - click/binary_output/precision: 0.5044 - click/binary_output/recall: 0.9450 - click/binary_output/binary_accuracy: 0.5045 - click/binary_output/auc: 0.5010 - follow/binary_output/precision: 0.0000e+00 - follow/binary_output/recall: 0.0000e+00 - follow/binary_output/binary_accuracy: 0.5023 - follow/binary_output/auc: 0.5048 - like/binary_output/precision: 0.5061 - like/binary_output/recall: 0.9998 - like/binary_output/binary_accuracy: 0.5062 - like/binary_output/auc: 0.4946 - share/binary_output/precision: 0.4998 - share/binary_output/recall: 0.9993 - share/binary_output/binary_accuracy: 0.4997 - share/binary_output/auc: 0.5049 - watching_times/regression_output/root_mean_squared_error: 2.2551 - regularization_loss: 0.0000e+00 - loss_batch: 7.8647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 7.8595967292785645,\n",
       " 'click/binary_output_loss': 0.6931833028793335,\n",
       " 'follow/binary_output_loss': 0.6933688521385193,\n",
       " 'like/binary_output_loss': 0.6938996315002441,\n",
       " 'share/binary_output_loss': 0.693648636341095,\n",
       " 'watching_times/regression_output_loss': 5.085495948791504,\n",
       " 'click/binary_output/precision': 0.5044233798980713,\n",
       " 'click/binary_output/recall': 0.9450178742408752,\n",
       " 'click/binary_output/binary_accuracy': 0.5045499801635742,\n",
       " 'click/binary_output/auc': 0.5010042190551758,\n",
       " 'follow/binary_output/precision': 0.0,\n",
       " 'follow/binary_output/recall': 0.0,\n",
       " 'follow/binary_output/binary_accuracy': 0.5023000240325928,\n",
       " 'follow/binary_output/auc': 0.5047903656959534,\n",
       " 'like/binary_output/precision': 0.5060530304908752,\n",
       " 'like/binary_output/recall': 0.9998023509979248,\n",
       " 'like/binary_output/binary_accuracy': 0.5062000155448914,\n",
       " 'like/binary_output/auc': 0.49457669258117676,\n",
       " 'share/binary_output/precision': 0.499799907207489,\n",
       " 'share/binary_output/recall': 0.999299943447113,\n",
       " 'share/binary_output/binary_accuracy': 0.4996500015258789,\n",
       " 'share/binary_output/auc': 0.5048731565475464,\n",
       " 'watching_times/regression_output/root_mean_squared_error': 2.2551043033599854,\n",
       " 'regularization_loss': 0.0,\n",
       " 'loss_batch': 7.886743545532227}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_ds, batch_size=BATCH_SIZE, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec16ffbb",
   "metadata": {},
   "source": [
    "### Setting loss weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831b6a5a",
   "metadata": {},
   "source": [
    "You can balance the importance of individual task losses into the final `loss` by setting `loss_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106c0e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 3s 24ms/step - loss: 14.0230 - click/binary_output_loss: 0.6937 - follow/binary_output_loss: 0.6955 - like/binary_output_loss: 0.6967 - share/binary_output_loss: 0.6943 - watching_times/regression_output_loss: 4.2935 - click/binary_output/precision: 0.4962 - click/binary_output/recall: 0.9505 - click/binary_output/binary_accuracy: 0.4958 - click/binary_output/auc: 0.4978 - follow/binary_output/precision: 0.0000e+00 - follow/binary_output/recall: 0.0000e+00 - follow/binary_output/binary_accuracy: 0.4994 - follow/binary_output/auc: 0.4964 - like/binary_output/precision: 0.4991 - like/binary_output/recall: 0.9998 - like/binary_output/binary_accuracy: 0.4992 - like/binary_output/auc: 0.4990 - share/binary_output/precision: 0.5020 - share/binary_output/recall: 0.9998 - share/binary_output/binary_accuracy: 0.5020 - share/binary_output/auc: 0.4998 - watching_times/regression_output/root_mean_squared_error: 2.0721 - regularization_loss: 0.0000e+00 - loss_batch: 13.9615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17582d4cd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_weights = {\n",
    "        \"click/binary_output\": 5.0,\n",
    "        \"like/binary_output\": 4.0,\n",
    "        \"share/binary_output\": 3.0,\n",
    "        \"follow/binary_output\": 2.0,\n",
    "        \"watching_times/regression_output\": 1.0,        \n",
    "    }\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\", run_eagerly=False, loss_weights=loss_weights)\n",
    "model.fit(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03486667",
   "metadata": {},
   "source": [
    "### Setting task-specific class / sample weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab76244",
   "metadata": {},
   "source": [
    "Keras supports setting `class_weight` and `sample_weight` for **single-task models** in `model.fit()`.  \n",
    "\n",
    "The `class_weight` allows weighting the classes of categorical/binary target in the loss, so that model training can pay more attention to samples from an under-represented class.  \n",
    "\n",
    "The `sample_weight` allows weighting data samples which should account more or less for the loss during training. If `weighted_metrics` is provided in `model.compile()`, then those metrics will also be weighted by `sample_weight` during training and testing.\n",
    "\n",
    "Merlin Models provides building blocks for **tasks-specific class and sample weights** with the `ColumnBasedSampleWeight` block. Here are some examples for different use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31678e8f",
   "metadata": {},
   "source": [
    "#### 1. Setting class weights per task\n",
    "Here we create an MTL model to predict `click` and `like` targets. We set negative events (0s) to have weight 1.0 and positive events (1s) to have a higher weight. As `like` target is a more rare event (sparser) than `click`  we should use higher sample weight for positive examples for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c681d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_block = mm.ParallelBlock(\n",
    "  mm.BinaryOutput(\"click\",\n",
    "                  post=mm.ColumnBasedSampleWeight(\n",
    "                        binary_class_weights=(1.0, 5.0), \n",
    "                  )), \n",
    "  mm.BinaryOutput(\"like\",\n",
    "                  post=mm.ColumnBasedSampleWeight(\n",
    "                        binary_class_weights=(1.0, 20.0), \n",
    "                  ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45a749a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 17ms/step - loss: 8.4014 - click/binary_output_loss: 1.9881 - like/binary_output_loss: 6.4133 - click/binary_output/precision: 0.4970 - click/binary_output/recall: 0.9984 - click/binary_output/binary_accuracy: 0.4970 - click/binary_output/auc: 0.4992 - like/binary_output/precision: 0.4991 - like/binary_output/recall: 0.9820 - like/binary_output/binary_accuracy: 0.4991 - like/binary_output/auc: 0.5008 - regularization_loss: 0.0000e+00 - loss_batch: 8.3273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f8dee9b20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mm.Model(\n",
    "    mm.InputBlockV2(schema),\n",
    "    mm.MLPBlock([32,16]),\n",
    "    output_block\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\", run_eagerly=False)\n",
    "model.fit(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572cb7c6",
   "metadata": {},
   "source": [
    "#### 2. Using other target / feature as weight per task\n",
    "Another use case would be using a feature or other target for sample weight. For example, the `watching_times` target column represents the number of times the user has watched the video. That column could be used as a strength indicator on how much the video is relevant for the user. So we can use it as a the sample weight for `click`, so that the loss emphasizes more from such samples.  \n",
    "P.s. Regular columns (non-target) can also be used as sample weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5cfb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_block = mm.BinaryOutput(\"click\",\n",
    "                  post=mm.ColumnBasedSampleWeight(\"watching_times\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4465730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 13ms/step - loss: 1.3854 - precision: 0.4960 - recall: 0.1748 - binary_accuracy: 0.5017 - auc: 0.5007 - regularization_loss: 0.0000e+00 - loss_batch: 1.3855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f8d9987c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mm.Model(\n",
    "    mm.InputBlockV2(schema),\n",
    "    mm.MLPBlock([32,16]),\n",
    "    output_block\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\", run_eagerly=False)\n",
    "model.fit(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827c9df",
   "metadata": {},
   "source": [
    "#### 3. Using another binary target as sample space\n",
    "In some cases, a target might be conditioned to another binary target. For example, there might be some event dependency in the system the user is interacting with, so that the user can only `like` or `share` if `click` event happened first. As the more specific events are usually much less frequent than `click`, they are sparser thus suffer more from unbalanced class training. In such cases, as you can only have a positive event (i.e., `like=1`) if `click=1`, we can use that as the sample space for training `like`, i.e., the sample is only considered for `like` loss if `click=1`. Here is how you can set such sample space dependency among targets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93740d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_block = mm.ParallelBlock(\n",
    "  mm.BinaryOutput(\"click\"), \n",
    "  mm.BinaryOutput(\"like\", post=mm.ColumnBasedSampleWeight(\"click\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a4062",
   "metadata": {},
   "source": [
    "In such cases you might want to compute metrics for `like` considering only its sample space, rather than the entire space. The **`weighed_metrics`** can be used for that, as regular metrics are not influenced by sample weights.  \n",
    "We also demonstrate below how to override the default **`metrics`** per task. Metrics can be either Keras-like metrics or string aliases supported by Merlin Models (e.g., \"auc\", \"precision\", \"recall\", \"binary_accuracy\", \"rmse\", \"mse\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b8e1924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 15ms/step - loss: 1.0377 - click/binary_output_loss: 0.6932 - like/binary_output_loss: 0.3445 - click/binary_output/auc: 0.4999 - click/binary_output/weighted_auc: 0.4999 - like/binary_output/auc: 0.4999 - like/binary_output/weighted_auc: 0.5004 - regularization_loss: 0.0000e+00 - loss_batch: 1.0366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f8d5fa880>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mm.Model(\n",
    "    mm.InputBlockV2(schema),\n",
    "    mm.MLPBlock([32,16]),\n",
    "    output_block\n",
    ")\n",
    "\n",
    "metrics = {\n",
    "        \"click/binary_output\": [tf.keras.metrics.AUC(name=\"auc\", num_thresholds=200)],\n",
    "        \"like/binary_output\": [\"auc\"],\n",
    "    }\n",
    "\n",
    "weighted_metrics = {\n",
    "        \"click/binary_output\": [\"auc\"],\n",
    "        \"like/binary_output\": [\"auc\"],\n",
    "    }\n",
    "\n",
    "model.compile(optimizer=\"adam\", run_eagerly=False, metrics=metrics, weighted_metrics=weighted_metrics)\n",
    "model.fit(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902eec69",
   "metadata": {},
   "source": [
    "You can notice that when `weighted_metrics` are set we get the specified metrics prefixed by `weighted_`. The regular metrics for `like` (`auc`) differs from the weighted metrics (`weighted_auc`) because the latter are affected by sample weights, i.e., computed only for samples where `click=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8327b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 9ms/step - loss: 1.0427 - click/binary_output_loss: 0.6932 - like/binary_output_loss: 0.3495 - click/binary_output/auc: 0.5048 - click/binary_output/weighted_auc: 0.5048 - like/binary_output/auc: 0.4945 - like/binary_output/weighted_auc: 0.4920 - regularization_loss: 0.0000e+00 - loss_batch: 1.0407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.0427143573760986,\n",
       " 'click/binary_output_loss': 0.6931734085083008,\n",
       " 'like/binary_output_loss': 0.3495410084724426,\n",
       " 'click/binary_output/auc': 0.5047846436500549,\n",
       " 'click/binary_output/weighted_auc': 0.5047846436500549,\n",
       " 'like/binary_output/auc': 0.4945274591445923,\n",
       " 'like/binary_output/weighted_auc': 0.49199676513671875,\n",
       " 'regularization_loss': 0.0,\n",
       " 'loss_batch': 1.0317333936691284}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_ds, batch_size=BATCH_SIZE, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce01139",
   "metadata": {},
   "source": [
    "You can also cascade multiple sample weights for a target by using a `SequentialBlock`. For example, setting class weights for a binary target and using another binary column as the sample space. If there are multiple `ColumnBasedSampleWeight`, the sample weights are multiplied element-wise.\n",
    "```python\n",
    "mm.BinaryOutput(\"like\", \n",
    "                post=mm.SequentialBlock(\n",
    "                    [mm.ColumnBasedSampleWeight(binary_class_weights=(1.0, 5.0)),\n",
    "                     mm.ColumnBasedSampleWeight(\"click\")]\n",
    "                )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb83a6e",
   "metadata": {},
   "source": [
    "## Multi-task learning architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44517f",
   "metadata": {},
   "source": [
    "In this section we describe different architectures for multi-task learning, which are summarized in the following illustration. The blue shapes are the ones that are shared for all tasks, and the other colored shapes are task-specific ones. We explain each of those architectures in the next sub-sections.\n",
    "\n",
    "<img src=\"../images/mtl_architectures.png\"  width=\"90%\">\n",
    "\n",
    "Image adapted from: [Progressive Layered Extraction (PLE): A Novel Multi-Task\n",
    "Learning (MTL) Model for Personalized Recommendations](https://dl.acm.org/doi/10.1145/3383313.3412236)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9ba78",
   "metadata": {},
   "source": [
    "### Hard parameter sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0878e7",
   "metadata": {},
   "source": [
    "The examples above used a **hard parameter sharing**, where all tasks share MLP layers in the bottom, and each task has a specific single-layer MLP tower that projects the shared-bottom output to a single neuron per task (for binary classification / regression tasks).  \n",
    "We can specify more powerful task towers, so that tasks have more freedom to learn different things, with either of the following examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f55315b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_block = mm.OutputBlock(schema, task_blocks=mm.MLPBlock([32]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6aab4",
   "metadata": {},
   "source": [
    "or..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4196806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_block = mm.ParallelBlock(\n",
    "  mm.BinaryOutput(\"click\", pre=mm.MLPBlock([64])), \n",
    "  mm.BinaryOutput(\"like\",  pre=mm.MLPBlock([32]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47a1a5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 17ms/step - loss: 1.3863 - click/binary_output_loss: 0.6932 - like/binary_output_loss: 0.6932 - click/binary_output/precision: 0.4988 - click/binary_output/recall: 0.0759 - click/binary_output/binary_accuracy: 0.5029 - click/binary_output/auc: 0.4980 - like/binary_output/precision: 0.4979 - like/binary_output/recall: 0.2582 - like/binary_output/binary_accuracy: 0.4998 - like/binary_output/auc: 0.4976 - regularization_loss: 0.0000e+00 - loss_batch: 1.3863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f8cf7be20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mm.Model(\n",
    "    mm.InputBlockV2(schema),\n",
    "    mm.MLPBlock([32,16]),\n",
    "    output_block\n",
    ")\n",
    "model.compile(optimizer=\"adam\", run_eagerly=False)\n",
    "model.fit(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18f7f1",
   "metadata": {},
   "source": [
    "### MMoE architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f3d66",
   "metadata": {},
   "source": [
    "The [**Multi-gate Mixture-of-Experts (MMoE)**](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007) architecture was introduced in 2018 and is one of the most popular models for multi-task learning on tabular data. It is based on the former one-gate **Mixture of Experts (MoE)**, which proposed having different sub-networks (experts) projecting the inputs independently and then having the experts output weighted averaged by a gate for a shared representation to be used for all tasks. The MMoE architecture took a step further and proposed having an independent gate for each task, so that they could choose how better combine the experts outputs. You can find more details in the [MMoE paper](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007).\n",
    "\n",
    "The MMoE architecture can be created for your dataset with just a few lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97dfb313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (blocks): _TupleWrapper((ParallelBlock(\n",
      "    (_aggregation): ConcatFeatures()\n",
      "    (parallel_layers): Dict(\n",
      "      (categorical): ParallelBlock(\n",
      "        (parallel_layers): Dict(\n",
      "          (user_id): EmbeddingTable(\n",
      "            (features): Dict(\n",
      "              (user_id): ColumnSchema(name='user_id', tags={<Tags.ID: 'id'>, <Tags.USER_ID: 'user_id'>, <Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, properties={'freq_threshold': 0.0, 'num_buckets': None, 'start_index': 1.0, 'cat_path': './/categories/unique.user_id.parquet', 'embedding_sizes': {'cardinality': 2633851.0, 'dimension': 512.0}, 'max_size': 0.0, 'domain': {'min': 0, 'max': 100000, 'name': 'user_id'}}, dtype=dtype('int32'), is_list=False, is_ragged=False)\n",
      "            )\n",
      "            (table): Embedding()\n",
      "          )\n",
      "          (item_id): EmbeddingTable(\n",
      "            (features): Dict(\n",
      "              (item_id): ColumnSchema(name='item_id', tags={<Tags.ITEM_ID: 'item_id'>, <Tags.ID: 'id'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, properties={'cat_path': './/categories/unique.item_id.parquet', 'embedding_sizes': {'dimension': 512.0, 'cardinality': 179280.0}, 'max_size': 0.0, 'num_buckets': None, 'freq_threshold': 0.0, 'start_index': 1.0, 'domain': {'min': 0, 'max': 179280, 'name': 'item_id'}}, dtype=dtype('int32'), is_list=False, is_ragged=False)\n",
      "            )\n",
      "            (table): Embedding()\n",
      "          )\n",
      "          (video_category): EmbeddingTable(\n",
      "            (features): Dict(\n",
      "              (video_category): ColumnSchema(name='video_category', tags={<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, properties={'embedding_sizes': {'dimension': 16.0, 'cardinality': 5.0}, 'cat_path': './/categories/unique.video_category.parquet', 'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 1.0, 'domain': {'min': 0, 'max': 5, 'name': 'video_category'}}, dtype=dtype('int32'), is_list=False, is_ragged=False)\n",
      "            )\n",
      "            (table): Embedding()\n",
      "          )\n",
      "          (gender): EmbeddingTable(\n",
      "            (features): Dict(\n",
      "              (gender): ColumnSchema(name='gender', tags={<Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>}, properties={'max_size': 0.0, 'num_buckets': None, 'freq_threshold': 0.0, 'cat_path': './/categories/unique.gender.parquet', 'start_index': 1.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 5.0}, 'domain': {'min': 0, 'max': 5, 'name': 'gender'}}, dtype=dtype('int32'), is_list=False, is_ragged=False)\n",
      "            )\n",
      "            (table): Embedding()\n",
      "          )\n",
      "          (age): EmbeddingTable(\n",
      "            (features): Dict(\n",
      "              (age): ColumnSchema(name='age', tags={<Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>}, properties={'cat_path': './/categories/unique.age.parquet', 'start_index': 1.0, 'num_buckets': None, 'embedding_sizes': {'cardinality': 10.0, 'dimension': 16.0}, 'freq_threshold': 0.0, 'max_size': 0.0, 'domain': {'min': 0, 'max': 10, 'name': 'age'}}, dtype=dtype('int32'), is_list=False, is_ragged=False)\n",
      "            )\n",
      "            (table): Embedding()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  ), mmoe(\n",
      "    (layers): List(\n",
      "      (0): WithShortcut(\n",
      "        (parallel_layers): Dict(\n",
      "          (experts): ParallelBlock(\n",
      "            (_aggregation): StackFeatures()\n",
      "            (parallel_layers): Dict(\n",
      "              (expert_0): SequentialBlock(\n",
      "                (layers): List(\n",
      "                  (0): _Dense(\n",
      "                    (dense): Dense(16, activation=relu, use_bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (expert_1): SequentialBlock(\n",
      "                (layers): List(\n",
      "                  (0): _Dense(\n",
      "                    (dense): Dense(16, activation=relu, use_bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (expert_2): SequentialBlock(\n",
      "                (layers): List(\n",
      "                  (0): _Dense(\n",
      "                    (dense): Dense(16, activation=relu, use_bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (expert_3): SequentialBlock(\n",
      "                (layers): List(\n",
      "                  (0): _Dense(\n",
      "                    (dense): Dense(16, activation=relu, use_bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (shortcut): NoOp()\n",
      "        )\n",
      "      )\n",
      "      (1): ParallelBlock(\n",
      "        (parallel_layers): Dict(\n",
      "          (click/binary_output): ExpertsGate(\n",
      "            (gate_block): SequentialBlock(\n",
      "              (layers): List(\n",
      "                (0): _Dense(\n",
      "                  (dense): Dense(16, activation=relu, use_bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (gate_final): Dense(4, activation=linear, use_bias=False)\n",
      "          )\n",
      "          (follow/binary_output): ExpertsGate(\n",
      "            (gate_block): SequentialBlock(\n",
      "              (layers): List(\n",
      "                (0): _Dense(\n",
      "                  (dense): Dense(16, activation=relu, use_bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (gate_final): Dense(4, activation=linear, use_bias=False)\n",
      "          )\n",
      "          (like/binary_output): ExpertsGate(\n",
      "            (gate_block): SequentialBlock(\n",
      "              (layers): List(\n",
      "                (0): _Dense(\n",
      "                  (dense): Dense(16, activation=relu, use_bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (gate_final): Dense(4, activation=linear, use_bias=False)\n",
      "          )\n",
      "          (share/binary_output): ExpertsGate(\n",
      "            (gate_block): SequentialBlock(\n",
      "              (layers): List(\n",
      "                (0): _Dense(\n",
      "                  (dense): Dense(16, activation=relu, use_bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (gate_final): Dense(4, activation=linear, use_bias=False)\n",
      "          )\n",
      "          (watching_times/regression_output): ExpertsGate(\n",
      "            (gate_block): SequentialBlock(\n",
      "              (layers): List(\n",
      "                (0): _Dense(\n",
      "                  (dense): Dense(16, activation=relu, use_bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (gate_final): Dense(4, activation=linear, use_bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  ), ParallelBlock(\n",
      "    (parallel_layers): Dict(\n",
      "      (click/binary_output): BinaryOutput(\n",
      "        (to_call): Dense(1, activation=sigmoid, use_bias=True)\n",
      "        (pre): SequentialBlock(\n",
      "          (layers): List(\n",
      "            (0): _Dense(\n",
      "              (dense): Dense(16, activation=relu, use_bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (follow/binary_output): BinaryOutput(\n",
      "        (to_call): Dense(1, activation=sigmoid, use_bias=True)\n",
      "        (pre): SequentialBlock(\n",
      "          (layers): List(\n",
      "            (0): _Dense(\n",
      "              (dense): Dense(16, activation=relu, use_bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (like/binary_output): BinaryOutput(\n",
      "        (to_call): Dense(1, activation=sigmoid, use_bias=True)\n",
      "        (pre): SequentialBlock(\n",
      "          (layers): List(\n",
      "            (0): _Dense(\n",
      "              (dense): Dense(16, activation=relu, use_bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (share/binary_output): BinaryOutput(\n",
      "        (to_call): Dense(1, activation=sigmoid, use_bias=True)\n",
      "        (pre): SequentialBlock(\n",
      "          (layers): List(\n",
      "            (0): _Dense(\n",
      "              (dense): Dense(16, activation=relu, use_bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (watching_times/regression_output): RegressionOutput(\n",
      "        (to_call): Dense(1, activation=linear, use_bias=True)\n",
      "        (pre): SequentialBlock(\n",
      "          (layers): List(\n",
      "            (0): _Dense(\n",
      "              (dense): Dense(16, activation=relu, use_bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )))\n",
      "  (context): ModelContext()\n",
      "  (process_list): ProcessList()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "inputs = mm.InputBlockV2(schema)\n",
    "output_block = mm.OutputBlock(schema, task_blocks=mm.MLPBlock([16]))\n",
    "mmoe = mm.MMOEBlock(\n",
    "    output_block,\n",
    "    expert_block=mm.MLPBlock([16]),\n",
    "    num_experts=4,\n",
    "    gate_block=mm.MLPBlock([16]),\n",
    ")\n",
    "model = mm.Model(inputs, mmoe, output_block)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2389cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 4s 32ms/step - loss: 8.3211 - click/binary_output_loss: 0.6932 - follow/binary_output_loss: 0.6932 - like/binary_output_loss: 0.6933 - share/binary_output_loss: 0.6931 - watching_times/regression_output_loss: 5.5482 - click/binary_output/precision: 0.4969 - click/binary_output/recall: 0.9563 - click/binary_output/binary_accuracy: 0.4971 - click/binary_output/auc: 0.4990 - follow/binary_output/precision: 0.4900 - follow/binary_output/recall: 0.0294 - follow/binary_output/binary_accuracy: 0.4988 - follow/binary_output/auc: 0.4969 - like/binary_output/precision: 0.4985 - like/binary_output/recall: 0.9174 - like/binary_output/binary_accuracy: 0.4981 - like/binary_output/auc: 0.5004 - share/binary_output/precision: 0.5016 - share/binary_output/recall: 0.9582 - share/binary_output/binary_accuracy: 0.5011 - share/binary_output/auc: 0.5022 - watching_times/regression_output/root_mean_squared_error: 2.3555 - regularization_loss: 0.0000e+00 - loss_batch: 8.2859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f8dcf22b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False)\n",
    "model.fit(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c04d721",
   "metadata": {},
   "source": [
    "### CGC and PLE architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cda70f",
   "metadata": {},
   "source": [
    "The **CGC** and **PLE** architectures were introduced in this [paper](https://dl.acm.org/doi/10.1145/3383313.3412236) (2020). The authors observed that architectures like **MMoE** presented a \"seesaw\" phenomenon, where improving the accuracy of one task hurts the accuracy of other tasks.  \n",
    "So instead of having all tasks sharing all the experts, they proposed allowing for some task-specific experts and shared experts, which they named **Customized Gate Control (CGC) Model**, for which we provide a building block.   \n",
    "Notice that `CGCBlock` has separate arguments for `num_task_experts` and `num_shared_experts`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a94dc",
   "metadata": {},
   "source": [
    "#### CGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db017305",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mm.InputBlockV2(schema)\n",
    "output_block = mm.OutputBlock(schema, task_blocks=mm.MLPBlock([16]))\n",
    "\n",
    "cgc = mm.CGCBlock(\n",
    "    output_block,\n",
    "    expert_block=mm.MLPBlock([16]),\n",
    "    num_task_experts=2,\n",
    "    num_shared_experts=3,\n",
    "    schema=schema,\n",
    ")\n",
    "model = mm.Model(inputs, cgc, output_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "beb41d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 4s 34ms/step - loss: 8.3688 - click/binary_output_loss: 0.6931 - follow/binary_output_loss: 0.6932 - like/binary_output_loss: 0.6932 - share/binary_output_loss: 0.6932 - watching_times/regression_output_loss: 5.5961 - click/binary_output/precision: 0.5032 - click/binary_output/recall: 0.0556 - click/binary_output/binary_accuracy: 0.5034 - click/binary_output/auc: 0.4995 - follow/binary_output/precision: 0.5029 - follow/binary_output/recall: 0.0771 - follow/binary_output/binary_accuracy: 0.4999 - follow/binary_output/auc: 0.4985 - like/binary_output/precision: 0.4990 - like/binary_output/recall: 0.1770 - like/binary_output/binary_accuracy: 0.5005 - like/binary_output/auc: 0.4985 - share/binary_output/precision: 0.4992 - share/binary_output/recall: 0.4052 - share/binary_output/binary_accuracy: 0.4973 - share/binary_output/auc: 0.4979 - watching_times/regression_output/root_mean_squared_error: 2.3656 - regularization_loss: 0.0000e+00 - loss_batch: 8.3364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f7fddca60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False)\n",
    "model.fit(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe6f59",
   "metadata": {},
   "source": [
    "#### PLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4839e",
   "metadata": {},
   "source": [
    "Furthermore, the [paper](https://dl.acm.org/doi/10.1145/3383313.3412236) authors proposed stacking multiple **CGC** models on top of each other to form a multi-level MTL model, which they called **Progressive Layered Extraction (PLE)**. The `PLEBlock` introduces the `num_layers`, which controls the number of levels.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "292fc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mm.InputBlockV2(schema)\n",
    "output_block = mm.OutputBlock(schema, task_blocks=mm.MLPBlock([16]))\n",
    "\n",
    "ple = mm.PLEBlock(\n",
    "    num_layers=2,\n",
    "    outputs=output_block,\n",
    "    expert_block=mm.MLPBlock([16]),\n",
    "    num_task_experts=2,\n",
    "    num_shared_experts=1,\n",
    ")\n",
    "model = mm.Model(inputs, ple, output_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0e83b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 5s 42ms/step - loss: 8.3397 - click/binary_output_loss: 0.6931 - follow/binary_output_loss: 0.6932 - like/binary_output_loss: 0.6932 - share/binary_output_loss: 0.6932 - watching_times/regression_output_loss: 5.5671 - click/binary_output/precision: 0.4991 - click/binary_output/recall: 0.3590 - click/binary_output/binary_accuracy: 0.5024 - click/binary_output/auc: 0.4994 - follow/binary_output/precision: 0.5016 - follow/binary_output/recall: 0.6424 - follow/binary_output/binary_accuracy: 0.5016 - follow/binary_output/auc: 0.4984 - like/binary_output/precision: 0.4903 - like/binary_output/recall: 0.0132 - like/binary_output/binary_accuracy: 0.5006 - like/binary_output/auc: 0.4980 - share/binary_output/precision: 0.5017 - share/binary_output/recall: 0.3701 - share/binary_output/binary_accuracy: 0.4992 - share/binary_output/auc: 0.4978 - watching_times/regression_output/root_mean_squared_error: 2.3595 - regularization_loss: 0.0000e+00 - loss_batch: 8.3027    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f7f097f70>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False)\n",
    "model.fit(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14b446f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 21ms/step - loss: 7.7380 - click/binary_output_loss: 0.6932 - follow/binary_output_loss: 0.6932 - like/binary_output_loss: 0.6933 - share/binary_output_loss: 0.6932 - watching_times/regression_output_loss: 4.9651 - click/binary_output/precision: 0.0000e+00 - click/binary_output/recall: 0.0000e+00 - click/binary_output/binary_accuracy: 0.4962 - click/binary_output/auc: 0.4990 - follow/binary_output/precision: 0.4714 - follow/binary_output/recall: 0.0066 - follow/binary_output/binary_accuracy: 0.5020 - follow/binary_output/auc: 0.4962 - like/binary_output/precision: 0.5000 - like/binary_output/recall: 9.8834e-05 - like/binary_output/binary_accuracy: 0.4941 - like/binary_output/auc: 0.5012 - share/binary_output/precision: 0.4981 - share/binary_output/recall: 0.1472 - share/binary_output/binary_accuracy: 0.4995 - share/binary_output/auc: 0.4976 - watching_times/regression_output/root_mean_squared_error: 2.2283 - regularization_loss: 0.0000e+00 - loss_batch: 7.7428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 7.737986087799072,\n",
       " 'click/binary_output_loss': 0.6932438611984253,\n",
       " 'follow/binary_output_loss': 0.6931751370429993,\n",
       " 'like/binary_output_loss': 0.6932631731033325,\n",
       " 'share/binary_output_loss': 0.6931727528572083,\n",
       " 'watching_times/regression_output_loss': 4.965130805969238,\n",
       " 'click/binary_output/precision': 0.0,\n",
       " 'click/binary_output/recall': 0.0,\n",
       " 'click/binary_output/binary_accuracy': 0.49619999527931213,\n",
       " 'click/binary_output/auc': 0.49897563457489014,\n",
       " 'follow/binary_output/precision': 0.4714285731315613,\n",
       " 'follow/binary_output/recall': 0.00663116667419672,\n",
       " 'follow/binary_output/binary_accuracy': 0.5019500255584717,\n",
       " 'follow/binary_output/auc': 0.496232807636261,\n",
       " 'like/binary_output/precision': 0.5,\n",
       " 'like/binary_output/recall': 9.883376333164051e-05,\n",
       " 'like/binary_output/binary_accuracy': 0.49410000443458557,\n",
       " 'like/binary_output/auc': 0.5011581778526306,\n",
       " 'share/binary_output/precision': 0.49813875555992126,\n",
       " 'share/binary_output/recall': 0.14721472561359406,\n",
       " 'share/binary_output/binary_accuracy': 0.49950000643730164,\n",
       " 'share/binary_output/auc': 0.49763041734695435,\n",
       " 'watching_times/regression_output/root_mean_squared_error': 2.228257417678833,\n",
       " 'regularization_loss': 0.0,\n",
       " 'loss_batch': 7.7640790939331055}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_results = model.evaluate(valid_ds, batch_size=BATCH_SIZE, return_dict=True)\n",
    "metrics_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bcc7c8",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b0990",
   "metadata": {},
   "source": [
    "In this notebook we introduced multi-task learning use cases for ranking models and the building blocks provided by Merlin Models to build, train and evaluate such models.  \n",
    "You can see how easy it is to leverage state-of-the-art MTL architectures on your own dataset, with just a few lines of code with Merlin Models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-tensorflow:latest"
   ]
  },
  "vscode": {
   "interpreter": {
    "hash": "67b01b24cb2518309f0749863665ff82dad1ad60adc88cabbb59c99b73117545"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
