{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c3b27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/NVIDIA-Merlin/core\n",
      "   9d9b5c6a..c5c9bc25 release-23.04       -> origin/release-23.04\n",
      " * [new branch]      feature/merlin-array-dispatch -> origin/feature/merlin-array-dispatch\n",
      " * [new branch]      fix-repartition     -> origin/fix-repartition\n",
      " * [new branch]      fix-with-properties -> origin/fix-with-properties\n",
      " * [new branch]      gh-pages            -> origin/gh-pages\n",
      " * [new branch]      laiacano/docs-on-pr -> origin/laiacano/docs-on-pr\n",
      " * [new branch]      main                -> origin/main\n",
      " * [new branch]      release-22.10       -> origin/release-22.10\n",
      " * [new branch]      release-22.11       -> origin/release-22.11\n",
      " * [new branch]      release-22.12       -> origin/release-22.12\n",
      " * [new branch]      release-23.02       -> origin/release-23.02\n",
      " * [new branch]      revert-163-refactor/dictarray-columns -> origin/revert-163-refactor/dictarray-columns\n",
      " * [new branch]      stable              -> origin/stable\n",
      " * [new branch]      tags-intersection   -> origin/tags-intersection\n",
      " * [new branch]      v0.2.0-docs         -> origin/v0.2.0-docs\n",
      " * [new tag]         v0.10.0             -> v0.10.0\n",
      " * [new tag]         v0.8.0              -> v0.8.0\n",
      " * [new tag]         v0.9.0              -> v0.9.0\n",
      " * [new tag]         v23.02.01           -> v23.02.01\n",
      " * [new tag]           v0.1.0              -> v0.1.0\n",
      " * [new tag]           v0.1.1              -> v0.1.1\n",
      " * [new tag]           v0.2.0              -> v0.2.0\n",
      " * [new tag]           v0.3.0              -> v0.3.0\n",
      " * [new tag]           v0.4.0              -> v0.4.0\n",
      " * [new tag]           v0.5.0              -> v0.5.0\n",
      " * [new tag]           v0.6.0              -> v0.6.0\n",
      " * [new tag]           v0.7.0              -> v0.7.0\n",
      " * [new tag]           v23.02.00           -> v23.02.00\n",
      " * [new tag]           v23.05.dev0         -> v23.05.dev0\n",
      "Switched to a new branch 'main'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 'main' set up to track remote branch 'main' from 'origin'.\n",
      "Processing /core\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Building wheels for collected packages: merlin-core\n",
      "  Building wheel for merlin-core (PEP 517): started\n",
      "  Building wheel for merlin-core (PEP 517): finished with status 'done'\n",
      "  Created wheel for merlin-core: filename=merlin_core-23.5.dev0+28.ge1eaf269-py3-none-any.whl size=164561 sha256=44951aa099780e32b4cd02bc4d0748571a7ba9c437364e634b7da8f0ccc2fba2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1pkiripq/wheels/8f/da/8c/c779661788874afaa32fd10abeac6016635956e3bad9940584\n",
      "Successfully built merlin-core\n",
      "Installing collected packages: merlin-core\n",
      "  Attempting uninstall: merlin-core\n",
      "    Found existing installation: merlin-core 23.4.0\n",
      "    Uninstalling merlin-core-23.4.0:\n",
      "      Successfully uninstalled merlin-core-23.4.0\n",
      "Successfully installed merlin-core-23.5.dev0+28.ge1eaf269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/NVIDIA-Merlin/dataloader\n",
      " * [new branch]      chore/comprehensive-shapes -> origin/chore/comprehensive-shapes\n",
      " * [new branch]      chore/packages-action  -> origin/chore/packages-action\n",
      " * [new branch]      collabify_examples     -> origin/collabify_examples\n",
      " * [new branch]      docs-add-seo           -> origin/docs-add-seo\n",
      " * [new branch]      docs-calver-banner     -> origin/docs-calver-banner\n",
      " * [new branch]      ds-api                 -> origin/ds-api\n",
      " * [new branch]      embedding_op_with_padding -> origin/embedding_op_with_padding\n",
      " * [new branch]      feature/embedding-tags -> origin/feature/embedding-tags\n",
      " * [new branch]      fix-sparse-logic       -> origin/fix-sparse-logic\n",
      " * [new branch]      fix/tf-batch-size-warning -> origin/fix/tf-batch-size-warning\n",
      " * [new branch]      gh-pages               -> origin/gh-pages\n",
      " * [new branch]      gha-test               -> origin/gha-test\n",
      " * [new branch]      laiacano/docs-pr       -> origin/laiacano/docs-pr\n",
      " * [new branch]      main                   -> origin/main\n",
      " * [new branch]      no_gpu                 -> origin/no_gpu\n",
      " * [new branch]      release-22.11          -> origin/release-22.11\n",
      " * [new branch]      release-22.12          -> origin/release-22.12\n",
      " * [new branch]      release-23.02          -> origin/release-23.02\n",
      " * [new branch]      stable                 -> origin/stable\n",
      " * [new branch]      update_github_actions  -> origin/update_github_actions\n",
      " * [new tag]         v0.0.3                 -> v0.0.3\n",
      " * [new tag]         v0.0.4                 -> v0.0.4\n",
      " * [new tag]         v23.02.01              -> v23.02.01\n",
      " * [new tag]         v0.0.1                 -> v0.0.1\n",
      " * [new tag]         v0.0.2                 -> v0.0.2\n",
      " * [new tag]         v23.02.00              -> v23.02.00\n",
      " * [new tag]         v23.05.dev0            -> v23.05.dev0\n",
      "bash: line 6: gheckout: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /dataloader\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Building wheels for collected packages: merlin-dataloader\n",
      "  Building wheel for merlin-dataloader (PEP 517): started\n",
      "  Building wheel for merlin-dataloader (PEP 517): finished with status 'done'\n",
      "  Created wheel for merlin-dataloader: filename=merlin_dataloader-23.4.0-py3-none-any.whl size=34732 sha256=907e32a86e091ceba0309393eb6ccaaff913adbe0b56daa93bb1243234deafea\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xsaonaut/wheels/8c/19/5b/15dc04f5a977f6a7f73ed66c91996a687b1d9e3154a4765536\n",
      "Successfully built merlin-dataloader\n",
      "Installing collected packages: merlin-dataloader\n",
      "  Attempting uninstall: merlin-dataloader\n",
      "    Found existing installation: merlin-dataloader 23.4.0\n",
      "    Uninstalling merlin-dataloader-23.4.0:\n",
      "      Successfully uninstalled merlin-dataloader-23.4.0\n",
      "Successfully installed merlin-dataloader-23.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/NVIDIA-Merlin/NVTabular\n",
      "   f8f484e5..90489194 release-23.04           -> origin/release-23.04\n",
      " * [new branch]      1077-implement          -> origin/1077-implement\n",
      " * [new branch]      21.09/column-tagging    -> origin/21.09/column-tagging\n",
      " * [new branch]      21.09/dataset-collection -> origin/21.09/dataset-collection\n",
      " * [new branch]      21.09/operator-block    -> origin/21.09/operator-block\n",
      " * [new branch]      21.09/schema            -> origin/21.09/schema\n",
      " * [new branch]      add_sum_to_supported_aggregations -> origin/add_sum_to_supported_aggregations\n",
      " * [new branch]      aiobotocore_v2          -> origin/aiobotocore_v2\n",
      " * [new branch]      alexanderronquillo-patch-1 -> origin/alexanderronquillo-patch-1\n",
      " * [new branch]      atomize_adding_of_tags  -> origin/atomize_adding_of_tags\n",
      " * [new branch]      automate_pypi           -> origin/automate_pypi\n",
      " * [new branch]      bench-pynvml-fix        -> origin/bench-pynvml-fix\n",
      " * [new branch]      branch-0.6              -> origin/branch-0.6\n",
      " * [new branch]      bschifferer-remove_examples_1 -> origin/bschifferer-remove_examples_1\n",
      " * [new branch]      categorify-inference-int16 -> origin/categorify-inference-int16\n",
      " * [new branch]      columns_with_aggs_in_names -> origin/columns_with_aggs_in_names\n",
      " * [new branch]      conda-package-python-versions -> origin/conda-package-python-versions\n",
      " * [new branch]      conda_gh_action         -> origin/conda_gh_action\n",
      " * [new branch]      dataloader-remove-sparse -> origin/dataloader-remove-sparse\n",
      " * [new branch]      dataloader_doc_fix      -> origin/dataloader_doc_fix\n",
      " * [new branch]      disable-package-build-on-pull-requests -> origin/disable-package-build-on-pull-requests\n",
      " * [new branch]      dont_install_tests      -> origin/dont_install_tests\n",
      " * [new branch]      drop_low_cardinality    -> origin/drop_low_cardinality\n",
      " * [new branch]      fix-docs-tox-env        -> origin/fix-docs-tox-env\n",
      " * [new branch]      fix-wf-file             -> origin/fix-wf-file\n",
      " * [new branch]      fix/inference-deprecation -> origin/fix/inference-deprecation\n",
      " * [new branch]      fix_data_path           -> origin/fix_data_path\n",
      " * [new branch]      fix_hugectr_nb          -> origin/fix_hugectr_nb\n",
      " * [new branch]      fix_nbs                 -> origin/fix_nbs\n",
      " * [new branch]      gh-pages                -> origin/gh-pages\n",
      " * [new branch]      groupby_without_groupby_col_in_col_selector -> origin/groupby_without_groupby_col_in_col_selector\n",
      " * [new branch]      hugectr-newapi          -> origin/hugectr-newapi\n",
      " * [new branch]      laiacano/check-list-from-schema -> origin/laiacano/check-list-from-schema\n",
      " * [new branch]      laiacano/workflow-subgraph -> origin/laiacano/workflow-subgraph\n",
      " * [new branch]      main                    -> origin/main\n",
      " * [new branch]      na_sentinel             -> origin/na_sentinel\n",
      " * [new branch]      notebooks-21.10         -> origin/notebooks-21.10\n",
      " * [new branch]      nvt-1195                -> origin/nvt-1195\n",
      " * [new branch]      nvtabular_examples      -> origin/nvtabular_examples\n",
      " * [new branch]      packages-workflow-split -> origin/packages-workflow-split\n",
      " * [new branch]      readme_updates          -> origin/readme_updates\n",
      " * [new branch]      refactor/fit-schema     -> origin/refactor/fit-schema\n",
      " * [new branch]      refactor/input-column-selection -> origin/refactor/input-column-selection\n",
      " * [new branch]      refactor/postpone-schema-binding -> origin/refactor/postpone-schema-binding\n",
      " * [new branch]      release-22.10           -> origin/release-22.10\n",
      " * [new branch]      release-22.11           -> origin/release-22.11\n",
      " * [new branch]      release-22.12           -> origin/release-22.12\n",
      " * [new branch]      release-23.02           -> origin/release-23.02\n",
      " * [new branch]      remove_poetry           -> origin/remove_poetry\n",
      " * [new branch]      remove_release_notes    -> origin/remove_release_notes\n",
      " * [new branch]      repeat-ops              -> origin/repeat-ops\n",
      " * [new branch]      revert_atomize_tags     -> origin/revert_atomize_tags\n",
      " * [new branch]      rjzamora-simplify-criteo -> origin/rjzamora-simplify-criteo\n",
      " * [new branch]      rnyak-patch-1           -> origin/rnyak-patch-1\n",
      " * [new branch]      romeyn/input-api        -> origin/romeyn/input-api\n",
      " * [new branch]      stable                  -> origin/stable\n",
      " * [new branch]      test-column-similarity-dataset-cpu-default-none -> origin/test-column-similarity-dataset-cpu-default-none\n",
      " * [new branch]      test-torch-dataloader-dataset-cpu-default-none -> origin/test-torch-dataloader-dataset-cpu-default-none\n",
      " * [new branch]      torch_catch             -> origin/torch_catch\n",
      " * [new branch]      update-dask-reqs        -> origin/update-dask-reqs\n",
      " * [new branch]      update_merlin_core      -> origin/update_merlin_core\n",
      " * [new branch]      update_requirements     -> origin/update_requirements\n",
      " * [new branch]      v0.10.0-docs            -> origin/v0.10.0-docs\n",
      " * [new branch]      v0.11.0-docs            -> origin/v0.11.0-docs\n",
      " * [new branch]      v0.7.1-docs             -> origin/v0.7.1-docs\n",
      " * [new branch]      v0.8.0-docs             -> origin/v0.8.0-docs\n",
      " * [new branch]      v0.9.0-docs             -> origin/v0.9.0-docs\n",
      " * [new branch]      v1.0.0-docs             -> origin/v1.0.0-docs\n",
      " * [new tag]         v0.6.1                  -> v0.6.1\n",
      " * [new tag]         v1.6.0                  -> v1.6.0\n",
      " * [new tag]         v1.7.0                  -> v1.7.0\n",
      " * [new tag]         v1.8.1                  -> v1.8.1\n",
      " * [new tag]         v23.02.00               -> v23.02.00\n",
      " * [new tag]           v0.1.0                  -> v0.1.0\n",
      " * [new tag]           v0.1.1                  -> v0.1.1\n",
      " * [new tag]           v0.10.0                 -> v0.10.0\n",
      " * [new tag]           v0.11.0                 -> v0.11.0\n",
      " * [new tag]           v0.2.0                  -> v0.2.0\n",
      " * [new tag]           v0.3.0                  -> v0.3.0\n",
      " * [new tag]           v0.4.0                  -> v0.4.0\n",
      " * [new tag]           v0.5.0                  -> v0.5.0\n",
      " * [new tag]           v0.5.1                  -> v0.5.1\n",
      " * [new tag]           v0.5.2                  -> v0.5.2\n",
      " * [new tag]           v0.5.3                  -> v0.5.3\n",
      " * [new tag]           v0.6.0                  -> v0.6.0\n",
      " * [new tag]           v0.7.0                  -> v0.7.0\n",
      " * [new tag]           v0.7.1                  -> v0.7.1\n",
      " * [new tag]           v0.8.0                  -> v0.8.0\n",
      " * [new tag]           v0.9.0                  -> v0.9.0\n",
      " * [new tag]           v1.0.0                  -> v1.0.0\n",
      " * [new tag]           v1.1.0                  -> v1.1.0\n",
      " * [new tag]           v1.1.1                  -> v1.1.1\n",
      " * [new tag]           v1.2.0                  -> v1.2.0\n",
      " * [new tag]           v1.2.1                  -> v1.2.1\n",
      " * [new tag]           v1.2.2                  -> v1.2.2\n",
      " * [new tag]           v1.3.0                  -> v1.3.0\n",
      " * [new tag]           v1.3.1                  -> v1.3.1\n",
      " * [new tag]           v1.3.2                  -> v1.3.2\n",
      " * [new tag]           v1.3.3                  -> v1.3.3\n",
      " * [new tag]           v1.4.0                  -> v1.4.0\n",
      " * [new tag]           v1.5.0                  -> v1.5.0\n",
      " * [new tag]           v1.8.0                  -> v1.8.0\n",
      " * [new tag]           v23.05.dev0             -> v23.05.dev0\n",
      "Switched to a new branch 'main'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 'main' set up to track remote branch 'main' from 'origin'.\n",
      "Processing /nvtabular\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Building wheels for collected packages: nvtabular\n",
      "  Building wheel for nvtabular (PEP 517): started\n",
      "  Building wheel for nvtabular (PEP 517): finished with status 'done'\n",
      "  Created wheel for nvtabular: filename=nvtabular-23.5.dev0+10.g55fe9344-cp38-cp38-linux_x86_64.whl size=261197 sha256=8146d4d342e5048998f6e578ec2175824deac1a43c36464ffbc7879367000887\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-h4nasy3b/wheels/df/bf/c2/9cc2a62fe6da42038c26a9c0c4e25f9767093528b102fa30a2\n",
      "Successfully built nvtabular\n",
      "Installing collected packages: nvtabular\n",
      "  Attempting uninstall: nvtabular\n",
      "    Found existing installation: nvtabular 23.4.0\n",
      "    Uninstalling nvtabular-23.4.0:\n",
      "      Successfully uninstalled nvtabular-23.4.0\n",
      "Successfully installed nvtabular-23.5.dev0+10.g55fe9344\n",
      "Processing /workspace\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Building wheels for collected packages: merlin-models\n",
      "  Building wheel for merlin-models (PEP 517): started\n",
      "  Building wheel for merlin-models (PEP 517): finished with status 'done'\n",
      "  Created wheel for merlin-models: filename=merlin_models-23.5.dev0+29.g4628b00db.dirty-py3-none-any.whl size=395336 sha256=a45f9964aed1ecb6ac70eb687ae86f64727e9dacb3c856d33269578bd721d94a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lrkoe1bk/wheels/59/14/70/d94958f41745fe226f3bc60bb3cabbbc8a98e4d6679e91038a\n",
      "Successfully built merlin-models\n",
      "Installing collected packages: merlin-models\n",
      "  Attempting uninstall: merlin-models\n",
      "    Found existing installation: merlin-models 0+unknown\n",
      "    Can't uninstall 'merlin-models'. No files were found to uninstall.\n",
      "Successfully installed merlin-models-23.5.dev0+29.g4628b00db.dirty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/NVIDIA-Merlin/systems\n",
      "   fce949f..2516efb  release-23.04           -> origin/release-23.04\n",
      " * [new branch]      add_xgboost_serving_example -> origin/add_xgboost_serving_example\n",
      " * [new branch]      bschifferer-patch-1     -> origin/bschifferer-patch-1\n",
      " * [new branch]      bschifferer-patch-2     -> origin/bschifferer-patch-2\n",
      " * [new branch]      ci/cpu-action           -> origin/ci/cpu-action\n",
      " * [new branch]      dataset-cpu-default-None -> origin/dataset-cpu-default-None\n",
      " * [new branch]      docs-nightly-build      -> origin/docs-nightly-build\n",
      " * [new branch]      docs-remove-deps        -> origin/docs-remove-deps\n",
      " * [new branch]      docs-tox                -> origin/docs-tox\n",
      " * [new branch]      docs/contributing       -> origin/docs/contributing\n",
      " * [new branch]      docs/coverage-threshold -> origin/docs/coverage-threshold\n",
      " * [new branch]      docs/docstring-coverage -> origin/docs/docstring-coverage\n",
      " * [new branch]      docs/interrogate-cfg    -> origin/docs/interrogate-cfg\n",
      " * [new branch]      docs/interrogate-config -> origin/docs/interrogate-config\n",
      " * [new branch]      docs/issue-templates    -> origin/docs/issue-templates\n",
      " * [new branch]      docs/readme             -> origin/docs/readme\n",
      " * [new branch]      feast-errors            -> origin/feast-errors\n",
      " * [new branch]      feature/pytorch         -> origin/feature/pytorch\n",
      " * [new branch]      feature/t4r-serving     -> origin/feature/t4r-serving\n",
      " * [new branch]      feature/torchscript     -> origin/feature/torchscript\n",
      " * [new branch]      fix/dask-dist-deps      -> origin/fix/dask-dist-deps\n",
      " * [new branch]      fix/faiss-types         -> origin/fix/faiss-types\n",
      " * [new branch]      fix/multi-hot-dtypes    -> origin/fix/multi-hot-dtypes\n",
      " * [new branch]      fix/multihot-schemas    -> origin/fix/multihot-schemas\n",
      " * [new branch]      fix/pkg-build-lib       -> origin/fix/pkg-build-lib\n",
      " * [new branch]      fix/pytest-feast        -> origin/fix/pytest-feast\n",
      " * [new branch]      fix/skipped-tests       -> origin/fix/skipped-tests\n",
      " * [new branch]      fix/tf-input-shapes     -> origin/fix/tf-input-shapes\n",
      " * [new branch]      fix/torch-importorskip  -> origin/fix/torch-importorskip\n",
      " * [new branch]      fix_model_outputnames   -> origin/fix_model_outputnames\n",
      " * [new branch]      fix_nb                  -> origin/fix_nb\n",
      " * [new branch]      gh-pages                -> origin/gh-pages\n",
      " * [new branch]      laiacano/slack-notif    -> origin/laiacano/slack-notif\n",
      " * [new branch]      laiacano/transformer-import -> origin/laiacano/transformer-import\n",
      " * [new branch]      laiacano/upgrade-feast  -> origin/laiacano/upgrade-feast\n",
      " * [new branch]      main                    -> origin/main\n",
      " * [new branch]      merlin_models_xgboost   -> origin/merlin_models_xgboost\n",
      " * [new branch]      migration/from-nvt      -> origin/migration/from-nvt\n",
      " * [new branch]      polish/remove-dtype-matching -> origin/polish/remove-dtype-matching\n",
      " * [new branch]      radekosmulski-patch-1   -> origin/radekosmulski-patch-1\n",
      " * [new branch]      radekosmulski-patch-1-1 -> origin/radekosmulski-patch-1-1\n",
      " * [new branch]      refactor/dtypes         -> origin/refactor/dtypes\n",
      " * [new branch]      refactor/organize-tests -> origin/refactor/organize-tests\n",
      " * [new branch]      refactor/schema-validation-hook -> origin/refactor/schema-validation-hook\n",
      " * [new branch]      refactor/virtual-dataframe -> origin/refactor/virtual-dataframe\n",
      " * [new branch]      release-22.10           -> origin/release-22.10\n",
      " * [new branch]      release-22.11           -> origin/release-22.11\n",
      " * [new branch]      release-22.12           -> origin/release-22.12\n",
      " * [new branch]      release-23.02           -> origin/release-23.02\n",
      " * [new branch]      run_triton_utils        -> origin/run_triton_utils\n",
      " * [new branch]      stable                  -> origin/stable\n",
      " * [new branch]      update-reqs             -> origin/update-reqs\n",
      " * [new branch]      update/precommit-hooks  -> origin/update/precommit-hooks\n",
      " * [new branch]      use_dataloader          -> origin/use_dataloader\n",
      " * [new branch]      v0.0.1-docs             -> origin/v0.0.1-docs\n",
      " * [new branch]      v0.1.0-docs             -> origin/v0.1.0-docs\n",
      " * [new tag]         v0.7.0                  -> v0.7.0\n",
      " * [new tag]         v0.8.0                  -> v0.8.0\n",
      " * [new tag]         v0.9.0                  -> v0.9.0\n",
      " * [new tag]         v23.02.00               -> v23.02.00\n",
      " * [new tag]         v0.0.1                  -> v0.0.1\n",
      " * [new tag]         v0.1.0                  -> v0.1.0\n",
      " * [new tag]         v0.2.0                  -> v0.2.0\n",
      " * [new tag]         v0.3.0                  -> v0.3.0\n",
      " * [new tag]         v0.4.0                  -> v0.4.0\n",
      " * [new tag]         v0.5.0                  -> v0.5.0\n",
      " * [new tag]         v0.6.0                  -> v0.6.0\n",
      " * [new tag]         v23.05.dev0             -> v23.05.dev0\n",
      "Switched to a new branch 'main'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 'main' set up to track remote branch 'main' from 'origin'.\n",
      "Processing /systems\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Building wheels for collected packages: merlin-systems\n",
      "  Building wheel for merlin-systems (PEP 517): started\n",
      "  Building wheel for merlin-systems (PEP 517): finished with status 'done'\n",
      "  Created wheel for merlin-systems: filename=merlin_systems-23.5.dev0+8.g2b1b90b-py3-none-any.whl size=83188 sha256=e127906aec0187a70c7ef44aea6766a36ffb1ee13479a609a882dcb2cb46f560\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-atkacac6/wheels/1f/e9/71/1b0c6295aa7f4b37cb70292d96d87d9f38204674e6531bdda6\n",
      "Successfully built merlin-systems\n",
      "Installing collected packages: merlin-systems\n",
      "  Attempting uninstall: merlin-systems\n",
      "    Found existing installation: merlin-systems 23.4.0\n",
      "    Uninstalling merlin-systems-23.4.0:\n",
      "      Successfully uninstalled merlin-systems-23.4.0\n",
      "Successfully installed merlin-systems-23.5.dev0+8.g2b1b90b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/NVIDIA-Merlin/Transformers4Rec\n",
      "   4a9e7373..911355f4 release-23.04           -> origin/release-23.04\n",
      " * [new branch]      DDP_fix                 -> origin/DDP_fix\n",
      " * [new branch]      HF-update               -> origin/HF-update\n",
      " * [new branch]      add_benchmarking_scripts -> origin/add_benchmarking_scripts\n",
      " * [new branch]      add_topk_layer          -> origin/add_topk_layer\n",
      " * [new branch]      albert17-check          -> origin/albert17-check\n",
      " * [new branch]      batches                 -> origin/batches\n",
      " * [new branch]      benfred/datasetschema   -> origin/benfred/datasetschema\n",
      " * [new branch]      clean_rnn_block         -> origin/clean_rnn_block\n",
      " * [new branch]      core-schema/deprecation-warning -> origin/core-schema/deprecation-warning\n",
      " * [new branch]      core-schema/tabular-features -> origin/core-schema/tabular-features\n",
      " * [new branch]      core-schema/trainer     -> origin/core-schema/trainer\n",
      " * [new branch]      dataloader              -> origin/dataloader\n",
      " * [new branch]      dataparallel_fix        -> origin/dataparallel_fix\n",
      " * [new branch]      doc/supported_transformers -> origin/doc/supported_transformers\n",
      " * [new branch]      doc_fix                 -> origin/doc_fix\n",
      " * [new branch]      docs                    -> origin/docs\n",
      " * [new branch]      etl-nvt                 -> origin/etl-nvt\n",
      " * [new branch]      examples                -> origin/examples\n",
      " * [new branch]      fix-data-repartition    -> origin/fix-data-repartition\n",
      " * [new branch]      fix-failing-ci          -> origin/fix-failing-ci\n",
      " * [new branch]      fix-inference           -> origin/fix-inference\n",
      " * [new branch]      fix/transformers_config -> origin/fix/transformers_config\n",
      " * [new branch]      fix_gettingstarted_nb   -> origin/fix_gettingstarted_nb\n",
      " * [new branch]      fix_inference           -> origin/fix_inference\n",
      " * [new branch]      fix_nbs                 -> origin/fix_nbs\n",
      " * [new branch]      fix_oom_tests           -> origin/fix_oom_tests\n",
      " * [new branch]      fix_req_paper_repro     -> origin/fix_req_paper_repro\n",
      " * [new branch]      fix_stochastic          -> origin/fix_stochastic\n",
      " * [new branch]      fix_unit_test           -> origin/fix_unit_test\n",
      " * [new branch]      gh-pages                -> origin/gh-pages\n",
      " * [new branch]      github-templates        -> origin/github-templates\n",
      " * [new branch]      ignore-masking          -> origin/ignore-masking\n",
      " * [new branch]      laiacano/merlin-core-schema -> origin/laiacano/merlin-core-schema\n",
      " * [new branch]      laiacano/skip-ci-on-closed-pr -> origin/laiacano/skip-ci-on-closed-pr\n",
      " * [new branch]      license                 -> origin/license\n",
      " * [new branch]      main                    -> origin/main\n",
      " * [new branch]      masking_quick_fix       -> origin/masking_quick_fix\n",
      " * [new branch]      metric-names-prefix     -> origin/metric-names-prefix\n",
      " * [new branch]      model_save_load         -> origin/model_save_load\n",
      " * [new branch]      multi_gpu_doc           -> origin/multi_gpu_doc\n",
      " * [new branch]      multi_gpu_doc_fix       -> origin/multi_gpu_doc_fix\n",
      " * [new branch]      pin-hf-version          -> origin/pin-hf-version\n",
      " * [new branch]      post_fusion_context     -> origin/post_fusion_context\n",
      " * [new branch]      pretrained_embeddings_init -> origin/pretrained_embeddings_init\n",
      " * [new branch]      pretrained_module       -> origin/pretrained_module\n",
      " * [new branch]      pyt_serving             -> origin/pyt_serving\n",
      " * [new branch]      pytorch/item-id-aggregator -> origin/pytorch/item-id-aggregator\n",
      " * [new branch]      pytorch/label_smoothing -> origin/pytorch/label_smoothing\n",
      " * [new branch]      pytorch/model-and-heads -> origin/pytorch/model-and-heads\n",
      " * [new branch]      pytorch/model-updates   -> origin/pytorch/model-updates\n",
      " * [new branch]      read_schema_from_core   -> origin/read_schema_from_core\n",
      " * [new branch]      recsys22                -> origin/recsys22\n",
      " * [new branch]      refactor-prediction-task -> origin/refactor-prediction-task\n",
      " * [new branch]      refactor_part1          -> origin/refactor_part1\n",
      " * [new branch]      refactor_part2          -> origin/refactor_part2\n",
      " * [new branch]      release-22.10           -> origin/release-22.10\n",
      " * [new branch]      release-22.11           -> origin/release-22.11\n",
      " * [new branch]      release-22.12           -> origin/release-22.12\n",
      " * [new branch]      release-23.02           -> origin/release-23.02\n",
      " * [new branch]      release-jperez999       -> origin/release-jperez999\n",
      " * [new branch]      remove-custom-padding   -> origin/remove-custom-padding\n",
      " * [new branch]      remove_paper_assets     -> origin/remove_paper_assets\n",
      " * [new branch]      romeyn/dev              -> origin/romeyn/dev\n",
      " * [new branch]      romeyn/transformer-configs -> origin/romeyn/transformer-configs\n",
      " * [new branch]      save-schema-for-t4rec-model -> origin/save-schema-for-t4rec-model\n",
      " * [new branch]      schema-pbtxt-bug        -> origin/schema-pbtxt-bug\n",
      " * [new branch]      schema-shape-fix        -> origin/schema-shape-fix\n",
      " * [new branch]      seq_binary_classification -> origin/seq_binary_classification\n",
      " * [new branch]      serve_nvt_and__model    -> origin/serve_nvt_and__model\n",
      " * [new branch]      session_features        -> origin/session_features\n",
      " * [new branch]      slim_doc_deps           -> origin/slim_doc_deps\n",
      " * [new branch]      soft_embeddings         -> origin/soft_embeddings\n",
      " * [new branch]      ssn_seed                -> origin/ssn_seed\n",
      " * [new branch]      stable                  -> origin/stable\n",
      " * [new branch]      stochastic_noise        -> origin/stochastic_noise\n",
      " * [new branch]      stochastic_noise2       -> origin/stochastic_noise2\n",
      " * [new branch]      synthetic-data          -> origin/synthetic-data\n",
      " * [new branch]      t4rec-MM-repro          -> origin/t4rec-MM-repro\n",
      " * [new branch]      t4rec_paper_repro2      -> origin/t4rec_paper_repro2\n",
      " * [new branch]      t4rec_refactor          -> origin/t4rec_refactor\n",
      " * [new branch]      tensorflow              -> origin/tensorflow\n",
      " * [new branch]      test-data               -> origin/test-data\n",
      " * [new branch]      test/text_module        -> origin/test/text_module\n",
      " * [new branch]      testing/updates         -> origin/testing/updates\n",
      " * [new branch]      tf/example_notebook     -> origin/tf/example_notebook\n",
      " * [new branch]      tf/fix_compute_loss     -> origin/tf/fix_compute_loss\n",
      " * [new branch]      tf/fix_graph_mode       -> origin/tf/fix_graph_mode\n",
      " * [new branch]      tf/model_saving_and_loading -> origin/tf/model_saving_and_loading\n",
      " * [new branch]      tf/refactor_item_prediction_task -> origin/tf/refactor_item_prediction_task\n",
      " * [new branch]      tf/refactor_masking     -> origin/tf/refactor_masking\n",
      " * [new branch]      tf/refactor_ranking_metric -> origin/tf/refactor_ranking_metric\n",
      " * [new branch]      tf/refactor_transformer_block -> origin/tf/refactor_transformer_block\n",
      " * [new branch]      tf/save_load_model      -> origin/tf/save_load_model\n",
      " * [new branch]      tf/test-utils           -> origin/tf/test-utils\n",
      " * [new branch]      tf/to_tf_model          -> origin/tf/to_tf_model\n",
      " * [new branch]      torch/demo_utils        -> origin/torch/demo_utils\n",
      " * [new branch]      torch/fit_eval          -> origin/torch/fit_eval\n",
      " * [new branch]      torch/fix_evaluation    -> origin/torch/fix_evaluation\n",
      " * [new branch]      torch/fix_examples_utils -> origin/torch/fix_examples_utils\n",
      " * [new branch]      torch/fix_wipe_memory   -> origin/torch/fix_wipe_memory\n",
      " * [new branch]      torch/label_smoothing_loss -> origin/torch/label_smoothing_loss\n",
      " * [new branch]      torch/next_item_prediction -> origin/torch/next_item_prediction\n",
      " * [new branch]      torch/stochastic_swap_noise -> origin/torch/stochastic_swap_noise\n",
      " * [new branch]      trainer_predict_step    -> origin/trainer_predict_step\n",
      " * [new branch]      tutorial                -> origin/tutorial\n",
      " * [new branch]      unittest_endtoend_multi -> origin/unittest_endtoend_multi\n",
      " * [new branch]      update/torchmetrics     -> origin/update/torchmetrics\n",
      " * [new branch]      utils                   -> origin/utils\n",
      " * [new branch]      v0.1.2-docs             -> origin/v0.1.2-docs\n",
      " * [new branch]      v0.1.3-docs             -> origin/v0.1.3-docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * [new branch]      v0.1.4-docs             -> origin/v0.1.4-docs\n",
      " * [new branch]      v0.1.5-docs             -> origin/v0.1.5-docs\n",
      " * [new branch]      v0.1.6-docs             -> origin/v0.1.6-docs\n",
      " * [new branch]      v0.1.7-docs             -> origin/v0.1.7-docs\n",
      " * [new tag]         v0.1.14                 -> v0.1.14\n",
      " * [new tag]         v0.1.15                 -> v0.1.15\n",
      " * [new tag]         v0.1.16                 -> v0.1.16\n",
      " * [new tag]         v23.02.00               -> v23.02.00\n",
      " * [new tag]         v23.05.dev0             -> v23.05.dev0\n",
      " * [new tag]           custom_dataloader       -> custom_dataloader\n",
      " * [new tag]           v0.1.0                  -> v0.1.0\n",
      " * [new tag]           v0.1.1                  -> v0.1.1\n",
      " * [new tag]           v0.1.10                 -> v0.1.10\n",
      " * [new tag]           v0.1.11                 -> v0.1.11\n",
      " * [new tag]           v0.1.12                 -> v0.1.12\n",
      " * [new tag]           v0.1.13                 -> v0.1.13\n",
      " * [new tag]           v0.1.2                  -> v0.1.2\n",
      " * [new tag]           v0.1.3                  -> v0.1.3\n",
      " * [new tag]           v0.1.4                  -> v0.1.4\n",
      " * [new tag]           v0.1.5                  -> v0.1.5\n",
      " * [new tag]           v0.1.6                  -> v0.1.6\n",
      " * [new tag]           v0.1.7                  -> v0.1.7\n",
      " * [new tag]           v0.1.8                  -> v0.1.8\n",
      " * [new tag]           v0.1.9                  -> v0.1.9\n",
      "Switched to a new branch 'main'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 'main' set up to track remote branch 'main' from 'origin'.\n",
      "Processing /transformers4rec\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Building wheels for collected packages: transformers4rec\n",
      "  Building wheel for transformers4rec (PEP 517): started\n",
      "  Building wheel for transformers4rec (PEP 517): finished with status 'done'\n",
      "  Created wheel for transformers4rec: filename=transformers4rec-23.5.dev0+12.g2c9ab409-py3-none-any.whl size=481659 sha256=60160cb11c6d644e8178cdbe9a1f9fc3f4773bdb98599c537aea700b78d657ef\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-kqpb6x_c/wheels/24/44/e3/c29f7de8e7315585705f880ad32ffeae66fcaeb79003405ef6\n",
      "Successfully built transformers4rec\n",
      "Installing collected packages: transformers4rec\n",
      "  Attempting uninstall: transformers4rec\n",
      "    Found existing installation: transformers4rec 23.4.0\n",
      "    Uninstalling transformers4rec-23.4.0:\n",
      "      Successfully uninstalled transformers4rec-23.4.0\n",
      "Successfully installed transformers4rec-23.5.dev0+12.g2c9ab409\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /core\n",
    "git config remote.origin.fetch \"+refs/heads/*:refs/remotes/origin/*\" && git fetch && git checkout main\n",
    "pip install . --no-deps\n",
    "\n",
    "cd /dataloader\n",
    "git config remote.origin.fetch \"+refs/heads/*:refs/remotes/origin/*\" && git fetch && gheckout main\n",
    "pip install . --no-deps\n",
    "\n",
    "cd /nvtabular\n",
    "git config remote.origin.fetch \"+refs/heads/*:refs/remotes/origin/*\" && git fetch && git checkout main\n",
    "pip install . --no-deps\n",
    "\n",
    "cd /workspace\n",
    "pip install . --no-deps\n",
    "\n",
    "cd /systems\n",
    "git config remote.origin.fetch \"+refs/heads/*:refs/remotes/origin/*\" && git fetch && git checkout main\n",
    "pip install . --no-deps\n",
    "\n",
    "cd /transformers4rec\n",
    "git config remote.origin.fetch \"+refs/heads/*:refs/remotes/origin/*\" && git fetch && git checkout main\n",
    "pip install . --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a556f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions anda\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d1452",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_models_entertainment-with-pretrained-embeddings/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Training with pretrained embeddings\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this use case we will consider how we might train with pretrained embeddings.\n",
    "\n",
    "Pretrained embeddings can allow our model to include information from additional modalities (for instance, we might want to grab CNN descriptors of product images). They can also come from other models that we train on our data. For example, we might train a word2vec model on the sequence of purchased items by a customer and want to include this information in our retrieval or ranking model.\n",
    "\n",
    "The use cases are many, but this particular example will focus on the technical aspects of working with pretrained embeddings.\n",
    "\n",
    "We will use a synthetic version of the MovieLens 100k dataset and emulate a scenario where we would have a pretrained embedding for each of the movies in the dataset.\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Training with pretrained embeddings\n",
    "- Understanding [the Schema file](https://github.com/NVIDIA-Merlin/core/blob/main/merlin/schema/schema.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cccd005",
   "metadata": {},
   "source": [
    "## Downloading and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c63b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 01:29:04.794838: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 01:29:07.383103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 01:29:07.383461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 01:29:07.383586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: sparse_operation_kit is imported\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.1.4-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.1.4-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Initialize finished, communication tool: horovod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 01:29:08.980953: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-16 01:29:08.981898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 01:29:08.982082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 01:29:08.982207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 01:29:09.058061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 01:29:09.058244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 01:29:09.058374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 01:29:09.058477: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-05-16 01:29:09.058501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1621] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24576 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:08:00.0, compute capability: 7.5\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import merlin.models.tf as mm\n",
    "from merlin.schema.tags import Tags\n",
    "import tensorflow as tf\n",
    "from merlin.models.tf.blocks import *\n",
    "from merlin.datasets.synthetic import generate_data\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16e1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = generate_data('movielens-100k', num_rows=100_000)\n",
    "train.schema = train.schema.excluding_by_name([\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5400a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rating_binary'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = train.schema.select_by_name([\"rating_binary\"]).column_names[0]\n",
    "target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4446d9e1",
   "metadata": {},
   "source": [
    "# Passing the embeddings directly to our model using `TensorInitializer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd32f93",
   "metadata": {},
   "source": [
    "One way of passing the embeddings directly to our model is using the `TensorInitializer` as part of the `mm.Embeddings`.\n",
    "\n",
    "This is a straightforward method that works well with small embedding tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fdb65b",
   "metadata": {},
   "source": [
    "Let's begin by looking at the schema which holds vital information about our dataset. We can extract the embedding table size for the `moveId` column from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3955ab52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1680.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.schema['movieId'].properties['embedding_sizes']['cardinality']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60873da",
   "metadata": {},
   "source": [
    "From the schema, we can tell that the cardinality of `movieId` is 1680. Index 0 will be used in case an unknown `movieId` is encountered\n",
    "\n",
    "In order to accommodate this, we initialize our embedding table of dimensionality of (1681, 128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc919e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_movie_embs = np.random.random((1681, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac0920fb-98fb-4881-a997-7fd55d21523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = train.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5beff74",
   "metadata": {},
   "source": [
    "This is only a mock up embedding table. In reality, this is where we would pass our embeddings from another model.\n",
    "\n",
    "The dimensionality of each embedding, that of 128, is arbitrary. We could have specified some other value here, though generally multiples of 8 tend to work well.\n",
    "\n",
    "We need to update the schema properties of our `movieId` column since we will not be using the default embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50409872-45dd-41ff-bd44-58687d0d91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema['movieId'].properties['embedding_sizes'] = {\n",
    "    'cardinality': float(pretrained_movie_embs.shape[0]), \n",
    "    'dimension': float(pretrained_movie_embs.shape[1])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a911f3-6613-401a-8d6a-f1be5fd2f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.schema = schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d462880-3aa9-46d4-91fd-9b79dcd49d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>movieId</td>\n",
       "      <td>(Tags.ITEM, Tags.CATEGORICAL, Tags.ID)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1681.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>.//categories/unique.movieId.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>movieId</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>userId</td>\n",
       "      <td>(Tags.ID, Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>.//categories/unique.userId.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>userId</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>genres</td>\n",
       "      <td>(Tags.ITEM, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>.//categories/unique.genres.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>genres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TE_movieId_rating</td>\n",
       "      <td>(Tags.CONTINUOUS)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>userId_count</td>\n",
       "      <td>(Tags.CONTINUOUS)</td>\n",
       "      <td>DType(name='float32', element_type=&lt;ElementTyp...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gender</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>.//categories/unique.gender.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zip_code</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>795.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>.//categories/unique.zip_code.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>795.0</td>\n",
       "      <td>zip_code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rating</td>\n",
       "      <td>(Tags.TARGET, Tags.REGRESSION)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rating_binary</td>\n",
       "      <td>(Tags.BINARY_CLASSIFICATION, Tags.TARGET)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>age</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>.//categories/unique.age.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'movieId', 'tags': {<Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ID: 'id'>}, 'properties': {'freq_threshold': 0.0, 'max_size': 0.0, 'embedding_sizes': {'cardinality': 1681.0, 'dimension': 128.0}, 'cat_path': './/categories/unique.movieId.parquet', 'num_buckets': None, 'start_index': 0.0, 'domain': {'min': 1, 'max': 1680, 'name': 'movieId'}}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'userId', 'tags': {<Tags.ID: 'id'>, <Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'embedding_sizes': {'cardinality': 943.0, 'dimension': 74.0}, 'num_buckets': None, 'max_size': 0.0, 'start_index': 0.0, 'freq_threshold': 0.0, 'cat_path': './/categories/unique.userId.parquet', 'domain': {'min': 0, 'max': 943, 'name': 'userId'}}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'genres', 'tags': {<Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'max_size': 0.0, 'start_index': 0.0, 'num_buckets': None, 'freq_threshold': 0.0, 'embedding_sizes': {'cardinality': 216.0, 'dimension': 32.0}, 'cat_path': './/categories/unique.genres.parquet', 'domain': {'min': 0, 'max': 216, 'name': 'genres'}}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'TE_movieId_rating', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'userId_count', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {}, 'dtype': DType(name='float32', element_type=<ElementType.Float: 'float'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'gender', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'max_size': 0.0, 'freq_threshold': 0.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 2.0}, 'num_buckets': None, 'start_index': 0.0, 'cat_path': './/categories/unique.gender.parquet', 'domain': {'min': 0, 'max': 2, 'name': 'gender'}}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'zip_code', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'embedding_sizes': {'cardinality': 795.0, 'dimension': 67.0}, 'freq_threshold': 0.0, 'max_size': 0.0, 'cat_path': './/categories/unique.zip_code.parquet', 'start_index': 0.0, 'num_buckets': None, 'domain': {'min': 0, 'max': 795, 'name': 'zip_code'}}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'rating', 'tags': {<Tags.TARGET: 'target'>, <Tags.REGRESSION: 'regression'>}, 'properties': {}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'rating_binary', 'tags': {<Tags.BINARY_CLASSIFICATION: 'binary_classification'>, <Tags.TARGET: 'target'>}, 'properties': {}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'age', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'embedding_sizes': {'dimension': 16.0, 'cardinality': 8.0}, 'max_size': 0.0, 'start_index': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.age.parquet', 'freq_threshold': 0.0, 'domain': {'min': 0, 'max': 8, 'name': 'age'}}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575b14b",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536b88b",
   "metadata": {},
   "source": [
    "We now have everything we need to construct a model and train on our custom embeddings. In order to do so, we will leverage the `TensorInitializer` class and `Embeddings` function to set the `trainable` arg to `False`, so that our pre-trained embedddings will be frozen and not be updated during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d8df690-5d0d-413a-b5dc-7124a10378d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dims = {}\n",
    "embed_dims[\"movieId\"] = pretrained_movie_embs.shape[1]\n",
    "\n",
    "embeddings_init={\n",
    "    \"movieId\": mm.TensorInitializer(pretrained_movie_embs),\n",
    "}\n",
    "\n",
    "embeddings_block = mm.Embeddings(\n",
    "    train.schema.select_by_tag(Tags.CATEGORICAL),\n",
    "    infer_embedding_sizes=True,\n",
    "    embeddings_initializer=embeddings_init,\n",
    "    trainable={'movieId': False},\n",
    "    dim=embed_dims,\n",
    ")\n",
    "input_block = mm.InputBlockV2(train.schema, categorical=embeddings_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deeb44f-ac18-4d27-836d-838c7cd1ebca",
   "metadata": {},
   "source": [
    "Let us now feed our input_block into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22af1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DCNModel(\n",
    "    train.schema,\n",
    "    depth=2,\n",
    "    input_block=input_block,\n",
    "    deep_block=mm.MLPBlock([64, 32]),\n",
    "    prediction_tasks=mm.BinaryOutput(target_column)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bdd67d",
   "metadata": {},
   "source": [
    "We could have created the model without passing the `embeddings_block` to the `input_block`. The model would still be able to infer how to construct itself (what should be the dimensionality of the input layer and so on) from the information contained in the schema.\n",
    "\n",
    "However, passing a `TensorInitializer` into the constructor of the `input_block` tells our model to use our embedding table (`pretrained_movie_embs`) for that particular column of our dataset (`movieId`) as opposed to the model randomly initializing a brand new embedding matrix. For categorical columns we do not provide this information, the model will go with the standard initialization logic, which is to create an embedding table of appropriate size and perform random preinitialization.\n",
    "\n",
    "Additionally, we set the `trainable` parameter for our pre-trained embeddings to `False` to ensure the embeddings will not be modified during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0df47",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cdb0c1",
   "metadata": {},
   "source": [
    "We train our model with `AUC` as our metric.\n",
    "\n",
    "As we use synthetic data, the AUC score will not improve significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b96fc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 ms, sys: 661 s, total: 14.6 ms\n",
      "Wall time: 13.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=1e-1)\n",
    "model.compile(optimizer=opt, run_eagerly=False, metrics=[tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9d78213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 01:29:20.836517: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x5aaab040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-16 01:29:20.836557: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Quadro RTX 8000, Compute Capability 7.5\n",
      "2023-05-16 01:29:20.841893: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-16 01:29:20.975759: I tensorflow/compiler/jit/xla_compilation_cache.cc:480] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 12s 73ms/step - loss: 0.6949 - auc: 0.4987 - regularization_loss: 0.0000e+00 - loss_batch: 0.6949\n",
      "Epoch 2/5\n",
      "98/98 [==============================] - 2s 21ms/step - loss: 0.6933 - auc: 0.5038 - regularization_loss: 0.0000e+00 - loss_batch: 0.6934\n",
      "Epoch 3/5\n",
      "98/98 [==============================] - 1s 11ms/step - loss: 0.6932 - auc: 0.5048 - regularization_loss: 0.0000e+00 - loss_batch: 0.6932\n",
      "Epoch 4/5\n",
      "98/98 [==============================] - 1s 9ms/step - loss: 0.6931 - auc: 0.5068 - regularization_loss: 0.0000e+00 - loss_batch: 0.6931\n",
      "Epoch 5/5\n",
      "98/98 [==============================] - 1s 9ms/step - loss: 0.6932 - auc: 0.5061 - regularization_loss: 0.0000e+00 - loss_batch: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f32d72b7250>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, batch_size=1024, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d6e47aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b0506",
   "metadata": {},
   "source": [
    "# Passing the `EmbeddingOperator` to the `Loader`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24718e1",
   "metadata": {},
   "source": [
    "Another way of training with pretrained embeddings is to create a custom `Loader` and equip it with the ability to feed embeddings to our model.\n",
    "\n",
    "When we use `mm.Embeddings` and `TensorInitializer` as above, the embeddings are moved to the GPU and can be considered part of our model. That might become problematic if the embedding table is large.\n",
    "\n",
    "Taking the approach below the pretrained embeddings are passed to the model as part of each batch. We do not hold the embedding table, which depending on the scenario might consists of millions of rows, in GPU memory.\n",
    "\n",
    "We can reuse the train data and the embedding information we generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b651eb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1681, 128)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_movie_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e158b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.schema = train.schema.without('rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86a347e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.dataloader.ops.embeddings import EmbeddingOperator\n",
    "\n",
    "loader = mm.Loader(\n",
    "    train,\n",
    "    batch_size=1024,\n",
    "    transforms=[\n",
    "        EmbeddingOperator(\n",
    "            pretrained_movie_embs,\n",
    "            lookup_key=\"movieId\",\n",
    "            embedding_name=\"pretrained_movie_embeddings\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92451b84",
   "metadata": {},
   "source": [
    "But we need to recreate the model, as now the  emebeddings will be passed to it straight from the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68f8b5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "      <th>properties.value_count.min</th>\n",
       "      <th>properties.value_count.max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>movieId</td>\n",
       "      <td>(Tags.ITEM, Tags.CATEGORICAL, Tags.ID)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1681.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>.//categories/unique.movieId.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>movieId</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>userId</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.USER, Tags.ID)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>.//categories/unique.userId.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>userId</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>genres</td>\n",
       "      <td>(Tags.ITEM, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>.//categories/unique.genres.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>genres</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TE_movieId_rating</td>\n",
       "      <td>(Tags.CONTINUOUS)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>userId_count</td>\n",
       "      <td>(Tags.CONTINUOUS)</td>\n",
       "      <td>DType(name='float32', element_type=&lt;ElementTyp...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gender</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>.//categories/unique.gender.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>gender</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zip_code</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>795.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>.//categories/unique.zip_code.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>795.0</td>\n",
       "      <td>zip_code</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rating_binary</td>\n",
       "      <td>(Tags.BINARY_CLASSIFICATION, Tags.TARGET)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>age</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int32', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>.//categories/unique.age.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>age</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pretrained_movie_embeddings</td>\n",
       "      <td>(Tags.EMBEDDING)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'movieId', 'tags': {<Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ID: 'id'>}, 'properties': {'freq_threshold': 0.0, 'max_size': 0.0, 'embedding_sizes': {'cardinality': 1681.0, 'dimension': 128.0}, 'cat_path': './/categories/unique.movieId.parquet', 'num_buckets': None, 'start_index': 0.0, 'domain': {'min': 1, 'max': 1680, 'name': 'movieId'}}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'userId', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>, <Tags.ID: 'id'>}, 'properties': {'embedding_sizes': {'cardinality': 943.0, 'dimension': 74.0}, 'num_buckets': None, 'max_size': 0.0, 'start_index': 0.0, 'freq_threshold': 0.0, 'cat_path': './/categories/unique.userId.parquet', 'domain': {'min': 0, 'max': 943, 'name': 'userId'}}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'genres', 'tags': {<Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'max_size': 0.0, 'start_index': 0.0, 'num_buckets': None, 'freq_threshold': 0.0, 'embedding_sizes': {'cardinality': 216.0, 'dimension': 32.0}, 'cat_path': './/categories/unique.genres.parquet', 'domain': {'min': 0, 'max': 216, 'name': 'genres'}}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'TE_movieId_rating', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'userId_count', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {}, 'dtype': DType(name='float32', element_type=<ElementType.Float: 'float'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'gender', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'max_size': 0.0, 'freq_threshold': 0.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 2.0}, 'num_buckets': None, 'start_index': 0.0, 'cat_path': './/categories/unique.gender.parquet', 'domain': {'min': 0, 'max': 2, 'name': 'gender'}}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'zip_code', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'embedding_sizes': {'cardinality': 795.0, 'dimension': 67.0}, 'freq_threshold': 0.0, 'max_size': 0.0, 'cat_path': './/categories/unique.zip_code.parquet', 'start_index': 0.0, 'num_buckets': None, 'domain': {'min': 0, 'max': 795, 'name': 'zip_code'}}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'rating_binary', 'tags': {<Tags.BINARY_CLASSIFICATION: 'binary_classification'>, <Tags.TARGET: 'target'>}, 'properties': {}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'age', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'embedding_sizes': {'dimension': 16.0, 'cardinality': 8.0}, 'max_size': 0.0, 'start_index': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.age.parquet', 'freq_threshold': 0.0, 'domain': {'min': 0, 'max': 8, 'name': 'age'}}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}, {'name': 'pretrained_movie_embeddings', 'tags': {<Tags.EMBEDDING: 'embedding'>}, 'properties': {'value_count': {'min': 128, 'max': 128}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=128, max=128)))), 'is_list': True, 'is_ragged': False}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.output_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ae075a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_block = mm.Embeddings(\n",
    "    loader.output_schema.select_by_tag(Tags.CATEGORICAL).remove_col('movieId'),\n",
    ")\n",
    "\n",
    "pretrained_embeddings = mm.PretrainedEmbeddings(\n",
    "    loader.output_schema.select_by_tag(Tags.EMBEDDING)\n",
    ")\n",
    "\n",
    "input_block = mm.InputBlockV2(loader.output_schema, categorical=embeddings_block, pretrained_embeddings=pretrained_embeddings)\n",
    "\n",
    "model = mm.DCNModel(\n",
    "    loader.output_schema,\n",
    "    depth=2,\n",
    "    input_block=input_block,\n",
    "    deep_block=mm.MLPBlock([64, 32]),\n",
    "    prediction_tasks=mm.BinaryOutput(target_column)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a55d0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0eb9aaa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rating_binary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/models/base.py:1377\u001b[0m, in \u001b[0;36mBaseModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, train_metrics_steps, pre, **kwargs)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pre, SequenceTransform):\n\u001b[1;32m   1375\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pre\u001b[38;5;241m.\u001b[39mconfigure_for_train()\n\u001b[0;32m-> 1377\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre:\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pre\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dataloader/tensorflow.py:93\u001b[0m, in \u001b[0;36mLoader.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets batch at position `index`.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Note: This returns the next batch in the iterator.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m          don't currently support fetching a batch by index.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dataloader/tensorflow.py:97\u001b[0m, in \u001b[0;36mLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the next batch from the dataloader\"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     converted_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_batch(\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m map_fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_fns:\n\u001b[1;32m     99\u001b[0m         converted_batch \u001b[38;5;241m=\u001b[39m map_fn(\u001b[38;5;241m*\u001b[39mconverted_batch)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dataloader/loader_base.py:261\u001b[0m, in \u001b[0;36mLoaderBase.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the next batch.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dataloader/loader_base.py:332\u001b[0m, in \u001b[0;36mLoaderBase._get_next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# try to iterate through existing batches\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_itr)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;66;03m# anticipate any more chunks getting created\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;66;03m# if not, raise the StopIteration\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_working \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buff\u001b[38;5;241m.\u001b[39mempty:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dataloader/loader_base.py:416\u001b[0m, in \u001b[0;36mLoaderBase.make_tensors\u001b[0;34m(self, gdf, use_row_lengths)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m         batch[tensor_key] \u001b[38;5;241m=\u001b[39m tensor_value[batch_idx]\n\u001b[0;32m--> 416\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nvtx/nvtx.py:101\u001b[0m, in \u001b[0;36mannotate.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    100\u001b[0m     libnvtx_push_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m--> 101\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     libnvtx_pop_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dataloader/loader_base.py:578\u001b[0m, in \u001b[0;36mLoaderBase._process_batch\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    575\u001b[0m         labels[label] \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mpop(label)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m--> 578\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensorTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, labels\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py:85\u001b[0m, in \u001b[0;36mLocalExecutor.transform\u001b[0;34m(self, transformable, graph, output_dtypes, additional_columns, capture_dtypes, strict)\u001b[0m\n\u001b[1;32m     83\u001b[0m output_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n\u001b[0;32m---> 85\u001b[0m     transformed_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_dtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     output_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_node_outputs(node, transformed_data, output_data)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# If there are any additional columns that weren't produced by one of the supplied nodes\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# we grab them directly from the supplied input data. Normally this would happen on a\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# per-node basis, but offers a safety net for the multi-node case\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py:99\u001b[0m, in \u001b[0;36mLocalExecutor._execute_node\u001b[0;34m(self, node, transformable, capture_dtypes, strict)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, node, transformable, capture_dtypes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 99\u001b[0m     upstream_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_upstream_transforms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_dtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     upstream_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_addl_root_columns(node, transformable, upstream_outputs)\n\u001b[1;32m    103\u001b[0m     formatted_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_standardize_formats(node, upstream_columns)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py:112\u001b[0m, in \u001b[0;36mLocalExecutor._run_upstream_transforms\u001b[0;34m(self, node, transformable, capture_dtypes, strict)\u001b[0m\n\u001b[1;32m    109\u001b[0m upstream_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m upstream_node \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mparents_with_dependencies:\n\u001b[0;32m--> 112\u001b[0m     node_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupstream_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node_output) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    119\u001b[0m         upstream_outputs\u001b[38;5;241m.\u001b[39mappend(node_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py:102\u001b[0m, in \u001b[0;36mLocalExecutor._execute_node\u001b[0;34m(self, node, transformable, capture_dtypes, strict)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, node, transformable, capture_dtypes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     99\u001b[0m     upstream_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_upstream_transforms(\n\u001b[1;32m    100\u001b[0m         node, transformable, capture_dtypes, strict\n\u001b[1;32m    101\u001b[0m     )\n\u001b[0;32m--> 102\u001b[0m     upstream_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append_addl_root_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupstream_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     formatted_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_standardize_formats(node, upstream_columns)\n\u001b[1;32m    104\u001b[0m     transform_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_upstream_columns(formatted_columns)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py:136\u001b[0m, in \u001b[0;36mLocalExecutor._append_addl_root_columns\u001b[0;34m(self, node, transformable, upstream_outputs)\u001b[0m\n\u001b[1;32m    133\u001b[0m root_columns \u001b[38;5;241m=\u001b[39m node_input_cols\u001b[38;5;241m.\u001b[39munion(addl_input_cols) \u001b[38;5;241m-\u001b[39m already_present\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m root_columns:\n\u001b[0;32m--> 136\u001b[0m     upstream_outputs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtransformable\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot_columns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m upstream_outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/table/tensor_table.py:148\u001b[0m, in \u001b[0;36mTensorTable.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m TensorTable({k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m key})\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns[key]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/table/tensor_table.py:148\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m TensorTable({k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_columns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m key})\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rating_binary'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=1e-1)\n",
    "model.compile(optimizer=opt, run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "model.fit(loader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d25bcb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_with_embeddings = model.history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20876414",
   "metadata": {},
   "source": [
    "The model trains using pretrained embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
