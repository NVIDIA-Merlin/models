{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a556f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions anda\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d1452",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_models_entertainment-with-pretrained-embeddings/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Training with pretrained embeddings\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this use case we will consider how we might train with pretrained embeddings.\n",
    "\n",
    "Pretrained embeddings can allow our model to include information from additional modalities (for instance, we might want to grab CNN descriptors of product images). They can also come from other models that we train on our data. For example, we might train a word2vec model on the sequence of purchased items by a customer and want to include this information in our retrieval or ranking model.\n",
    "\n",
    "The use cases are many, but this particular example will focus on the technical aspects of working with pretrained embeddings.\n",
    "\n",
    "We will use a synthetic version of the MovieLens 100k dataset and emulate a scenario where we would have a pretrained embedding for each of the movies in the dataset.\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Training with pretrained embeddings\n",
    "- Understanding [the Schema file](https://github.com/NVIDIA-Merlin/core/blob/stable/merlin/schema/schema.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cccd005",
   "metadata": {},
   "source": [
    "## Downloading and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c63b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import merlin.models.tf as mm\n",
    "from merlin.schema.tags import Tags\n",
    "import tensorflow as tf\n",
    "from merlin.models.tf.blocks import *\n",
    "from merlin.datasets.synthetic import generate_data\n",
    "from merlin.dataloader.ops.embeddings import EmbeddingOperator\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e16e1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = generate_data('movielens-100k', num_rows=100_000)\n",
    "train.schema = train.schema.excluding_by_name([\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5400a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = train.schema.select_by_name([\"rating_binary\"]).column_names[0]\n",
    "target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4446d9e1",
   "metadata": {},
   "source": [
    "# Passing the embeddings directly to our model using `TensorInitializer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd32f93",
   "metadata": {},
   "source": [
    "One way of passing the embeddings directly to our model is using the `TensorInitializer` as part of the `mm.Embeddings`.\n",
    "\n",
    "This is a straightforward method that works well with small embedding tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fdb65b",
   "metadata": {},
   "source": [
    "Let's begin by looking at the schema which holds vital information about our dataset. We can extract the embedding table size for the `moveId` column from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3955ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.schema['movieId'].properties['embedding_sizes']['cardinality']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60873da",
   "metadata": {},
   "source": [
    "From the schema, we can tell that the cardinality of `movieId` is 1680. Index 0 will be used in case an unknown `movieId` is encountered\n",
    "\n",
    "In order to accommodate this, we initialize our embedding table of dimensionality of (1681, 128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc919e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_movie_embs = np.random.random((1681, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac0920fb-98fb-4881-a997-7fd55d21523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = train.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5beff74",
   "metadata": {},
   "source": [
    "This is only a mock up embedding table. In reality, this is where we would pass our embeddings from another model.\n",
    "\n",
    "The dimensionality of each embedding, that of 128, is arbitrary. We could have specified some other value here, though generally multiples of 8 tend to work well.\n",
    "\n",
    "We need to update the schema properties of our `movieId` column since we will not be using the default embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50409872-45dd-41ff-bd44-58687d0d91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema['movieId'].properties['embedding_sizes'] = {\n",
    "    'cardinality': float(pretrained_movie_embs.shape[0]), \n",
    "    'dimension': float(pretrained_movie_embs.shape[1])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41a911f3-6613-401a-8d6a-f1be5fd2f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.schema = schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d462880-3aa9-46d4-91fd-9b79dcd49d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575b14b",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536b88b",
   "metadata": {},
   "source": [
    "We now have everything we need to construct a model and train on our custom embeddings. In order to do so, we will leverage the `TensorInitializer` class and `Embeddings` function to set the `trainable` arg to `False`, so that our pre-trained embedddings will be frozen and not be updated during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d8df690-5d0d-413a-b5dc-7124a10378d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dims = {}\n",
    "embed_dims[\"movieId\"] = pretrained_movie_embs.shape[1]\n",
    "\n",
    "embeddings_init={\n",
    "    \"movieId\": mm.TensorInitializer(pretrained_movie_embs),\n",
    "}\n",
    "\n",
    "embeddings_block = mm.Embeddings(\n",
    "    train.schema.select_by_tag(Tags.CATEGORICAL),\n",
    "    infer_embedding_sizes=True,\n",
    "    embeddings_initializer=embeddings_init,\n",
    "    trainable={'movieId': False},\n",
    "    dim=embed_dims,\n",
    ")\n",
    "input_block = mm.InputBlockV2(train.schema, categorical=embeddings_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deeb44f-ac18-4d27-836d-838c7cd1ebca",
   "metadata": {},
   "source": [
    "Let us now feed our input_block into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22af1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DCNModel(\n",
    "    train.schema,\n",
    "    depth=2,\n",
    "    input_block=input_block,\n",
    "    deep_block=mm.MLPBlock([64, 32]),\n",
    "    prediction_tasks=mm.BinaryOutput(target_column)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bdd67d",
   "metadata": {},
   "source": [
    "We could have created the model without passing the `embeddings_block` to the `input_block`. The model would still be able to infer how to construct itself (what should be the dimensionality of the input layer and so on) from the information contained in the schema.\n",
    "\n",
    "However, passing a `TensorInitializer` into the constructor of the `input_block` tells our model to use our embedding table (`pretrained_movie_embs`) for that particular column of our dataset (`movieId`) as opposed to the model randomly initializing a brand new embedding matrix. For categorical columns we do not provide this information, the model will go with the standard initialization logic, which is to create an embedding table of appropriate size and perform random preinitialization.\n",
    "\n",
    "Additionally, we set the `trainable` parameter for our pre-trained embeddings to `False` to ensure the embeddings will not be modified during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0df47",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cdb0c1",
   "metadata": {},
   "source": [
    "We train our model with `AUC` as our metric.\n",
    "\n",
    "As we use synthetic data, the AUC score will not improve significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b96fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "opt = tf.keras.optimizers.legacy.Adagrad(learning_rate=1e-1)\n",
    "model.compile(optimizer=opt, run_eagerly=False, metrics=[tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9d78213",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, batch_size=1024, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d6e47aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b0506",
   "metadata": {},
   "source": [
    "# Passing the `EmbeddingOperator` to the `Loader`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24718e1",
   "metadata": {},
   "source": [
    "Another way of training with pretrained embeddings is to create a custom `Loader` and equip it with the ability to feed embeddings to our model.\n",
    "\n",
    "When we use `mm.Embeddings` and `TensorInitializer` as above, the embeddings are moved to the GPU and can be considered part of our model. That might become problematic if the embedding table is large.\n",
    "\n",
    "Taking the approach below the pretrained embeddings are passed to the model as part of each batch. We do not hold the embedding table in GPU memory, which depending on the scenario might consists of millions of rows.\n",
    "\n",
    "We can reuse the train data and the embedding information we generated above.\n",
    "\n",
    "Just as a reminder, these are the dimensions of our embeddings array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b651eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_movie_embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313409ce",
   "metadata": {},
   "source": [
    "We instantiate the `Loader` passing in the `EmbeddingOperator`. It encapsulates the recipe how to link the pretrained embeddings that we pass in (`pretrained_movie_embs`) to the data we provide as `train` (the association of records will be performed on the `movieId` column -- when a row with `movieId` of 10 is encountered, `pretrained_movie_embs[10]` will be feed along with that row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86a347e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = mm.Loader(\n",
    "    train,\n",
    "    batch_size=1024,\n",
    "    transforms=[\n",
    "        EmbeddingOperator(\n",
    "            pretrained_movie_embs,\n",
    "            lookup_key=\"movieId\",\n",
    "            embedding_name=\"pretrained_movie_embeddings\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409e5ed",
   "metadata": {},
   "source": [
    "Let us take a look at an example of a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69fd68a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f117a",
   "metadata": {},
   "source": [
    "We can seethe `pretrained_movie_embeddings` included in the batch and being represented as a `float64` `tf.Tensor`.\n",
    "\n",
    "Next, we need to recreate the model, to indicate that the embeddings will be contained in the batch. We do so by providing a value for the `pretrained_embeddings` in `InputBlockV2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ae075a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_block = mm.Embeddings(\n",
    "    loader.output_schema.select_by_tag(Tags.CATEGORICAL).remove_col('movieId'),\n",
    ")\n",
    "\n",
    "pretrained_embeddings = mm.PretrainedEmbeddings(\n",
    "    loader.output_schema.select_by_tag(Tags.EMBEDDING)\n",
    ")\n",
    "\n",
    "input_block = mm.InputBlockV2(loader.output_schema, categorical=embeddings_block, pretrained_embeddings=pretrained_embeddings)\n",
    "\n",
    "model = mm.DCNModel(\n",
    "    loader.output_schema,\n",
    "    depth=2,\n",
    "    input_block=input_block,\n",
    "    deep_block=mm.MLPBlock([64, 32]),\n",
    "    prediction_tasks=mm.BinaryOutput(target_column)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a55d0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a19a4a",
   "metadata": {},
   "source": [
    "Let us train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eb9aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=1e-1)\n",
    "model.compile(optimizer=opt, run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "model.fit(loader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d25bcb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_with_embeddings = model.history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20876414",
   "metadata": {},
   "source": [
    "The model trains using pretrained embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
