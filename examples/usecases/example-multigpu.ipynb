{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeccd4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = '0'\n",
    "\n",
    "import os, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1f9b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-29 08:33:41.622310: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-29 08:33:43.730202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16255 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from nvtabular.loader.tf_utils import configure_tensorflow\n",
    "\n",
    "configure_tensorflow()\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform, save_results\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a255dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.datasets.synthetic import generate_data\n",
    "\n",
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/raid/data/\")\n",
    "\n",
    "NUM_ROWS = os.environ.get(\"NUM_ROWS\", 10000000)\n",
    "SYNTHETIC_DATA = eval(os.environ.get(\"SYNTHETIC_DATA\", \"True\"))\n",
    "\n",
    "if SYNTHETIC_DATA:\n",
    "    train, valid = generate_data(\"aliccp-raw\", int(NUM_ROWS), set_sizes=(0.7, 0.3))\n",
    "    # save the datasets as parquet files\n",
    "    train.to_ddf().to_parquet(os.path.join(DATA_FOLDER, \"train\"))\n",
    "    valid.to_ddf().to_parquet(os.path.join(DATA_FOLDER, \"valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36513b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(DATA_FOLDER, \"train\", \"*.parquet\")\n",
    "valid_path = os.path.join(DATA_FOLDER, \"valid\", \"*.parquet\")\n",
    "output_path = os.path.join(DATA_FOLDER, \"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15732be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "user_id = [\"user_id\"] >> Categorify(freq_threshold=5) >> TagAsUserID()\n",
    "item_id = [\"item_id\"] >> Categorify(freq_threshold=5) >> TagAsItemID()\n",
    "add_feat = [\n",
    "    \"user_item_categories\",\n",
    "    \"user_item_shops\",\n",
    "    \"user_item_brands\",\n",
    "    \"user_item_intentions\",\n",
    "    \"item_category\",\n",
    "    \"item_shop\",\n",
    "    \"item_brand\",\n",
    "] >> Categorify()\n",
    "\n",
    "te_feat = (\n",
    "    [\"user_id\", \"item_id\"] + add_feat\n",
    "    >> TargetEncoding([\"click\"], kfold=1, p_smooth=20)\n",
    "    >> Normalize()\n",
    ")\n",
    "\n",
    "targets = [\"click\"] >> AddMetadata(tags=[Tags.BINARY_CLASSIFICATION, \"target\"])\n",
    "\n",
    "outputs = user_id + item_id + targets + add_feat + te_feat\n",
    "\n",
    "# Remove rows where item_id==0 and user_id==0\n",
    "outputs = outputs >> Filter(f=lambda df: (df[\"item_id\"] != 0) & (df[\"user_id\"] != 0))\n",
    "\n",
    "workflow_fit_transform(outputs, train_path, valid_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89053986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid/data/processed'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07eac1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tf_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './tf_trainer.py'\n",
    "\n",
    "import os, time\n",
    "MPI_SIZE = int(os.getenv(\"OMPI_COMM_WORLD_SIZE\"))\n",
    "MPI_RANK = int(os.getenv(\"OMPI_COMM_WORLD_RANK\"))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(MPI_RANK)\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from nvtabular.loader.tf_utils import configure_tensorflow\n",
    "\n",
    "configure_tensorflow()\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform, save_results\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow as hvd\n",
    "\n",
    "import cupy\n",
    "\n",
    "hvd.init()\n",
    "\n",
    "# Seed with system randomness (or a static seed)\n",
    "cupy.random.seed(None)\n",
    "\n",
    "def seed_fn():\n",
    "    \"\"\"\n",
    "    Generate consistent dataloader shuffle seeds across workers\n",
    "\n",
    "    Reseeds each worker's dataloader each epoch to get fresh a shuffle\n",
    "    that's consistent across workers.\n",
    "    \"\"\"\n",
    "    min_int, max_int = tf.int32.limits\n",
    "    max_rand = max_int // hvd.size()\n",
    "\n",
    "    # Generate a seed fragment on each worker\n",
    "    seed_fragment = cupy.random.randint(0, max_rand).get()\n",
    "\n",
    "    # Aggregate seed fragments from all Horovod workers\n",
    "    seed_tensor = tf.constant(seed_fragment)\n",
    "    reduced_seed = hvd.allreduce(seed_tensor, name=\"shuffle_seed\", op=hvd.mpi_ops.Sum)\n",
    "\n",
    "    return reduced_seed % max_rand\n",
    "\n",
    "\n",
    "output_path = '/raid/data/processed'\n",
    "\n",
    "train = Dataset(\n",
    "    os.path.join(output_path, \"train\", \"*.parquet\"), \n",
    "    part_size=\"500MB\"\n",
    ")\n",
    "\n",
    "batch_size = 16 * 1024\n",
    "LR = 0.03\n",
    "\n",
    "print(hvd.size())\n",
    "print(hvd.rank())\n",
    "\n",
    "import merlin.models.tf.dataset as tf_dataloader\n",
    "train_dl = tf_dataloader.BatchedDataset(\n",
    "    train,\n",
    "    batch_size = batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    global_size=2,\n",
    "    global_rank=hvd.rank(),\n",
    "    seed_fn=seed_fn\n",
    ")\n",
    "print(len(train_dl))\n",
    "\n",
    "# define schema object\n",
    "schema = train.schema\n",
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "target_column\n",
    "\n",
    "\n",
    "\n",
    "model = mm.DLRMModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column),\n",
    ")\n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=LR)\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "model.compile(\n",
    "    optimizer=opt, \n",
    "    run_eagerly=False, \n",
    "    metrics=[tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "callbacks = [hvd.keras.callbacks.BroadcastGlobalVariablesCallback(0)]\n",
    "\n",
    "verbose = 1 if hvd.rank() == 0 else 0\n",
    "\n",
    "model.fit(\n",
    "    train, \n",
    "    batch_size=batch_size,\n",
    "    verbose=verbose,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8539375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,0]<stderr>:2022-09-29 08:55:37.501434: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "[1,0]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[1,1]<stderr>:2022-09-29 08:55:37.567914: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "[1,1]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[1,0]<stderr>:2022-09-29 08:55:39.506393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16255 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "[1,1]<stderr>:2022-09-29 08:55:39.600653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16255 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "[1,0]<stdout>:2\n",
      "[1,0]<stdout>:0\n",
      "[1,1]<stdout>:2\n",
      "[1,1]<stdout>:1\n",
      "[1,0]<stdout>:322\n",
      "[1,1]<stdout>:104\n",
      "  5/428 [..............................] - ETA: 10s - loss: 0.6962 - auc: 0.4494 - regularization_loss: 0.0000e+00[1,1]<stderr>:/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "[1,1]<stderr>:  warnings.warn(\n",
      "[1,1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0246s vs `on_train_batch_end` time: 0.1852s). Check your callbacks.\n",
      "[1,0]<stderr>:/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "[1,0]<stderr>:  warnings.warn(\n",
      "[1,0]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0220s vs `on_train_batch_end` time: 0.1877s). Check your callbacks.\n",
      "428/428 [==============================] - 24s 25ms/step - loss: 0.6595 - auc: 0.6470 - regularization_loss: 0.0000e+00stdout>[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>\n"
     ]
    }
   ],
   "source": [
    "!horovodrun -np 2 python './tf_trainer.py' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be754e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
