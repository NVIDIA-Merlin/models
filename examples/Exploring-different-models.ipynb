{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa480ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888076a",
   "metadata": {},
   "source": [
    "## Building Recommender Systems Easily with Merlin Models\n",
    "\n",
    "In this example notebook, we use the `Ali-CCP: Alibaba Click and Conversion Prediction` dataset to build our recommender system models. In order to download the dataset visit [here](https://tianchi.aliyun.com/dataset/dataDetail?dataId=408#1). We have curated the raw dataset via this [script]() and generated the parquet files that we will use in this example.\n",
    "\n",
    "We define different deep learning-based model architectures, and then train and evaluate the models only in couple lines of code.\n",
    "\n",
    "### Learning objectives\n",
    "- Preparing the data with NVTabular\n",
    "- Training different deep learning-based recommender models with Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f354e2a",
   "metadata": {},
   "source": [
    "## Feature Engineering with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eadd76d",
   "metadata": {},
   "source": [
    "When we work on a new recommender systems, we explore the dataset, first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c4b1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:32:17.079946: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:214] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-03-16 15:32:17.080101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "import cudf\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "from example_utils import workflow_fit_transform\n",
    "from example_utils import save_results\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.schema import Schema\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "import merlin.models.tf.dataset as tf_dataloader\n",
    "\n",
    "from merlin.io.dataset import Dataset\n",
    "from merlin.schema.io.tensorflow_metadata import TensorflowMetadata\n",
    "from merlin.models.tf.blocks.core.aggregation import CosineSimilarity\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39ad9e",
   "metadata": {},
   "source": [
    "First, we define our input and output paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e0eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/workspace/data/\"\n",
    "train_path = os.path.join(DATA_FOLDER, 'train/*.parquet')\n",
    "test_path = os.path.join(DATA_FOLDER, 'test/*.parquet')\n",
    "output_path = '/workspace/data/processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe79531",
   "metadata": {},
   "source": [
    "NVTabular has already implemented multiple calculations, called `ops`. An `op` can be applied to a `ColumnGroup` from an overloaded `>>` operator\n",
    "**Example:**<br>\n",
    "```python\n",
    "features = [ column_name, ...] >> op1 >> op2 >> ...\n",
    "```\n",
    "\n",
    "This may sounds more complicated as it is. Let's define our the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992ca87",
   "metadata": {},
   "source": [
    "Our dataset has only categorical features. Below, we create continuous features using Target Encoding (TE) technique. Target Encoding calculates the statistics from a target variable grouped by the unique values of one or more categorical features. For example in a binary classification problem, it calculates the conditional probability that the target is true for each category value- a simple mean. To learn more about TE, visit [here](https://medium.com/rapids-ai/target-encoding-with-rapids-cuml-do-more-with-your-categorical-data-8c762c79e784)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562912a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:1253: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.4 s, sys: 31.4 s, total: 54.8 s\n",
      "Wall time: 57.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "user_id = [\"user_id\"] >> Categorify(freq_threshold=5) >> AddMetadata(tags=[Tags.USER_ID])\n",
    "item_id = [\"item_id\"] >> Categorify(freq_threshold=5) >> AddMetadata(tags=[Tags.ITEM_ID])\n",
    "targets = [\"click\"] >> AddMetadata(tags=[str(Tags.BINARY_CLASSIFICATION), \"target\"])\n",
    "\n",
    "add_feat = [\"user_item_categories\", \"user_item_shops\", \"user_item_brands\", \"user_item_intentions\",\"item_category\", \"item_shop\", \"item_brand\"] >> nvt.ops.Categorify()\n",
    "\n",
    "te_feat = (\n",
    "    [\"user_id\", \"item_id\"] + add_feat >>\n",
    "    TargetEncoding(\n",
    "        ['click'],\n",
    "        kfold=1,\n",
    "        p_smooth=20\n",
    "    ) >>\n",
    "    Normalize()\n",
    ")\n",
    "\n",
    "outputs = user_id+item_id+targets+add_feat+te_feat\n",
    "\n",
    "workflow_fit_transform(outputs, train_path, test_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005dfc17",
   "metadata": {},
   "source": [
    "We will also use a util function to wrap up the workflow transform to a one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ffa117",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47713372",
   "metadata": {},
   "source": [
    "NVTabular exported the schema file of our processed dataset. Merlin Models library relies on a schema object that takes the input features as input and automatically builds all necessary layers to represent, normalize and aggregate input features. `schema.pbtxt` is a protobuf text file contains features metadata, including statistics about features such as cardinality, min and max values and also tags based on their characteristics and dtypes (e.g., categorical, continuous, list, item_id). The metadata information loaded from Schema and their tags are used to automatically set the parameters of Merlin models.\n",
    "\n",
    "We use the `schema` object to define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1965f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = TensorflowMetadata.from_proto_text_file(output_path + '/train/').to_merlin_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7513ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'click'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e964791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'item_id',\n",
       " 'click',\n",
       " 'user_item_categories',\n",
       " 'user_item_shops',\n",
       " 'user_item_brands',\n",
       " 'user_item_intentions',\n",
       " 'item_category',\n",
       " 'item_shop',\n",
       " 'item_brand',\n",
       " 'TE_user_item_categories_click',\n",
       " 'TE_user_item_shops_click',\n",
       " 'TE_user_item_brands_click',\n",
       " 'TE_user_item_intentions_click',\n",
       " 'TE_item_category_click',\n",
       " 'TE_item_shop_click',\n",
       " 'TE_item_brand_click',\n",
       " 'TE_user_id_click',\n",
       " 'TE_item_id_click']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4e2ee",
   "metadata": {},
   "source": [
    "### Initialize Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f04211",
   "metadata": {},
   "source": [
    "We're ready to start training, for that, we need to initialize the dataloaders. We'll use Merlin `BatchedDataset` class for reading chunks of parquet files. `BatchedDataset` asynchronously iterate through CSV or Parquet dataframes on GPU by leveraging an NVTabular `Dataset`. To read more about Merlin optimized dataloaders visit [here](https://github.com/NVIDIA-Merlin/models/blob/main/merlin/models/tf/dataset.py#L141)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f8b3a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:1253: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16*1024\n",
    "LR=0.003\n",
    "\n",
    "train_dl = tf_dataloader.BatchedDataset(\n",
    "    Dataset(output_path + '/train/*.parquet', part_size=\"500MB\"),\n",
    "    batch_size = batch_size,\n",
    "    shuffle= True,\n",
    "    schema = schema,\n",
    ")\n",
    "\n",
    "test_dl = tf_dataloader.BatchedDataset(\n",
    "    Dataset(output_path + '/test/*.parquet', part_size=\"500MB\"),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    schema = schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ffde0",
   "metadata": {},
   "source": [
    "### MLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef3211",
   "metadata": {},
   "source": [
    "<img src=\"./images/mlp.png\"  width=\"30%\">\n",
    "We will first build a Multi-Layer Percepton (MLP) model. MLP models feed categorical features into embedding layer, concat the embedding outputs and add multiple hidden layers. Note that the `Ali-CCP` dataset has `click` and `conversion` target columns but in this example, we only focus on building different ranking models with binary target column `click`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "808290a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses default embedding_dim = 64\n",
    "model = mm.MLPBlock([64, 32]).to_model(\n",
    "    schema, \n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column, metrics=[tf.keras.metrics.AUC()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eca6a39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:33:15.830973: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442/2442 [==============================] - ETA: 0s - auc: 0.6242 - loss: 0.1966 - regularization_loss: 0.0000e+00 - total_loss: 0.1966 ETA"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:34:28.885503: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/then/_0/cond/cond/branch_executed/_140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442/2442 [==============================] - 122s 46ms/step - auc: 0.6242 - loss: 0.1965 - regularization_loss: 0.0000e+00 - total_loss: 0.1965 - val_auc: 0.5687 - val_loss: 0.1290 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.1290\n",
      "CPU times: user 4min 16s, sys: 41.1 s, total: 4min 57s\n",
      "Wall time: 2min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0dc84c5ac0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=LR)\n",
    "\n",
    "model.compile(optimizer=opt, run_eagerly=False)\n",
    "model.fit(train_dl, validation_data=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18674d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results('MLP', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13fa9e2",
   "metadata": {},
   "source": [
    "### NCF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067938f",
   "metadata": {},
   "source": [
    "Neural Collaborative Filtering [(NCF)](https://arxiv.org/pdf/1708.05031.pdf) Model  architecture explores neural network architectures for collaborative filtering, in other words explores the use of deep neural networks for learning the interaction function from data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66656dcd",
   "metadata": {},
   "source": [
    "<img src=\"./images/ncf.png\"  width=\"30%\">\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1708.05031.pdf\">Image Source: NCF paper</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ffb9b",
   "metadata": {},
   "source": [
    "We will change the model to a Neural Collaborative Filtering (NCF). NCF feed categorical features into embedding layer, concat the embedding outputs and add multiple hidden layers via its MLP layer tower as seen in the figure. GMF and MLP share the same embedding layer, and then outputs of their interaction functions are concatenated.\n",
    "\n",
    "Steps:<br>\n",
    "* Change the model to `NCFModel`\n",
    "* Rerun the pipeline from there from model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "addd9fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.benchmark.NCFModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    mlp_block=mm.MLPBlock([128, 64]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column, metrics=[tf.keras.metrics.AUC()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80559898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442/2442 [==============================] - ETA: 0s - auc_1: 0.5382 - loss: 0.2215 - regularization_loss: 0.0000e+00 - total_loss: 0.2215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:36:22.066168: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/then/_0/cond/cond/branch_executed/_125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442/2442 [==============================] - 106s 42ms/step - auc_1: 0.5382 - loss: 0.2215 - regularization_loss: 0.0000e+00 - total_loss: 0.2215 - val_auc_1: 0.4960 - val_loss: 0.1241 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.1241\n",
      "CPU times: user 3min 8s, sys: 27.2 s, total: 3min 36s\n",
      "Wall time: 1min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0d6ea4e490>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=LR)\n",
    "model.compile(optimizer=opt, run_eagerly=False)\n",
    "model.fit(train_dl, validation_data=test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72dd64d",
   "metadata": {},
   "source": [
    "Let's save our accuracy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db01edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results('NCF', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96875a77",
   "metadata": {},
   "source": [
    "### DLRM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d684ef27",
   "metadata": {},
   "source": [
    "Deep Learning Recommendation Model [(DLRM)](https://arxiv.org/abs/1906.00091) architecture is a popular neural network model originally proposed by Facebook in 2019. The model was introduced as a personalization deep learning model that uses embeddings to process sparse features that represent categorical data and a multilayer perceptron (MLP) to process dense features, then interacts these features explicitly using the statistical techniques proposed in [here](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5694074).\n",
    "\n",
    "![DLRM](./images/DLRM.png)\n",
    "\n",
    "\n",
    "DLRM accepts two types of features: categorical and numerical. \n",
    "- For each categorical feature, an embedding table is used to provide dense representation to each unique value. \n",
    "- For numerical features, they are fed to model as dense features, and then transformed by a simple neural network referred to as \"bottom MLP\". This part of the network consists of a series of linear layers with ReLU activations. \n",
    "- The output of the bottom MLP and the embedding vectors are then fed into the dot product interaction operation (see Pairwise interaction step). The output of \"dot interaction\" is then concatenated with the features resulting from the bottom MLP (we apply a skip-connection there) and fed into the \"top MLP\" which is also a series of dense layers with activations ((a fully connected NN). \n",
    "- The model outputs a single number (here we use sigmoid function to generate probabilities) which can be interpreted as a likelihood of a certain user clicking on an ad, watching a movie, or viewing a news page.\n",
    "\n",
    "\n",
    "Steps:<br>\n",
    "* Change the model to `DLRMModel`\n",
    "* Rerun the pipeline from there from model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e02eeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DLRMModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column, metrics=[tf.keras.metrics.AUC()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e39bae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442/2442 [==============================] - ETA: 0s - auc_2: 0.6560 - loss: 0.1864 - regularization_loss: 0.0000e+00 - total_loss: 0.1864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:38:43.261571: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/then/_0/cond/cond/branch_executed/_158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442/2442 [==============================] - 151s 58ms/step - auc_2: 0.6560 - loss: 0.1864 - regularization_loss: 0.0000e+00 - total_loss: 0.1864 - val_auc_2: 0.5646 - val_loss: 0.1284 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.1284\n",
      "CPU times: user 5min 11s, sys: 51.8 s, total: 6min 3s\n",
      "Wall time: 2min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0d6e466070>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=LR)\n",
    "model.compile(optimizer=opt, run_eagerly=False)\n",
    "model.fit(train_dl, validation_data=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98efae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(\"DLRM\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491573f",
   "metadata": {},
   "source": [
    "### DCN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539aff46",
   "metadata": {},
   "source": [
    "DCN-V2 is an architecture proposed as an improvement upon the original [DCN model](https://arxiv.org/pdf/1708.05123.pdf). The explicit feature interactions of the inputs are learned through cross layers, and then combined with a deep network to learn complementary implicit interactions. The overall model architecture is depicted in Figure below, with two ways to combine the cross network with the deep network: (1) stacked and (2) parallel. The output of the embbedding layer is the concatenation of all the embedded vectors and the normalized dense features: x<sub>0</sub> = [x<sub>embed,1</sub>; . . . ; x<sub>embed,ùëõ</sub>; ùë•<sub>dense</sub>].\n",
    "\n",
    "![DCN](./images/DCN.png)\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/2008.13535\">Image Source: DCN V2 paper</a>\n",
    "\n",
    "Steps:<br>\n",
    "* Change the model to `DCNModel`\n",
    "* Rerun the pipeline from there to model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5549d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DCNModel(\n",
    "    schema,\n",
    "    depth=2,\n",
    "    deep_block=mm.MLPBlock([64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column, metrics=[tf.keras.metrics.AUC()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce55ab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2441/2442 [============================>.] - ETA: 0s - auc_3: 0.6708 - loss: 0.1809 - regularization_loss: 0.0000e+00 - total_loss: 0.1809"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:41:04.342881: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/then/_0/cond/cond/branch_executed/_152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442/2442 [==============================] - 140s 55ms/step - auc_3: 0.6708 - loss: 0.1808 - regularization_loss: 0.0000e+00 - total_loss: 0.1808 - val_auc_3: 0.5640 - val_loss: 0.1307 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.1307\n",
      "CPU times: user 4min 55s, sys: 50.8 s, total: 5min 46s\n",
      "Wall time: 2min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0d6d3a86a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=0.005)\n",
    "model.compile(optimizer=opt, run_eagerly=False)\n",
    "model.fit(train_dl, validation_data=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05f65f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(\"DCN\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca042f62",
   "metadata": {},
   "source": [
    "Let's visualize our model validation accuracy values. Since we did not do any hyper-parameter optimization here, we do not come up with a final conclusion that one model is superior to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbf20e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from example_utils import create_bar_chart\n",
    "models_name = [\"MLP\", \"NCF\", \"DLRM\", \"DCN\"]\n",
    "create_bar_chart(\"results.txt\", models_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
