{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5e6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f307c8d",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Applying to your own dataset with Merlin Models and NVTabular\n",
    "\n",
    "## Overview\n",
    "\n",
    "In [01-getting-started.ipynb](01-getting-started.ipynb), we provide a getting started example to train a DLRM model on the MovieLens 1M dataset. In this notebook, we will explore how Merlin Models uses the ETL output from [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular/).<br><br>\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "This notebook provides details how NVTabular and Merlin Models are linked together. We will discuss the concept of the `schema` file.\n",
    "\n",
    "## Merlin\n",
    "\n",
    "[Merlin](https://developer.nvidia.com/nvidia-merlin) is an open-source framework for building large-scale (deep learning) recommender systems. It is designed to support recommender systems end-to-end from ETL to training to deployment on CPU or GPU. Common deep learning frameworks are integrated such as TensorFlow or PyTorch. Its key benefits are the easy-to-use APIs, accelerations with GPU and scaling to multi-GPU or multi-node systems.\n",
    "\n",
    "Merlin Models and NVTabular are components of Merlin. They are designed to work closely together. \n",
    "\n",
    "[Merlin Models](https://github.com/NVIDIA-Merlin/models/) is a library to make it easy for users in industry or academia to train and deploy recommender models with best practices baked into the library. This will let users in industry easily train standard models against their own dataset, getting high performance GPU accelerated models into production. This will also let researchers to build custom models by incorporating standard components of deep learning recommender models, and then benchmark their new models on example offline datasets.\n",
    "\n",
    "[NVTabular](https://github.com/NVIDIA-Merlin/NVTabular/)) is a feature engineering and preprocessing library for tabular data that is designed to easily manipulate terabyte scale datasets and train deep learning (DL) based recommender systems. It provides high-level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS Dask-cuDF library.\n",
    "\n",
    "## Integration of NVTabular and Merlin Models\n",
    "\n",
    "<img src=\"images/schema.png\">\n",
    "\n",
    "If you use NVTabular for feature engineering, in addition to the data, NVTabular will provide a `schema file` describing the dataset structures. NVTabular will automatically detect some types of `Tags`. Some `Tags` have to be provided manually. \n",
    "\n",
    "Let's take a look on the MovieLens 1M example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7525f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 14:51:49.328297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16255 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:0b:00.0, compute capability: 7.0\n",
      "2022-03-15 14:51:49.329539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 29922 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:85:00.0, compute capability: 7.0\n",
      "2022-03-15 14:51:49.330666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 29924 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:86:00.0, compute capability: 7.0\n",
      "2022-03-15 14:51:49.331690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 29924 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n",
      "2022-03-15 14:51:49.346190: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 15.87G (17044602880 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2022-03-15 14:51:49.348505: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 14.29G (15340142592 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import nvtabular as nvt\n",
    "import merlin.io\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "\n",
    "from os import path\n",
    "from nvtabular import ops\n",
    "from merlin.core.utils import download_file\n",
    "from merlin.models.data.movielens import get_movielens\n",
    "from merlin.schema.tags import Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99659676",
   "metadata": {},
   "source": [
    "We will use the utils function to download, extract and preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23ac3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:1253: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train, valid = get_movielens(variant=\"ml-1m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72b240",
   "metadata": {},
   "source": [
    "When NVTabular process the data, it will persist the schema as a file to disk in . The dataset contains the schema as a property, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f93fad",
   "metadata": {},
   "source": [
    "The `schema` can be interpreted as a list of features in the dataset, where each element describe the feature. It contains the name, some properties (e.g. statistics) and multiple tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e6dcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'userId', 'tags': {<Tags.USER_ID: 'user_id'>, <Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.userId.parquet', 'embedding_sizes': {'cardinality': 6041.0, 'dimension': 210.0}, 'domain': {'min': 0, 'max': 6041}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'movieId', 'tags': {<Tags.ITEM: 'item'>, <Tags.ITEM_ID: 'item_id'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.movieId.parquet', 'embedding_sizes': {'cardinality': 3680.0, 'dimension': 159.0}, 'domain': {'min': 0, 'max': 3680}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'title', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.title.parquet', 'embedding_sizes': {'cardinality': 3680.0, 'dimension': 159.0}, 'domain': {'min': 0, 'max': 3680}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'genres', 'tags': {<Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.genres.parquet', 'embedding_sizes': {'cardinality': 19.0, 'dimension': 16.0}, 'domain': {'min': 0, 'max': 19}}, 'dtype': dtype('int32'), 'is_list': True, 'is_ragged': True}, {'name': 'gender', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.gender.parquet', 'embedding_sizes': {'cardinality': 3.0, 'dimension': 16.0}, 'domain': {'min': 0, 'max': 3}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'age', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.age.parquet', 'embedding_sizes': {'cardinality': 8.0, 'dimension': 16.0}, 'domain': {'min': 0, 'max': 8}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'occupation', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.occupation.parquet', 'embedding_sizes': {'cardinality': 22.0, 'dimension': 16.0}, 'domain': {'min': 0, 'max': 22}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'zipcode', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.zipcode.parquet', 'embedding_sizes': {'cardinality': 3440.0, 'dimension': 153.0}, 'domain': {'min': 0, 'max': 3440}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'TE_age_rating', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.USER: 'user'>}, 'properties': {}, 'dtype': dtype('float64'), 'is_list': False, 'is_ragged': False}, {'name': 'TE_gender_rating', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.USER: 'user'>}, 'properties': {}, 'dtype': dtype('float64'), 'is_list': False, 'is_ragged': False}, {'name': 'TE_occupation_rating', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.USER: 'user'>}, 'properties': {}, 'dtype': dtype('float64'), 'is_list': False, 'is_ragged': False}, {'name': 'TE_zipcode_rating', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.USER: 'user'>}, 'properties': {}, 'dtype': dtype('float64'), 'is_list': False, 'is_ragged': False}, {'name': 'TE_movieId_rating', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.ITEM: 'item'>}, 'properties': {}, 'dtype': dtype('float64'), 'is_list': False, 'is_ragged': False}, {'name': 'TE_userId_rating', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.USER: 'user'>}, 'properties': {}, 'dtype': dtype('float64'), 'is_list': False, 'is_ragged': False}, {'name': 'rating_binary', 'tags': {<Tags.BINARY_CLASSIFICATION: 'binary_classification'>, <Tags.TARGET: 'target'>}, 'properties': {}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'rating', 'tags': {<Tags.REGRESSION: 'regression'>, <Tags.TARGET: 'target'>}, 'properties': {}, 'dtype': dtype('float32'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b668d1",
   "metadata": {},
   "source": [
    "We can select the features by `Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a4cf3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'userId', 'tags': {<Tags.USER_ID: 'user_id'>, <Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.userId.parquet', 'embedding_sizes': {'cardinality': 6041.0, 'dimension': 210.0}, 'domain': {'min': 0, 'max': 6041}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.schema.select_by_name(\"userId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d31ca",
   "metadata": {},
   "source": [
    "Alternativley, we can select them by `Tag`. We add `column_names` to the object to receive only names without all the additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dac63b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId',\n",
       " 'movieId',\n",
       " 'title',\n",
       " 'genres',\n",
       " 'gender',\n",
       " 'age',\n",
       " 'occupation',\n",
       " 'zipcode']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All categorical features\n",
    "train.schema.select_by_tag(Tags.CATEGORICAL).column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "478cf7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TE_age_rating',\n",
       " 'TE_gender_rating',\n",
       " 'TE_occupation_rating',\n",
       " 'TE_zipcode_rating',\n",
       " 'TE_movieId_rating',\n",
       " 'TE_userId_rating']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All continuous features\n",
    "train.schema.select_by_tag(Tags.CONTINUOUS).column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df013b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rating_binary', 'rating']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All targets\n",
    "train.schema.select_by_tag(Tags.TARGET).column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64f68da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId', 'genres', 'TE_movieId_rating']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All features related to the item\n",
    "train.schema.select_by_tag(Tags.ITEM).column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "121bb3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId',\n",
       " 'TE_age_rating',\n",
       " 'TE_gender_rating',\n",
       " 'TE_occupation_rating',\n",
       " 'TE_zipcode_rating',\n",
       " 'TE_userId_rating']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All features related to the user\n",
    "train.schema.select_by_tag(Tags.USER).column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b33d12",
   "metadata": {},
   "source": [
    "The `schema` is a great way to combine Feature Engineering and Model Training as one end-to-end pipeline. Many popular (deep learning) recommender models define the architecture based on different feature types. \n",
    "\n",
    "DLRM applies embedding layers to each categorical input feature and applies a MLP (called bottom MLP) to the continuous input features.\n",
    "\n",
    "Two Tower model applies a MLP (with embedding layers for categorical features) to all item features (called item tower) and another MLP to all user features (called user tower).\n",
    "\n",
    "The `schema` file contains all required information to build the architecture. If the dataset changes (e.g. more features are added), then the same code can be used to define the same architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07d913",
   "metadata": {},
   "source": [
    "## Applying NVTabular and Merlin Models to an own dataset.\n",
    "\n",
    "We have a solid understanding of the importance of the schema and how the schema works. Let's take a look on how to apply it to your own dataset.\n",
    "\n",
    "The best way is to use [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular/) for the feature engineering step. We will look on a minimal example for the MovieLens dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c04651b",
   "metadata": {},
   "source": [
    "We will download the dataset, if it is not available on the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23ae0a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading ml-1m.zip: 5.93MB [00:02, 2.63MB/s]                                 \n",
      "unzipping files: 100%|█████████████████████████| 5/5 [00:00<00:00, 36.20files/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = os.environ.get(\n",
    "    \"INPUT_DATA_DIR\",\n",
    "    os.path.expanduser(\"~/merlin-models-data/movielens/\")\n",
    ")\n",
    "name = \"ml-1m\"\n",
    "download_file(\n",
    "    \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\",\n",
    "    os.path.join(input_path, \"ml-1m.zip\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60476b37",
   "metadata": {},
   "source": [
    "We preprocess the dataset and split it into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aee80c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9359/3325759393.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  ratings = pd.read_csv(os.path.join(input_path, \"ml-1m/ratings.dat\"), sep=\"::\", names=['userId', 'movieId', 'rating', 'timestamp'])\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv(\n",
    "    os.path.join(input_path, \"ml-1m/ratings.dat\"),\n",
    "    sep=\"::\",\n",
    "    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
    ")\n",
    "ratings = ratings.sample(len(ratings), replace=False)\n",
    "\n",
    "num_valid = int(len(ratings) * 0.2)\n",
    "train = ratings[:-num_valid]\n",
    "valid = ratings[-num_valid:]\n",
    "train.to_parquet(os.path.join(input_path, name, \"train.parquet\"))\n",
    "valid.to_parquet(os.path.join(input_path, name, \"valid.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d44121",
   "metadata": {},
   "source": [
    "We use NVTabular to define a feature engineering pipeline. \n",
    "\n",
    "NVTabular has already implemented multiple calculations, called `ops`. An `op` can be applied to a `ColumnGroup` from an overloaded `>>` operator.<br><br>\n",
    "**Example:**<br>\n",
    "```python\n",
    "features = [ column_name, ...] >> op1 >> op2 >> ...\n",
    "```\n",
    "\n",
    "We need to perform following steps:\n",
    "- Categorify userId and movieId, that the values are continuous integers from 0 ... |C|\n",
    "- Transform the rating column to a binary target by using `>3` as `1` and otherwise `0`\n",
    "- Add Tags with `ops.AddMetadata` for `item_id`, `user_id`, `item`, user and `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13a178c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\"userId\", \"movieId\"] >> ops.Categorify(dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb8ca769",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_itemId = (\n",
    "    cat_features[\"movieId\"] >> ops.AddMetadata(tags=[\"item_id\", \"item\"])\n",
    ")\n",
    "feats_userId = (\n",
    "    cat_features[\"userId\"] >> ops.AddMetadata(tags=[\"user_id\", \"user\"])\n",
    ")\n",
    "feats_target = (\n",
    "    nvt.ColumnSelector([\"rating\"])\n",
    "    >> ops.LambdaOp(lambda col: (col > 3).astype(\"int32\"))\n",
    "    >> ops.AddMetadata(tags=[\"binary_classification\", \"target\"])\n",
    "    >> nvt.ops.Rename(name=\"rating_binary\")\n",
    ")\n",
    "output = feats_itemId + feats_userId + feats_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f71c7",
   "metadata": {},
   "source": [
    "We apply the workflow to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2e576af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo replace with fit_transform\n",
    "\n",
    "workflow = nvt.Workflow(output)\n",
    "\n",
    "train_dataset = nvt.Dataset([os.path.join(input_path, name, \"train.parquet\")])\n",
    "valid_dataset = nvt.Dataset([os.path.join(input_path, name, \"valid.parquet\")])\n",
    "\n",
    "if path.exists(os.path.join(input_path, name, \"train\")):\n",
    "    shutil.rmtree(os.path.join(input_path, name, \"train\"))\n",
    "if path.exists(os.path.join(input_path, name, \"valid\")):\n",
    "    shutil.rmtree(os.path.join(input_path, name, \"valid\"))\n",
    "\n",
    "workflow.fit(train_dataset)\n",
    "workflow.transform(train_dataset).to_parquet(\n",
    "    output_path=os.path.join(input_path, name, \"train\"),\n",
    "    out_files_per_proc=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "workflow.transform(valid_dataset).to_parquet(\n",
    "    output_path=os.path.join(input_path, name, \"valid\"),\n",
    "    out_files_per_proc=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "# Save the workflow\n",
    "workflow.save(os.path.join(input_path, name, \"workflow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ca8a0",
   "metadata": {},
   "source": [
    "We can load the data as a Merlin Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d15b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = merlin.io.Dataset(\n",
    "    os.path.join(input_path, name, \"train\"), engine=\"parquet\"\n",
    ")\n",
    "valid = merlin.io.Dataset(\n",
    "    os.path.join(input_path, name, \"valid\"), engine=\"parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad3ac07",
   "metadata": {},
   "source": [
    "We can train and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb11e1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 14:52:01.170856: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 9s 6ms/step - rating_binary/binary_classification_task/precision: 0.7119 - rating_binary/binary_classification_task/recall: 0.8240 - rating_binary/binary_classification_task/binary_accuracy: 0.7070 - rating_binary/binary_classification_task/auc: 0.7690 - loss: 0.5621 - regularization_loss: 0.0000e+00 - total_loss: 0.5621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 14:52:12.331654: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/then/_0/cond/cond/branch_executed/_101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 3s 5ms/step - rating_binary/binary_classification_task/precision: 0.7368 - rating_binary/binary_classification_task/recall: 0.8161 - rating_binary/binary_classification_task/binary_accuracy: 0.7266 - rating_binary/binary_classification_task/auc: 0.7927 - loss: 0.5385 - regularization_loss: 0.0000e+00 - total_loss: 0.5385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7367859482765198,\n",
       " 0.8161250352859497,\n",
       " 0.7265610694885254,\n",
       " 0.7926554679870605,\n",
       " 0.5303010940551758,\n",
       " 0.0,\n",
       " 0.5303010940551758]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model = mm.DLRMModel(\n",
    "    train.schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(\n",
    "        train.schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "    ),\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\")\n",
    "model.fit(train, batch_size=1024)\n",
    "model.evaluate(valid, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a796b",
   "metadata": {},
   "source": [
    "We can take a look on the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ade4f005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'movieId', 'tags': {<Tags.ITEM: 'item'>, <Tags.ITEM_ID: 'item_id'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.movieId.parquet', 'embedding_sizes': {'cardinality': 3682.0, 'dimension': 159.0}, 'domain': {'min': 0, 'max': 3682}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'userId', 'tags': {<Tags.USER_ID: 'user_id'>, <Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.userId.parquet', 'embedding_sizes': {'cardinality': 6041.0, 'dimension': 210.0}, 'domain': {'min': 0, 'max': 6041}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'rating_binary', 'tags': {<Tags.BINARY_CLASSIFICATION: 'binary_classification'>, <Tags.TARGET: 'target'>}, 'properties': {}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714b554",
   "metadata": {},
   "source": [
    "As we prepared only a minimal example, our schema has only tree features `movieId`, `userId` and `rating_binary`.\n",
    "\n",
    "NVTabular can automatically add `Tags` for certrain operations. For example, the output of `Categorify` is always a categorical feature and will be tagged. Similar, the output of `Normalize` is always continuous.\n",
    "\n",
    "You can take a look on the full example of our util function for MovieLens in our repository.\n",
    "\n",
    "You can learn more about NVTabular, its functionality and suppored ops by visiting our [github repository]() or exploring the [examples](https://github.com/NVIDIA-Merlin/NVTabular/tree/main/examples), such as [`Getting Started MovieLens`](https://github.com/NVIDIA-Merlin/NVTabular/blob/main/examples/getting-started-movielens/02-ETL-with-NVTabular.ipynb) or [`Scaling Criteo`](https://github.com/NVIDIA-Merlin/NVTabular/tree/main/examples/scaling-criteo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ed321",
   "metadata": {},
   "source": [
    "The easist way is to use NVTabular to generate a schema file. Alternatively, you can manually create a schema file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
