{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f816c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d426b9",
   "metadata": {},
   "source": [
    "## Training a DLRM model with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1596b8e",
   "metadata": {},
   "source": [
    "In the previous notebooks, we have downloaded the movielens data, converted it to parquet files and then used NVTabular library to process the data, join data frames, and create input features. In this notebook we will use NVIDIA Merlin Models library to build and train a Deep Learning Recommendation Model [(DLRM)](https://arxiv.org/abs/1906.00091) architecture originally proposed by Facebook in 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a73f7d",
   "metadata": {},
   "source": [
    "Figure 1 illustrates DLRM architecture. The model was introduced as a personalization deep learning model that uses embeddings to process sparse features that represent categorical data and a multilayer perceptron (MLP) to process dense features, then interacts these features explicitly using the statistical techniques proposed in [here](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5694074)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8fee0e",
   "metadata": {},
   "source": [
    "![DLRM](../images/DLRM.png)\n",
    "\n",
    "<p>Figure 1. DLRM architecture. Image source: <a href=\"https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/DLRM\">Nvidia DL Examples</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820fef2",
   "metadata": {},
   "source": [
    "DLRM accepts two types of features: categorical and numerical. For details of the DLRM architecture and how to build it using Merlin Models low-level API please visit `Binary_classificaion_DLRM` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdce432",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c2076dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/workspace/merlin_models/\")\n",
    "sys.path.append(\"/nvtabular/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9de8bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 01:22:52.960361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nvtabular as nvt\n",
    "\n",
    "import merlin_models.tf as ml\n",
    "from merlin_standard_lib import Schema, Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e955f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "logging.disable(logging.WARNING) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769082b5",
   "metadata": {},
   "source": [
    "### Data Download and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03fb6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\n",
    "    \"INPUT_DATA_DIR\", os.path.expanduser(\"/workspace/data/movielens/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fba67e",
   "metadata": {},
   "source": [
    "With help of a utility function first we download and unzip the data. Second, we convert data via basic preprocessing, and split data into train and validation files and save them as parquet files. Afterwards, we preprocess the train and validation parquet files and generate features for model training using NVTabular.\n",
    "\n",
    "Let's download Movielens 25M dataset and then process it, and save files to disk in parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e43bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin_standard_lib.utils.data_etl_utils import movielens_download_etl\n",
    "movielens_download_etl(INPUT_DATA_DIR, 'ml-25m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c520447",
   "metadata": {},
   "source": [
    "Merlin Models library relies on a `schema` object to automatically build all necessary layers to represent, normalize and aggregate input features. As you can see below, schema.pb is a protobuf file that contains metadata including statistics about features such as cardinality, min and max values and also tags features based on their characteristics and dtypes (e.g., categorical, continuous, list, integer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6881d61",
   "metadata": {},
   "source": [
    "We also generated our `schema.pbtxt` file in using NVTabular. Now we read this schema file to create a `schema` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eead0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "SCHEMA_PATH = os.path.join(INPUT_DATA_DIR, 'ml-25m' \"/train/schema.pbtxt\")\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a976d",
   "metadata": {},
   "source": [
    "We can print out the feature names including the binary target column, `rating_binary`, in the schema easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98082c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema.remove_by_name(['rating', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5abb8861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId',\n",
       " 'userId',\n",
       " 'genres',\n",
       " 'TE_movieId_rating',\n",
       " 'userId_count',\n",
       " 'rating_binary']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ada0e",
   "metadata": {},
   "source": [
    "Select continuous and categorical columns from schema using feature tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b74e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_schema = schema.select_by_tag(Tag.CONTINUOUS)\n",
    "cat_schema = schema.select_by_tag(Tag.CATEGORICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11497f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['TE_movieId_rating', 'userId_count'], ['movieId', 'userId', 'genres'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_schema.column_names, cat_schema.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280057d6",
   "metadata": {},
   "source": [
    "### Define Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7797f8ba",
   "metadata": {},
   "source": [
    "Below we define our input block using the `ml.ContinuousEmbedding` function. The from_schema() method processes the schema and creates the necessary layers to represent features and aggregate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bd5fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import merlin_models.tf.dataset as tf_dataloader\n",
    "\n",
    "# Define categorical and continuous columns\n",
    "x_cat_names, x_cont_names = cat_schema.column_names, con_schema.column_names\n",
    "\n",
    "# dictionary representing max sequence length for each column\n",
    "sparse_features_max = {'genres': 10}\n",
    "\n",
    "def get_dataloader(paths_or_dataset, batch_size=4096, shuffle=True):\n",
    "    dataloader = tf_dataloader.Dataset(\n",
    "        paths_or_dataset,\n",
    "        batch_size=batch_size,\n",
    "        label_names=['rating_binary'],\n",
    "        cat_names=x_cat_names,\n",
    "        cont_names=x_cont_names,\n",
    "        sparse_names=list(sparse_features_max.keys()),\n",
    "        sparse_max=sparse_features_max,\n",
    "        sparse_as_dense=True,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return dataloader.map(lambda X, y: (X, tf.reshape(y, (-1,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af63a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"/workspace/data/movielens/ml-25m/\")\n",
    "train_paths = glob.glob(os.path.join(OUTPUT_DIR, \"train/*.parquet\"))\n",
    "eval_paths = glob.glob(os.path.join(OUTPUT_DIR, \"valid/*.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96adad",
   "metadata": {},
   "source": [
    "In the DLRM architecture, categorical features are processed using embeddings. Below, for each categorical feature, we create an embedding table used to provide dense representation to each unique value of this feature. The dense vector values in the embedding tables are learned during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151106d0",
   "metadata": {},
   "source": [
    "### Building a DLRM model with Merlin Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "374b7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlrm_body = ml.DLRMBlock(schema,\n",
    "        embedding_dim=16,\n",
    "        bottom_block=ml.MLPBlock([64, 16]),\n",
    "        top_block=ml.MLPBlock([64, 32]),\n",
    "    )\n",
    "model = dlrm_body.connect(ml.BinaryClassificationTask(\"rating_binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fde057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "optimizer = tf.keras.optimizers.Adam(0.005)\n",
    "model.compile(optimizer=optimizer, run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b6bfa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 01:10:27.512305: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-02-01 01:10:28.350752: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING: AutoGraph could not transform <bound method Block.parse of <class 'merlin_models.tf.core.Block'>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: annotated name 'output' can't be nonlocal (tmpygc__wyq.py, line 36)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "4883/4883 [==============================] - 101s 13ms/step - rating_binary/binary_classification_task/precision: 0.7626 - rating_binary/binary_classification_task/recall: 0.8611 - rating_binary/binary_classification_task/binary_accuracy: 0.7455 - rating_binary/binary_classification_task/auc: 0.8021 - loss: 0.5141 - regularization_loss: 0.0000e+00 - total_loss: 0.5141\n",
      "Epoch 2/3\n",
      "4883/4883 [==============================] - 96s 13ms/step - rating_binary/binary_classification_task/precision: 0.7778 - rating_binary/binary_classification_task/recall: 0.8632 - rating_binary/binary_classification_task/binary_accuracy: 0.7603 - rating_binary/binary_classification_task/auc: 0.8242 - loss: 0.4893 - regularization_loss: 0.0000e+00 - total_loss: 0.4893\n",
      "Epoch 3/3\n",
      "4883/4883 [==============================] - 96s 13ms/step - rating_binary/binary_classification_task/precision: 0.7844 - rating_binary/binary_classification_task/recall: 0.8650 - rating_binary/binary_classification_task/binary_accuracy: 0.7669 - rating_binary/binary_classification_task/auc: 0.8330 - loss: 0.4786 - regularization_loss: 0.0000e+00 - total_loss: 0.4786\n",
      "********************\n",
      "Start evaluation\n",
      "1221/1221 [==============================] - 20s 9ms/step - rating_binary/binary_classification_task/precision: 0.7798 - rating_binary/binary_classification_task/recall: 0.8641 - rating_binary/binary_classification_task/binary_accuracy: 0.7625 - rating_binary/binary_classification_task/auc: 0.8261 - loss: 0.4875 - regularization_loss: 0.0000e+00 - total_loss: 0.4875\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_dataloader(nvt.Dataset(train_paths), shuffle=True) \n",
    "losses = model.fit(train_loader, epochs=3)\n",
    "model.reset_metrics()\n",
    "\n",
    "print('*'*20)\n",
    "print(\"Start evaluation\")\n",
    "eval_loader = get_dataloader(nvt.Dataset(eval_paths), shuffle=False) \n",
    "eval_metrics = model.evaluate(eval_loader, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb63a46",
   "metadata": {},
   "source": [
    "## Training a Deep & Cross Network (DCN)-V2 model with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d69ec",
   "metadata": {},
   "source": [
    "[Deep & Cross Network (DCN)-V2](https://arxiv.org/pdf/2008.13535.pdf) architecture was proposed by Google in 2020 as an improve upon the original [DCN model](https://arxiv.org/pdf/1708.05123.pdf). The overall model architecture is depicted in Figure 2, with two ways to combine the cross network with the deep network: (1) stacked and (2) parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786cb7a0",
   "metadata": {},
   "source": [
    "![DCN](../images/DCN.png)\n",
    "\n",
    "<p>Figure 2. DCN-v2 architecture. Image source: <a href=\"https://arxiv.org/pdf/2008.13535.pdf\">DCN V2</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd45c7",
   "metadata": {},
   "source": [
    "The output of the embbedding layer is the concatenation of all the embedded vectors and the normalized dense features: x<sub>0</sub> = [x<sub>embed,1</sub>; . . . ; x<sub>embed,ùëõ</sub>; x<sub>dense</sub>]. Below, we build a stacked structure shown in Figure 2(a). Basically, it starts with an input layer (typically an embedding layer), and then the input x<sub>0</sub> is fed to the cross network, containing multiple cross layers that models explicit feature interactions, and then followed by the deep network. At the last step, we connect the final layer to the `BinaryClassificationTask` head for doing binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "191c5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcn_body = (\n",
    "    ml.InputBlock(schema,\n",
    "        embedding_options=ml.EmbeddingOptions(embedding_dim_default=16),\n",
    "        aggregation=\"concat\",\n",
    "    )\n",
    "    .connect(ml.CrossBlock(3))\n",
    "    .connect(ml.MLPBlock([512, 256]))\n",
    ")\n",
    "model = dcn_body.connect(ml.BinaryClassificationTask(\"rating_binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9160b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 01:24:11.423250: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-02-01 01:24:12.341618: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING: AutoGraph could not transform <bound method Block.parse of <class 'merlin_models.tf.core.Block'>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: annotated name 'output' can't be nonlocal (tmpskqv39bh.py, line 36)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "4883/4883 [==============================] - 101s 13ms/step - rating_binary/binary_classification_task/precision: 0.7569 - rating_binary/binary_classification_task/recall: 0.8608 - rating_binary/binary_classification_task/binary_accuracy: 0.7401 - rating_binary/binary_classification_task/auc: 0.7932 - loss: 0.5234 - regularization_loss: 0.0000e+00 - total_loss: 0.5234\n",
      "Epoch 2/3\n",
      "4883/4883 [==============================] - 95s 13ms/step - rating_binary/binary_classification_task/precision: 0.7755 - rating_binary/binary_classification_task/recall: 0.8625 - rating_binary/binary_classification_task/binary_accuracy: 0.7579 - rating_binary/binary_classification_task/auc: 0.8212 - loss: 0.4926 - regularization_loss: 0.0000e+00 - total_loss: 0.4926\n",
      "Epoch 3/3\n",
      "4883/4883 [==============================] - 95s 13ms/step - rating_binary/binary_classification_task/precision: 0.7814 - rating_binary/binary_classification_task/recall: 0.8633 - rating_binary/binary_classification_task/binary_accuracy: 0.7635 - rating_binary/binary_classification_task/auc: 0.8290 - loss: 0.4832 - regularization_loss: 0.0000e+00 - total_loss: 0.4832\n",
      "********************\n",
      "Start evaluation\n",
      "1221/1221 [==============================] - 20s 9ms/step - rating_binary/binary_classification_task/precision: 0.7825 - rating_binary/binary_classification_task/recall: 0.8541 - rating_binary/binary_classification_task/binary_accuracy: 0.7604 - rating_binary/binary_classification_task/auc: 0.8242 - loss: 0.4893 - regularization_loss: 0.0000e+00 - total_loss: 0.4893\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False)\n",
    "train_loader = get_dataloader(nvt.Dataset(train_paths), shuffle=True) \n",
    "losses = losses = model.fit(train_loader, epochs=3)\n",
    "\n",
    "print('*'*20)\n",
    "print(\"Start evaluation\")\n",
    "eval_loader = get_dataloader(nvt.Dataset(eval_paths), shuffle=False) \n",
    "eval_metrics = model.evaluate(eval_loader, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061d7e4",
   "metadata": {},
   "source": [
    "Just like that, with couple lines of codes we are able to build state-of-the-art Deep Learning-based Recommender Systems models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
