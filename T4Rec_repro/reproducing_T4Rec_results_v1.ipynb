{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14beb6b6",
   "metadata": {},
   "source": [
    "These are logs from training the following model from the CI script from T4Rec (the trianing was for 5 epochs):\n",
    "\n",
    "`### XLNet (MLM) - Item Id feature\n",
    "python3 transf_exp_main_modified.py --output_dir ./tmp/ --overwrite_output_dir --do_train --do_eval --validate_every 10 --logging_steps 20 --save_steps 0 --data_path $DATA_PATH --features_schema_path $FEATURE_SCHEMA_PATH --fp16 --data_loader_engine merlin --start_time_window_index 1 --final_time_window_index 2 --time_window_folder_pad_digits 4 --model_type xlnet --loss_type cross_entropy --per_device_eval_batch_size 128 --similarity_type concat_mlp --tf_out_activation tanh --inp_merge mlp --learning_rate_warmup_steps 0 --learning_rate_schedule linear_with_warmup --hidden_act gelu --num_train_epochs $NUM_EPOCHS --dataloader_drop_last --compute_metrics_each_n_steps 1 --session_seq_length_max 20 --eval_on_last_item_seq_only --mf_constrained_embeddings --layer_norm_featurewise --attn_type bi --mlm --per_device_train_batch_size 128 --learning_rate 0.0006667377132554976 --dropout 0.0 --input_dropout 0.1 --weight_decay 3.910060265627374e-05 --d_model 192 --item_embedding_dim 448 --n_layer 3 --n_head 16 --label_smoothing 0.0 --stochastic_shared_embeddings_replacement_prob 0.1 --item_id_embeddings_init_std 0.11 --other_embeddings_init_std 0.02 --mlm_probability 0.30000000000000004 --eval_on_test_set --seed 100 --report_to none\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7010a6a1",
   "metadata": {},
   "source": [
    "And here are the logs and the results, maybe reproducing that is something that we could work towards (the XLNet with MLM is what I used for benchmarking T4Rec, starting with it would be great)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d6ef61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/NVIDIA-Merlin/Models\n",
      " * [new branch]        ci/horovod             -> origin/ci/horovod\n",
      " * [new branch]        codespell_fix          -> origin/codespell_fix\n",
      "   16fb4149..b1c10317  fea-sok-integration-wj -> origin/fea-sok-integration-wj\n",
      " * [new branch]        fea-sok-load-dump      -> origin/fea-sok-load-dump\n",
      "   95462360..a69adf75  gh-pages               -> origin/gh-pages\n",
      " * [new branch]        mtl_example            -> origin/mtl_example\n",
      "   cb431a8a..b90e9a1b  release-22.12          -> origin/release-22.12\n",
      " * [new branch]        tf/column_sampling_serialization_fix -> origin/tf/column_sampling_serialization_fix\n",
      " * [new branch]        tf/continuous_seq_feats_fix -> origin/tf/continuous_seq_feats_fix\n",
      " * [new branch]        tf/dataloader_changes  -> origin/tf/dataloader_changes\n",
      " * [new branch]        tf/fix_broadcast_to_sequence -> origin/tf/fix_broadcast_to_sequence\n",
      " * [new branch]        tf/fix_training_smaller_accuracy -> origin/tf/fix_training_smaller_accuracy\n",
      " * [new branch]        tf/mtl_example_updates_v2 -> origin/tf/mtl_example_updates_v2\n",
      " + 169f3df5...06eecddd tf/output-block        -> origin/tf/output-block  (forced update)\n",
      " * [new branch]        tf/process_list_to_prepare_features -> origin/tf/process_list_to_prepare_features\n",
      " * [new branch]        tf/quick_start_ranking -> origin/tf/quick_start_ranking\n",
      " + 0a65d603...9f53e8ff update_07              -> origin/update_07  (forced update)\n",
      " * [new tag]           v23.02.00              -> v23.02.00\n",
      "error: Your local changes to the following files would be overwritten by checkout:\n",
      "\tT4Rec_repro/reproducing_T4Rec_results.ipynb\n",
      "Please commit your changes or stash them before you switch branches.\n",
      "Aborting\n",
      "Warning: you are leaving 2 commits behind, not connected to\n",
      "any of your branches:\n",
      "\n",
      "  e284ebd Merge branch 'main' of https://github.com/NVIDIA-Merlin/core into HEAD\n",
      "  b2372e4 Merge branch 'main' of https://github.com/NVIDIA-Merlin/core into HEAD\n",
      "\n",
      "If you want to keep them by creating a new branch, this may be a good time\n",
      "to do so with:\n",
      "\n",
      " git branch <new-branch-name> e284ebd\n",
      "\n",
      "Switched to branch 'main'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your branch is behind 'origin/main' by 22 commits, and can be fast-forwarded.\n",
      "  (use \"git pull\" to update your local branch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/NVIDIA-Merlin/core\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   5dbafa68..aad0c874 main       -> origin/main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating cd96ca5f..aad0c874\n",
      "Fast-forward\n",
      " .github/release-drafter.yml                        |  44 +--\n",
      " .github/workflows/ISSUE_TEMPLATE/bug-report.md     |  17 +-\n",
      " .../ISSUE_TEMPLATE/documentation-request.md        |  12 +-\n",
      " .../workflows/ISSUE_TEMPLATE/feature-request.md    |   5 +-\n",
      " .../workflows/ISSUE_TEMPLATE/submit-question.md    |   3 +-\n",
      " .github/workflows/ISSUE_TEMPLATE/task.md           |   5 +-\n",
      " .github/workflows/cpu-ci.yml                       | 145 +++-------\n",
      " .github/workflows/cpu-models.yml                   |  52 ++--\n",
      " .github/workflows/cpu-nvtabular.yml                |  52 ++--\n",
      " .github/workflows/cpu-packages.yml                 | 126 +++++++++\n",
      " .github/workflows/cpu-systems.yml                  |  52 ++--\n",
      " .github/workflows/docs-preview-pr.yaml             |   2 +-\n",
      " .github/workflows/docs-sched-rebuild.yaml          |   7 +-\n",
      " .github/workflows/gpu-ci.yml                       |  30 +-\n",
      " .github/workflows/release-drafter.yaml             |   2 +-\n",
      " .pre-commit-config.yaml                            |  55 ++--\n",
      " .prettierignore                                    |   2 +\n",
      " CLA.md                                             |   9 +-\n",
      " CONTRIBUTING.md                                    |  28 +-\n",
      " README.md                                          |  68 ++---\n",
      " ci/pr.gpu.Jenkinsfile                              |   2 +-\n",
      " docs/README.md                                     |  49 ++--\n",
      " merlin/core/compat.py                              |  59 +++-\n",
      " merlin/core/dispatch.py                            |  51 +++-\n",
      " merlin/dag/__init__.py                             |   1 +\n",
      " merlin/dag/base_operator.py                        |  30 +-\n",
      " merlin/dag/dictarray.py                            |   3 +-\n",
      " merlin/dag/executors.py                            | 107 ++++---\n",
      " merlin/dag/graph.py                                |  20 ++\n",
      " merlin/dag/node.py                                 |   2 +-\n",
      " merlin/dag/utils.py                                |  69 +++++\n",
      " merlin/dispatch/lazy.py                            | 152 ++++++++++\n",
      " merlin/dtypes/__init__.py                          |  60 ++++\n",
      " merlin/dtypes/aliases.py                           |  52 ++++\n",
      " merlin/dtypes/base.py                              | 178 ++++++++++++\n",
      " merlin/dtypes/mapping.py                           | 173 ++++++++++++\n",
      " merlin/dtypes/mappings/__init__.py                 |  18 ++\n",
      " merlin/dtypes/mappings/cudf.py                     |  57 ++++\n",
      " merlin/dtypes/mappings/numpy.py                    |  52 ++++\n",
      " merlin/dtypes/mappings/pandas.py                   |  38 +++\n",
      " merlin/dtypes/mappings/python.py                   |  31 ++\n",
      " merlin/dtypes/mappings/tf.py                       |  52 ++++\n",
      " merlin/dtypes/mappings/torch.py                    |  43 +++\n",
      " merlin/dtypes/mappings/triton.py                   |  53 ++++\n",
      " merlin/dtypes/registry.py                          | 142 ++++++++++\n",
      " merlin/dtypes/shape.py                             | 183 ++++++++++++\n",
      " merlin/io/avro.py                                  |   4 -\n",
      " merlin/io/csv.py                                   |   1 -\n",
      " merlin/io/dask.py                                  |   6 +-\n",
      " merlin/io/dataset.py                               |  19 +-\n",
      " merlin/io/fsspec_utils.py                          |   8 +-\n",
      " merlin/io/parquet.py                               |   8 -\n",
      " merlin/io/writer.py                                |   1 -\n",
      " merlin/schema/io/tensorflow_metadata.py            |  86 +++---\n",
      " merlin/schema/schema.py                            | 298 +++++++++++---------\n",
      " merlin/table/__init__.py                           |  24 ++\n",
      " merlin/table/conversions.py                        | 135 +++++++++\n",
      " merlin/table/cupy_column.py                        |  92 ++++++\n",
      " merlin/table/numpy_column.py                       | 100 +++++++\n",
      " merlin/table/tensor_column.py                      | 217 ++++++++++++++\n",
      " merlin/table/tensor_table.py                       | 222 +++++++++++++++\n",
      " merlin/table/tensorflow_column.py                  | 159 +++++++++++\n",
      " merlin/table/torch_column.py                       | 124 ++++++++\n",
      " requirements.txt                                   |   5 +-\n",
      " tests/conftest.py                                  |  16 +-\n",
      " tests/unit/core/test_dispatch.py                   |  19 ++\n",
      " tests/unit/core/test_version.py                    |   4 +\n",
      " tests/unit/dag/test_dag_utils.py                   |  31 ++\n",
      " tests/unit/dispatch/test_lazy_dispatch.py          |  61 ++++\n",
      " tests/unit/dtypes/test_module.py                   |  48 ++++\n",
      " tests/unit/dtypes/test_shape.py                    | 222 +++++++++++++++\n",
      " tests/unit/io/test_io.py                           |  27 +-\n",
      " tests/unit/schema/test_column_schemas.py           | 142 ++++++----\n",
      " tests/unit/schema/test_schema.py                   |   7 +-\n",
      " tests/unit/schema/test_schema_io.py                |  27 +-\n",
      " tests/unit/table/test_convert_column.py            |  75 +++++\n",
      " tests/unit/table/test_tensor_column.py             | 186 ++++++++++++\n",
      " tests/unit/table/test_tensor_table.py              | 311 +++++++++++++++++++++\n",
      " tests/unit/utils/test_utils.py                     |   3 -\n",
      " tox.ini                                            |   4 +\n",
      " 80 files changed, 4413 insertions(+), 672 deletions(-)\n",
      " create mode 100644 .github/workflows/cpu-packages.yml\n",
      " create mode 100644 .prettierignore\n",
      " create mode 100644 merlin/dag/utils.py\n",
      " create mode 100644 merlin/dispatch/lazy.py\n",
      " create mode 100644 merlin/dtypes/__init__.py\n",
      " create mode 100644 merlin/dtypes/aliases.py\n",
      " create mode 100644 merlin/dtypes/base.py\n",
      " create mode 100644 merlin/dtypes/mapping.py\n",
      " create mode 100644 merlin/dtypes/mappings/__init__.py\n",
      " create mode 100644 merlin/dtypes/mappings/cudf.py\n",
      " create mode 100644 merlin/dtypes/mappings/numpy.py\n",
      " create mode 100644 merlin/dtypes/mappings/pandas.py\n",
      " create mode 100644 merlin/dtypes/mappings/python.py\n",
      " create mode 100644 merlin/dtypes/mappings/tf.py\n",
      " create mode 100644 merlin/dtypes/mappings/torch.py\n",
      " create mode 100644 merlin/dtypes/mappings/triton.py\n",
      " create mode 100644 merlin/dtypes/registry.py\n",
      " create mode 100644 merlin/dtypes/shape.py\n",
      " create mode 100644 merlin/table/__init__.py\n",
      " create mode 100644 merlin/table/conversions.py\n",
      " create mode 100644 merlin/table/cupy_column.py\n",
      " create mode 100644 merlin/table/numpy_column.py\n",
      " create mode 100644 merlin/table/tensor_column.py\n",
      " create mode 100644 merlin/table/tensor_table.py\n",
      " create mode 100644 merlin/table/tensorflow_column.py\n",
      " create mode 100644 merlin/table/torch_column.py\n",
      " create mode 100644 tests/unit/dag/test_dag_utils.py\n",
      " create mode 100644 tests/unit/dispatch/test_lazy_dispatch.py\n",
      " create mode 100644 tests/unit/dtypes/test_module.py\n",
      " create mode 100644 tests/unit/dtypes/test_shape.py\n",
      " create mode 100644 tests/unit/table/test_convert_column.py\n",
      " create mode 100644 tests/unit/table/test_tensor_column.py\n",
      " create mode 100644 tests/unit/table/test_tensor_table.py\n",
      "Processing /core\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.9.0+56.gaad0c874) (0.56.4)\n",
      "Requirement already satisfied: betterproto<2.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.9.0+56.gaad0c874) (1.2.5)\n",
      "Requirement already satisfied: fsspec==2022.5.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.9.0+56.gaad0c874) (2022.5.0)\n",
      "Requirement already satisfied: distributed>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.9.0+56.gaad0c874) (2022.7.1)\n",
      "Requirement already satisfied: pandas<1.6.0dev0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.9.0+56.gaad0c874) (1.3.5)\n",
      "Requirement already satisfied: dask>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.9.0+56.gaad0c874) (2022.7.1)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.9.0+56.gaad0c874) (8.0.0)\n",
      "Requirement already satisfied: pynvml<11.5,>=11.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.9.0+56.gaad0c874) (11.4.1)\n",
      "Requirement already satisfied: tensorflow-metadata>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core==0.9.0+56.gaad0c874) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Operation cancelled by user\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while terminating subprocess (pid=1791146): \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /models && git fetch origin && git checkout origin/tf/transformer-api && pip install .\n",
    "cd /core && git checkout main && git pull origin main && pip install .\n",
    "cd /nvtabular && git checkout main && git pull origin main && pip install .\n",
    "cd /systems && git checkout main && git pull origin main && pip install .\n",
    "cd /dataloader && git checkout main && git pull origin main && pip install .\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152aee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf ecom_dataset\n",
    "mkdir -p ecom_dataset\n",
    "\n",
    "pip install gdown\n",
    "# gdown https://drive.google.com/uc?id=1BvCHc4eXComuNK93bKhRM6cbg9y5p350  # <-- full dataset\n",
    "gdown https://drive.google.com/uc?id=1NCFZ5ya3zyxPsrmupEoc9UEm4sslAddV\n",
    "apt-get update -y\n",
    "apt-get install unzip -y\n",
    "unzip -d ecom_dataset \"rees46_ecom_dataset_small_for_ci.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb3ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-09 18:01:08.237320: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 18:01:17.553146: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:214] Using CUDA malloc Async allocator for GPU: 0\n",
      "2023-03-09 18:01:17.554189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24570 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.io.dataset import Dataset\n",
    "import merlin.models.tf as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9903e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparams similar to T4Rec benchmark script\n",
    "d_model = 192\n",
    "n_layer = 3\n",
    "n_head = 16\n",
    "batch_size = 128\n",
    "learning_rate = 0.0006667377132554976\n",
    "weight_decay = 3.910060265627374e-05 \n",
    "n_epoch = 5\n",
    "item_embedding_dim = 448 \n",
    "item_id_embeddings_init_std = 3\n",
    "input_dropout = 0.1\n",
    "initializer_range = 0.02\n",
    "layer_norm_eps = 1e-12\n",
    "dropout = 0\n",
    "mlm_prob = 0.3\n",
    "eval_on_test_set = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e181e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset(\"ecom_dataset/0001/train.parquet\")\n",
    "if eval_on_test_set:  \n",
    "    valid = Dataset(\"ecom_dataset/0002/test.parquet\")\n",
    "else: \n",
    "    valid = Dataset(\"ecom_dataset/0002/valid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f121dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'sess_pid_seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec55f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.models.tf.core.tabular import TabularBlock\n",
    "\n",
    "# Create equivalent class of T4Rec's TabularDroupout\n",
    "class TabularDropout(TabularBlock):\n",
    "    \"\"\"\n",
    "    Applies dropout transformation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs, **kwargs):\n",
    "        outputs = {key: self.dropout(val) for key, val in inputs.items()}  # type: ignore\n",
    "        return outputs\n",
    "\n",
    "# Create equivalent class of T4Rec's 'layer-norm'\n",
    "class TabularNorm(TabularBlock):\n",
    "    \"\"\"\n",
    "    Applies layr-norm transformation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def forward(self, inputs, **kwargs):\n",
    "        outputs = {key: self.layer_norm(val) for key, val in inputs.items()}  # type: ignore\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a286ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from merlin.schema.io.tensorflow_metadata import TensorflowMetadata\n",
    "\n",
    "schema = TensorflowMetadata.from_proto_text_file(\n",
    "    './',\n",
    "    file_name='rees46_schema_modified.pbtxt'\n",
    ").to_merlin_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f59155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only use the item-id as input to the model\n",
    "schema_model = schema.select_by_tag(Tags.ITEM_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d07aa5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.models.tf import InputBlockV2\n",
    "import tensorflow as tf\n",
    "input_block = InputBlockV2(\n",
    "    schema_model,\n",
    "    categorical=mm.Embeddings(\n",
    "            schema_model.select_by_tag(Tags.CATEGORICAL),\n",
    "            dim=item_embedding_dim,\n",
    "            #This is equivalent of torch.nn.init.normal_\n",
    "            embeddings_initializer=tf.keras.initializers.RandomNormal(\n",
    "                mean=0.0,\n",
    "                stddev=item_id_embeddings_init_std\n",
    "            ),\n",
    "            sequence_combiner=None,\n",
    "        ),\n",
    "    #pre=mm.StochasticSwapNoise(schema_model, replacement_prob=0.1) # This is not working with sequences transforms\n",
    "    # we apply dropout and layer-norm as post-processing steps before aggregation\n",
    "    post=TabularDropout(input_dropout).connect(TabularNorm())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b6d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projet the output of the input block into the same dimension as d_model (equivalent of d_output in T4Rec)\n",
    "mlp_block = mm.MLPBlock(\n",
    "    [d_model],\n",
    "    activation='relu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14c35b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the xlnet block with the necessary parameters\n",
    "xlnet_block = mm.XLNetBlock(\n",
    "    d_model=d_model, \n",
    "    n_head=n_head, \n",
    "    n_layer=n_layer, \n",
    "    attn_type='bi', \n",
    "    hidden_act='gelu', \n",
    "    initializer_range=initializer_range, \n",
    "    layer_norm_eps=layer_norm_eps, \n",
    "    dropout=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "866f3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_block = mm.SequentialBlock(\n",
    "    input_block,\n",
    "    mlp_block,\n",
    "    xlnet_block\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4beb1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the output of the transformer to the same dimension as `item_embedding_dim`\n",
    "# this is needed for weight-tying\n",
    "mlp_block2 = mm.MLPBlock(\n",
    "    [item_embedding_dim],\n",
    "    activation='relu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "064ea5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set next-item prediction task with weight tying option by providing the embeddings table of the `item-id` \n",
    "# as the `to_call` layer\n",
    "prediction_task = mm.CategoricalOutput(\n",
    "    to_call=input_block[\"categorical\"][target],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c008e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 18:01:20.111251: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "# Create the end-to-end Keras model\n",
    "model_transformer = mm.Model(dense_block, mlp_block2, prediction_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b89e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements optimizer with linear decay of the learning rate. This is what T4Rec pytorch-trainer is using. \n",
    "\n",
    "# For that we will use the custom optimizer `AdamWeightDecay` provided by HuggingFace\n",
    "from transformers.optimization_tf import AdamWeightDecay\n",
    "\n",
    "\n",
    "num_warmup_steps = 0\n",
    "# compute the total steps in the training iteration:\n",
    "import math\n",
    "steps_per_epoch = math.floor(train.compute().shape[0] / batch_size)\n",
    "total_step = steps_per_epoch * n_epoch\n",
    "\n",
    "# Set the linear-decay learning scheduler\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=total_step,\n",
    "    power=1,\n",
    ")\n",
    "# Set the optimizer with the `weight_decay` rate\n",
    "if weight_decay > 0.0:\n",
    "    optimizer = AdamWeightDecay(\n",
    "        learning_rate=lr_schedule,\n",
    "        weight_decay_rate=weight_decay,\n",
    "    )\n",
    "else: \n",
    "    optimizer = AdamWeightDecay(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d84a30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with ranking metrics computed at 10 and 20 thresholds\n",
    "model_transformer.compile(run_eagerly=False, optimizer=optimizer, loss=\"categorical_crossentropy\",\n",
    "              metrics=mm.TopKMetricsAggregator.default_metrics(top_ks=[10, 20])\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8cc8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the schema of the dataloader and the schema used by the model\n",
    "train.schema = schema_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7474131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 18:01:47.567695: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/sequential_block_5/xl_net_block/prepare_transformer_inputs_4/RaggedToTensor_1/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/sequential_block_5/xl_net_block/prepare_transformer_inputs_4/RaggedToTensor_1/boolean_mask/GatherV2:0\", shape=(None, 192), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/sequential_block_5/xl_net_block/prepare_transformer_inputs_4/RaggedToTensor_1/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/sequential_block_5/xl_net_block/sequential_block_8/replace_masked_embeddings/RaggedWhere/Reshape_3:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/model/sequential_block_5/xl_net_block/sequential_block_8/replace_masked_embeddings/RaggedWhere/Reshape_2:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/sequential_block_5/xl_net_block/sequential_block_8/replace_masked_embeddings/RaggedWhere/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['model/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0', 'transformer/layer_._2/rel_attn/r_s_bias:0', 'transformer/layer_._2/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/sequential_block_5/xl_net_block/sequential_block_8/replace_masked_embeddings/RaggedWhere/RaggedTile_2/Reshape_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/sequential_block_5/xl_net_block/sequential_block_8/replace_masked_embeddings/RaggedWhere/RaggedTile_2/Reshape_2:0\", shape=(None, 192), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/sequential_block_5/xl_net_block/sequential_block_8/replace_masked_embeddings/RaggedWhere/RaggedTile_2/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:436: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 174720448 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['model/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0', 'transformer/layer_._2/rel_attn/r_s_bias:0', 'transformer/layer_._2/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 18:02:45.911807: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: model/sequential_block_5/xl_net_block/sequential_block_8/replace_masked_embeddings/RaggedWhere/Assert/AssertGuard/branch_executed/_31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677/677 [==============================] - 202s 204ms/step - loss: 13.5418 - recall_at_10: 0.0333 - mrr_at_10: 0.0120 - ndcg_at_10: 0.0170 - map_at_10: 0.0120 - precision_at_10: 0.0033 - recall_at_20: 0.0524 - mrr_at_20: 0.0133 - ndcg_at_20: 0.0218 - map_at_20: 0.0133 - precision_at_20: 0.0026 - regularization_loss: 0.0000e+00 - loss_batch: 13.5668\n",
      "Epoch 2/5\n",
      "677/677 [==============================] - 135s 200ms/step - loss: 12.3608 - recall_at_10: 0.0430 - mrr_at_10: 0.0149 - ndcg_at_10: 0.0214 - map_at_10: 0.0149 - precision_at_10: 0.0043 - recall_at_20: 0.0656 - mrr_at_20: 0.0164 - ndcg_at_20: 0.0271 - map_at_20: 0.0164 - precision_at_20: 0.0033 - regularization_loss: 0.0000e+00 - loss_batch: 12.3602\n",
      "Epoch 3/5\n",
      "677/677 [==============================] - 139s 205ms/step - loss: 12.0906 - recall_at_10: 0.0435 - mrr_at_10: 0.0154 - ndcg_at_10: 0.0219 - map_at_10: 0.0154 - precision_at_10: 0.0043 - recall_at_20: 0.0672 - mrr_at_20: 0.0170 - ndcg_at_20: 0.0279 - map_at_20: 0.0170 - precision_at_20: 0.0034 - regularization_loss: 0.0000e+00 - loss_batch: 12.0902\n",
      "Epoch 4/5\n",
      "677/677 [==============================] - 134s 197ms/step - loss: 11.8980 - recall_at_10: 0.0423 - mrr_at_10: 0.0158 - ndcg_at_10: 0.0220 - map_at_10: 0.0158 - precision_at_10: 0.0042 - recall_at_20: 0.0648 - mrr_at_20: 0.0173 - ndcg_at_20: 0.0276 - map_at_20: 0.0173 - precision_at_20: 0.0032 - regularization_loss: 0.0000e+00 - loss_batch: 11.8978\n",
      "Epoch 5/5\n",
      "677/677 [==============================] - 136s 200ms/step - loss: 11.7795 - recall_at_10: 0.0421 - mrr_at_10: 0.0154 - ndcg_at_10: 0.0216 - map_at_10: 0.0154 - precision_at_10: 0.0042 - recall_at_20: 0.0659 - mrr_at_20: 0.0170 - ndcg_at_20: 0.0276 - map_at_20: 0.0170 - precision_at_20: 0.0033 - regularization_loss: 0.0000e+00 - loss_batch: 11.7790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f880c7b16a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch a training iteration with `n_epoch` epochs\n",
    "# For mlm, we need to use `SequenceMaskRandom` and specify the masking probability\n",
    "model_transformer.fit(\n",
    "    train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=n_epoch,\n",
    "    pre=mm.SequenceMaskRandom(schema=train.schema, target=target, transformer=xlnet_block, masking_prob=mlm_prob)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bf839e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using `SequenceMaskLast` to mask the last item only\n",
    "valid.schema = schema_model\n",
    "predict_last = mm.SequenceMaskLast(schema=valid.schema, target=target, transformer=xlnet_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15ccc448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "2023-03-09 18:28:21.499587: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: model/sequential_block_5/xl_net_block/sequential_block_8/replace_masked_embeddings/RaggedWhere/Assert/AssertGuard/branch_executed/_23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 34s 128ms/step - loss: 11.7879 - recall_at_10: 0.0458 - mrr_at_10: 0.0155 - ndcg_at_10: 0.0225 - map_at_10: 0.0155 - precision_at_10: 0.0046 - recall_at_20: 0.0711 - mrr_at_20: 0.0171 - ndcg_at_20: 0.0288 - map_at_20: 0.0171 - precision_at_20: 0.0036 - regularization_loss: 0.0000e+00 - loss_batch: 11.7897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 11.787938117980957,\n",
       " 'recall_at_10': 0.0489937923848629,\n",
       " 'mrr_at_10': 0.017020391300320625,\n",
       " 'ndcg_at_10': 0.024413621053099632,\n",
       " 'map_at_10': 0.017020391300320625,\n",
       " 'precision_at_10': 0.004899379797279835,\n",
       " 'recall_at_20': 0.07645288854837418,\n",
       " 'mrr_at_20': 0.018829816952347755,\n",
       " 'ndcg_at_20': 0.03123626857995987,\n",
       " 'map_at_20': 0.018829816952347755,\n",
       " 'precision_at_20': 0.003822644241154194,\n",
       " 'regularization_loss': 0.0,\n",
       " 'loss_batch': 11.866037368774414}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transformer.evaluate(\n",
    "    valid,\n",
    "    batch_size=batch_size,\n",
    "    pre=predict_last,\n",
    "    return_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17fd65b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_/next-item/ndcg_at_10': 0.08305524289608002,\n",
       " 'eval_/next-item/ndcg_at_20': 0.09936655312776566,\n",
       " 'eval_/next-item/recall_at_10': 0.15436746180057526,\n",
       " 'eval_/next-item/recall_at_20': 0.2190323770046234,\n",
       " 'eval_/loss': 8.334789276123047}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'eval_/next-item/ndcg_at_10': 0.08305524289608002, 'eval_/next-item/ndcg_at_20': 0.09936655312776566, 'eval_/next-item/recall_at_10': 0.15436746180057526, 'eval_/next-item/recall_at_20': 0.2190323770046234, 'eval_/loss': 8.334789276123047}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
