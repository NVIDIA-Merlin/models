{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50385353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f459bc8",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Applying to your own dataset with Merlin Models and NVTabular\n",
    "\n",
    "## Overview\n",
    "\n",
    "In [01-getting-started.ipynb](01-getting-started.ipynb), we provide a getting started example to train a DLRM model on the MovieLens 1M dataset. In this notebook, we will explore how Merlin Models uses the ETL output from [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular/).<br><br>\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "This notebook provides details how NVTabular and Merlin Models are linked together. We will discuss the concept of the `schema` file.\n",
    "\n",
    "## Merlin\n",
    "\n",
    "[Merlin](https://developer.nvidia.com/nvidia-merlin) is an open-source framework for building large-scale (deep learning) recommender systems. It is designed to support recommender systems end-to-end from ETL to training to deployment on CPU or GPU. Common deep learning frameworks are integrated such as TensorFlow or PyTorch. Its key benefits are the easy-to-use APIs, accelerations with GPU and scaling to multi-GPU or multi-node systems.\n",
    "\n",
    "Merlin Models and NVTabular are components of Merlin. They are designed to work closely together. \n",
    "\n",
    "[Merlin Models](https://github.com/NVIDIA-Merlin/models/) is a library to make it easy for users in industry or academia to train and deploy recommender models with best practices baked into the library. This will let users in industry easily train standard models against their own dataset, getting high performance GPU accelerated models into production. This will also let researchers to build custom models by incorporating standard components of deep learning recommender models, and then benchmark their new models on example offline datasets.\n",
    "\n",
    "[NVTabular](https://github.com/NVIDIA-Merlin/NVTabular/) is a feature engineering and preprocessing library for tabular data that is designed to easily manipulate terabyte scale datasets and train deep learning (DL) based recommender systems. It provides high-level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS Dask-cuDF library.\n",
    "\n",
    "## Integration of NVTabular and Merlin Models\n",
    "\n",
    "<img src=\"images/schema.png\">\n",
    "\n",
    "We take a look on a subsequence of our pipeline. We are interested in the interactions of feature engineering and model training.\n",
    "\n",
    "If you use NVTabular for feature engineering, in addition to the data, NVTabular will provide a `schema file` describing the dataset structures. NVTabular will automatically collect statistics and detect some types of `Tags`. For example:\n",
    "- Categorify: Transform categorical columns into continuous integers 0, ..., |C| for embedding layers. It collects the cardinality of the embedding table and tags it as categorical.\n",
    "- Normalize: Normalize continuous features. It collects the mean and std of the feature and tags it as continuous.\n",
    "\n",
    "\n",
    "Some `Tags` have to be provided manually. \n",
    "\n",
    "Let's take a look on the MovieLens 1M example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "157fe18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 18:08:06.544710: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-25 18:08:07.655653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import nvtabular as nvt\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "import merlin.io\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "\n",
    "from os import path\n",
    "from nvtabular import ops\n",
    "from merlin.core.utils import download_file\n",
    "from merlin.models.data.movielens import get_movielens\n",
    "from merlin.schema.tags import Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382da96f",
   "metadata": {},
   "source": [
    "We will use the utils function to download, extract and preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ac055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:1253: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train, valid = get_movielens(variant=\"ml-1m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d956e4",
   "metadata": {},
   "source": [
    "## Understanding the Schema File and Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ba7f9",
   "metadata": {},
   "source": [
    "When NVTabular process the data, it will persist the schema as a file to disk. The dataset contains the schema as a property, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ceb1c9",
   "metadata": {},
   "source": [
    "The `schema` can be interpreted as a list of features in the dataset, where each element describes the feature. It contains the name, some properties (e.g. statistics) and multiple tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3bd70c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'movieId', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>, <Tags.ITEM_ID: 'item_id'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.movieId.parquet', 'embedding_sizes': {'cardinality': 3685.0, 'dimension': 159.0}, 'domain': {'min': 0, 'max': 3685}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'userId', 'tags': {<Tags.USER_ID: 'user_id'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.userId.parquet', 'embedding_sizes': {'cardinality': 6041.0, 'dimension': 210.0}, 'domain': {'min': 0, 'max': 6041}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'rating_binary', 'tags': {<Tags.BINARY_CLASSIFICATION: 'binary_classification'>, <Tags.TARGET: 'target'>}, 'properties': {}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f74a04",
   "metadata": {},
   "source": [
    "We can select the features by `Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9824190f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'userId', 'tags': {<Tags.USER_ID: 'user_id'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.userId.parquet', 'embedding_sizes': {'cardinality': 6041.0, 'dimension': 210.0}, 'domain': {'min': 0, 'max': 6041}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.schema.select_by_name(\"userId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc733d",
   "metadata": {},
   "source": [
    "We can see, that NVTabular set the `Tags` `user_id`, `user` and `categorical`. In additionl, it collected the cardinality of the categorical features `6041`. Merlin Models uses the information to define the embedding table size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ece8e",
   "metadata": {},
   "source": [
    "Alternatively, we can select them by `Tag`. We add `column_names` to the object to receive only names without all the additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6fc4e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId', 'userId']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All categorical features\n",
    "train.schema.select_by_tag(Tags.CATEGORICAL).column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ccda0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All continuous features\n",
    "train.schema.select_by_tag(Tags.CONTINUOUS).column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3e7813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rating_binary']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All targets\n",
    "train.schema.select_by_tag(Tags.TARGET).column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1046b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All features related to the item\n",
    "train.schema.select_by_tag(Tags.ITEM).column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f0e26e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All features related to the user\n",
    "train.schema.select_by_tag(Tags.USER).column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f361318c",
   "metadata": {},
   "source": [
    "The `schema` is a great way to combine Feature Engineering and Model Training as one end-to-end pipeline. Many popular (deep learning) recommender models define the architecture based on different feature types. \n",
    "\n",
    "DLRM applies embedding layers to each categorical input feature and applies a MLP (called bottom MLP) to the continuous input features.\n",
    "\n",
    "Two Tower model applies a MLP (with embedding layers for categorical features) to all item features (called item tower) and another MLP to all user features (called user tower).\n",
    "\n",
    "The `schema` file contains all required information to build the architecture. If the dataset changes (e.g. more features are added), then the same code can be used to define the same architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa0769",
   "metadata": {},
   "source": [
    "## Applying NVTabular and Merlin Models to your own dataset.\n",
    "\n",
    "We have a solid understanding of the importance of the schema and how the schema works. Let's take a look on how to apply it to your own dataset.\n",
    "\n",
    "The best way is to use [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular/) for the feature engineering step. We will look on a minimal example for the MovieLens dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6e83c",
   "metadata": {},
   "source": [
    "### Download and prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b1e0b",
   "metadata": {},
   "source": [
    "We will download the dataset, if it is not already downloaded or cached locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7588e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading ml-1m.zip: 5.93MB [00:00, 10.6MB/s]                                                                                                                                              \n",
      "unzipping files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 39.65files/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = os.environ.get(\"INPUT_DATA_DIR\", os.path.expanduser(\"~/merlin-models-data/movielens/\"))\n",
    "name = \"ml-1m\"\n",
    "download_file(\n",
    "    \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\",\n",
    "    os.path.join(input_path, \"ml-1m.zip\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6070a68",
   "metadata": {},
   "source": [
    "We preprocess the dataset and split it into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a324b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24861/340885424.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  ratings = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv(\n",
    "    os.path.join(input_path, \"ml-1m/ratings.dat\"),\n",
    "    sep=\"::\",\n",
    "    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
    ")\n",
    "ratings = ratings.sample(len(ratings), replace=False)\n",
    "\n",
    "num_valid = int(len(ratings) * 0.2)\n",
    "train = ratings[:-num_valid]\n",
    "valid = ratings[-num_valid:]\n",
    "train.to_parquet(os.path.join(input_path, name, \"train.parquet\"))\n",
    "valid.to_parquet(os.path.join(input_path, name, \"valid.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8cca2",
   "metadata": {},
   "source": [
    "### Feature Engineering and Generating Schema File with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356461f",
   "metadata": {},
   "source": [
    "We use NVTabular to define a feature engineering pipeline. \n",
    "\n",
    "NVTabular has already implemented multiple transformations, called `ops`. An `op` can be applied to a `ColumnGroup` from an overloaded `>>` operator.<br><br>\n",
    "**Example:**<br>\n",
    "```python\n",
    "features = [ column_name, ...] >> op1 >> op2 >> ...\n",
    "```\n",
    "\n",
    "We need to perform following steps:\n",
    "- Categorify userId and movieId, that the values are continuous integers from 0 ... |C|\n",
    "- Transform the rating column to a binary target by using `>3` as `1` and otherwise `0`\n",
    "- Add Tags with `ops.AddMetadata` for `item_id`, `user_id`, `item`, `user` and `target`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8ff5c",
   "metadata": {},
   "source": [
    "Categorify will transform categorical columns into continuous integers 0, ..., |C| for embedding layers. It collects the cardinality of the embedding table and tags it as categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7ea1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\"userId\", \"movieId\"] >> ops.Categorify(dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a2c63",
   "metadata": {},
   "source": [
    "The tags for `user`, `userId`, `item` and `itemID` cannot be inferred from the dataset. Therefore, we need to provide them manually during the NVTabular workflow. Actually, the DLRM model does not differentiate between `user` and `item` features. But other architectures, such as the `Two Tower Model`, depends on the `user` and `item` feature. We will show how to add them manually in a NVTabular workflow below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "565595ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_itemId = cat_features[\"movieId\"] >> ops.TagAsItemID()\n",
    "feats_userId = cat_features[\"userId\"] >> ops.TagAsUserID()\n",
    "feats_target = (\n",
    "    nvt.ColumnSelector([\"rating\"])\n",
    "    >> ops.LambdaOp(lambda col: (col > 3).astype(\"int32\"))\n",
    "    >> ops.AddTags([\"binary_classification\", \"target\"])\n",
    "    >> nvt.ops.Rename(name=\"rating_binary\")\n",
    ")\n",
    "output = feats_itemId + feats_userId + feats_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a384b",
   "metadata": {},
   "source": [
    "We apply the workflow to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8881f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 680 ms, sys: 276 ms, total: 956 ms\n",
      "Wall time: 993 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_path = os.path.join(input_path, name, \"train.parquet\")\n",
    "valid_path = os.path.join(input_path, name, \"valid.parquet\")\n",
    "output_path = os.path.join(input_path, name)\n",
    "\n",
    "workflow_fit_transform(output, train_path, valid_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043c7bde",
   "metadata": {},
   "source": [
    "### Training a Recommender Model with Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a09bb6",
   "metadata": {},
   "source": [
    "We can load the data as a Merlin Dataset object. The Dataset expect the schema as `.pb` file in the train/valid folder, which NVTabular automatically generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e67f0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = merlin.io.Dataset(os.path.join(input_path, name, \"train\"), engine=\"parquet\")\n",
    "valid = merlin.io.Dataset(os.path.join(input_path, name, \"valid\"), engine=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fabd60",
   "metadata": {},
   "source": [
    "We can train and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeda7ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 18:09:33.123429: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 12s 10ms/step - rating_binary/binary_classification_task/precision: 0.7117 - rating_binary/binary_classification_task/recall: 0.8238 - rating_binary/binary_classification_task/binary_accuracy: 0.7068 - rating_binary/binary_classification_task/auc: 0.7685 - loss: 0.5628 - regularization_loss: 0.0000e+00 - total_loss: 0.5628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 18:09:47.324792: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/then/_0/cond/cond/branch_executed/_101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 3s 8ms/step - rating_binary/binary_classification_task/precision: 0.7318 - rating_binary/binary_classification_task/recall: 0.8307 - rating_binary/binary_classification_task/binary_accuracy: 0.7272 - rating_binary/binary_classification_task/auc: 0.7927 - loss: 0.5386 - regularization_loss: 0.0000e+00 - total_loss: 0.5386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7317950129508972,\n",
       " 0.8306925892829895,\n",
       " 0.7272159457206726,\n",
       " 0.7927199006080627,\n",
       " 0.5734753608703613,\n",
       " 0.0,\n",
       " 0.5734753608703613]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mm.DLRMModel(\n",
    "    train.schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(\n",
    "        train.schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "    ),\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\")\n",
    "model.fit(train, batch_size=1024)\n",
    "model.evaluate(valid, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee46b11",
   "metadata": {},
   "source": [
    "We can take a look on the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "255f122a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'movieId', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>, <Tags.ITEM_ID: 'item_id'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.movieId.parquet', 'embedding_sizes': {'cardinality': 3685.0, 'dimension': 159.0}, 'domain': {'min': 0, 'max': 3685}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'userId', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.USER_ID: 'user_id'>, <Tags.USER: 'user'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.userId.parquet', 'embedding_sizes': {'cardinality': 6041.0, 'dimension': 210.0}, 'domain': {'min': 0, 'max': 6041}}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}, {'name': 'rating_binary', 'tags': {<Tags.BINARY_CLASSIFICATION: 'binary_classification'>, <Tags.TARGET: 'target'>}, 'properties': {}, 'dtype': dtype('int32'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275a8cc",
   "metadata": {},
   "source": [
    "As we prepared only a minimal example, our schema has only tree features `movieId`, `userId` and `rating_binary`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36497b81",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Feature engineering and model training are depending on each other. The `schema` object is a convient way to provide information from the feature engineering step to the model training step. It avoids that the user has to declare the same information mutliple times and prevents errors.\n",
    "\n",
    "The `schema` defined the dataset schema and characteristics and Merlin Models can create the neural network architecture based on the information.\n",
    "\n",
    "The dataset features will be `tagged` to indicate which type of feature it represents. \n",
    "\n",
    "The recommended practice is to use `NVTabular` for feature engineering, which generates a `schema` file. NVTabular can automatically add `Tags` for certrain operations. For example, the output of `Categorify` is always a categorical feature and will be tagged. Similar, the output of `Normalize` is always continuous.\n",
    "\n",
    "You can take a look on the full example of our util function for MovieLens in our repository.\n",
    "\n",
    "Alternatively, you can manually create a schema file. We support JSON format for schema files.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebooks, we will explore multiple ranking models with Merlin Models.\n",
    "\n",
    "You can learn more about NVTabular, its functionality and suppored ops by visiting our [github repository](https://github.com/NVIDIA-Merlin/NVTabular/) or exploring the [examples](https://github.com/NVIDIA-Merlin/NVTabular/tree/main/examples), such as [`Getting Started MovieLens`](https://github.com/NVIDIA-Merlin/NVTabular/blob/main/examples/getting-started-movielens/02-ETL-with-NVTabular.ipynb) or [`Scaling Criteo`](https://github.com/NVIDIA-Merlin/NVTabular/tree/main/examples/scaling-criteo)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}